{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn28qgmjmpwU"
      },
      "source": [
        "# Bllossom-8B 양자화 모델을 이용한 한국어 LLM 튜토리얼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwtKm4_Wmud6"
      },
      "source": [
        "## 01. 활용할 package 설정\n",
        " - GPU사용하기: colab에서 런타임 --> 런타임유형변경 --> T4 선택\n",
        " - 패키지설치: 아래 pip를 이용해 Transformers, accelerate 설치\n",
        " - 런타임재시작: 런타임 --> 세션다시시작  (accelerate설치 시 런타임 다시시작하셔야됩니다!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5xmsZ2fOdAk",
        "outputId": "7452b7bc-a499-4d5a-f181-fac7aba69d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.40.0\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 transformers-4.40.0\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=172352159 sha256=c6596b29608b2a98d20b4ba045109cd2b9c1de4507bd130f62e958647164c011\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\n",
            "Fetching 6 files:   0% 0/6 [00:00<?, ?it/s]Downloading 'tokenizer.json' to '.huggingface/download/tokenizer.json.1e08eea05dd130df12c7535a8f01dcb5426ea1fc.incomplete'\n",
            "Downloading 'llama-3-Korean-Bllossom-8B-Q4_K_M.gguf' to '.huggingface/download/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf.882dc22ffebe74c225373af08375017c4c529af439ccf6e0b5587188fb43d0bd.incomplete'\n",
            "Downloading 'special_tokens_map.json' to '.huggingface/download/special_tokens_map.json.86e4627c9ec4a278205b4598a034199e721e5bd2.incomplete'\n",
            "Downloading 'tokenizer_config.json' to '.huggingface/download/tokenizer_config.json.6c407ae86d155b3c4100b980084c5c8105cec980.incomplete'\n",
            "Downloading 'README.md' to '.huggingface/download/README.md.f68979286f9cb7e46e1baa6b217248f303f094f0.incomplete'\n",
            "Downloading '.gitattributes' to '.huggingface/download/.gitattributes.4eba1088cbec5eae276a6990930ab6a5136103d1.incomplete'\n",
            "\n",
            "tokenizer.json:   0% 0.00/10.1M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "special_tokens_map.json: 100% 507/507 [00:00<00:00, 4.12MB/s]\n",
            "Download complete. Moving file to special_tokens_map.json\n",
            "\n",
            "\n",
            "tokenizer_config.json:   0% 0.00/51.3k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "README.md: 100% 7.61k/7.61k [00:00<00:00, 48.2MB/s]\n",
            "Download complete. Moving file to README.md\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 100% 1.59k/1.59k [00:00<00:00, 15.1MB/s]\n",
            "Download complete. Moving file to .gitattributes\n",
            "Fetching 6 files:  17% 1/6 [00:00<00:02,  2.03it/s]\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 51.3k/51.3k [00:00<00:00, 633kB/s]\n",
            "Download complete. Moving file to tokenizer_config.json\n",
            "\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   0% 10.5M/5.02G [00:00<01:00, 83.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   1% 41.9M/5.02G [00:00<00:28, 176MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   1% 73.4M/5.02G [00:00<00:23, 215MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tokenizer.json: 100% 10.1M/10.1M [00:00<00:00, 17.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 10.1M/10.1M [00:00<00:00, 16.9MB/s]\n",
            "Download complete. Moving file to tokenizer.json\n",
            "\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   3% 136M/5.02G [00:00<00:18, 259MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   3% 168M/5.02G [00:00<00:18, 266MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   4% 199M/5.02G [00:00<00:17, 271MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   5% 241M/5.02G [00:00<00:16, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   5% 273M/5.02G [00:01<00:16, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   6% 304M/5.02G [00:01<00:16, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   7% 336M/5.02G [00:01<00:16, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   7% 367M/5.02G [00:01<00:16, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   8% 409M/5.02G [00:01<00:15, 304MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   9% 440M/5.02G [00:01<00:15, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  10% 482M/5.02G [00:01<00:15, 298MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  10% 514M/5.02G [00:01<00:15, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  11% 545M/5.02G [00:02<00:15, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  11% 577M/5.02G [00:02<00:15, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  12% 608M/5.02G [00:02<00:15, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  13% 650M/5.02G [00:02<00:14, 301MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  14% 682M/5.02G [00:02<00:15, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  14% 713M/5.02G [00:02<00:15, 280MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  15% 744M/5.02G [00:02<00:15, 272MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  15% 776M/5.02G [00:02<00:15, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  16% 807M/5.02G [00:02<00:16, 262MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  17% 839M/5.02G [00:03<00:18, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  18% 881M/5.02G [00:03<00:16, 255MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  18% 912M/5.02G [00:03<00:15, 261MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  19% 944M/5.02G [00:03<00:16, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  19% 975M/5.02G [00:03<00:28, 142MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  20% 996M/5.02G [00:04<00:39, 101MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  20% 1.03G/5.02G [00:04<00:31, 126MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  21% 1.06G/5.02G [00:04<00:25, 154MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  22% 1.10G/5.02G [00:04<00:19, 196MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  23% 1.14G/5.02G [00:04<00:16, 229MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  23% 1.17G/5.02G [00:04<00:15, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  24% 1.21G/5.02G [00:05<00:15, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  25% 1.24G/5.02G [00:05<00:14, 258MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  25% 1.27G/5.02G [00:05<00:14, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  26% 1.30G/5.02G [00:05<00:13, 279MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  27% 1.33G/5.02G [00:05<00:13, 274MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  27% 1.36G/5.02G [00:05<00:13, 269MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  28% 1.39G/5.02G [00:05<00:12, 279MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  28% 1.43G/5.02G [00:05<00:12, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  29% 1.46G/5.02G [00:05<00:12, 279MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  30% 1.49G/5.02G [00:06<00:13, 265MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  30% 1.52G/5.02G [00:06<00:13, 258MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  31% 1.55G/5.02G [00:06<00:13, 257MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  32% 1.58G/5.02G [00:06<00:13, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  32% 1.61G/5.02G [00:06<00:13, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  33% 1.65G/5.02G [00:06<00:13, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  33% 1.68G/5.02G [00:06<00:13, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  34% 1.71G/5.02G [00:07<00:13, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  35% 1.74G/5.02G [00:07<00:13, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  35% 1.77G/5.02G [00:07<00:13, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  36% 1.80G/5.02G [00:07<00:13, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  37% 1.84G/5.02G [00:07<00:13, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  37% 1.87G/5.02G [00:07<00:13, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  38% 1.90G/5.02G [00:07<00:13, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  38% 1.93G/5.02G [00:07<00:12, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  39% 1.96G/5.02G [00:08<00:12, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  40% 1.99G/5.02G [00:08<00:12, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  40% 2.02G/5.02G [00:08<00:12, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  41% 2.06G/5.02G [00:08<00:12, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  42% 2.09G/5.02G [00:08<00:11, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  42% 2.12G/5.02G [00:08<00:12, 237MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  43% 2.15G/5.02G [00:08<00:11, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  43% 2.18G/5.02G [00:08<00:11, 249MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  44% 2.21G/5.02G [00:09<00:10, 259MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  45% 2.24G/5.02G [00:09<00:10, 271MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  45% 2.28G/5.02G [00:09<00:10, 273MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  46% 2.31G/5.02G [00:09<00:09, 280MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  47% 2.34G/5.02G [00:09<00:09, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  47% 2.38G/5.02G [00:09<00:08, 295MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  48% 2.42G/5.02G [00:09<00:08, 303MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  49% 2.45G/5.02G [00:09<00:08, 295MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  50% 2.49G/5.02G [00:09<00:08, 298MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  50% 2.53G/5.02G [00:10<00:08, 305MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  51% 2.56G/5.02G [00:10<00:08, 307MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  52% 2.59G/5.02G [00:10<00:13, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  52% 2.62G/5.02G [00:10<00:12, 199MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  53% 2.65G/5.02G [00:10<00:10, 217MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  53% 2.68G/5.02G [00:10<00:10, 225MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  54% 2.72G/5.02G [00:11<00:09, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  55% 2.75G/5.02G [00:11<00:09, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  55% 2.78G/5.02G [00:11<00:08, 256MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  56% 2.81G/5.02G [00:11<00:08, 266MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  57% 2.84G/5.02G [00:11<00:07, 274MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  57% 2.87G/5.02G [00:11<00:07, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  58% 2.90G/5.02G [00:11<00:07, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  59% 2.94G/5.02G [00:11<00:07, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  59% 2.97G/5.02G [00:11<00:07, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  60% 3.00G/5.02G [00:12<00:07, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  60% 3.03G/5.02G [00:12<00:07, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  61% 3.06G/5.02G [00:12<00:06, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  62% 3.09G/5.02G [00:12<00:06, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  62% 3.12G/5.02G [00:12<00:06, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  63% 3.16G/5.02G [00:12<00:06, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  64% 3.19G/5.02G [00:12<00:06, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  64% 3.22G/5.02G [00:14<00:40, 44.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  65% 3.26G/5.02G [00:14<00:27, 64.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  66% 3.30G/5.02G [00:15<00:19, 89.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  66% 3.33G/5.02G [00:15<00:15, 110MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  67% 3.37G/5.02G [00:15<00:13, 126MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  68% 3.40G/5.02G [00:15<00:11, 141MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  68% 3.43G/5.02G [00:15<00:09, 165MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  69% 3.46G/5.02G [00:15<00:08, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  70% 3.49G/5.02G [00:15<00:07, 207MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  70% 3.53G/5.02G [00:15<00:06, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  71% 3.57G/5.02G [00:16<00:05, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  72% 3.60G/5.02G [00:16<00:05, 237MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  72% 3.63G/5.02G [00:16<00:05, 251MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  73% 3.67G/5.02G [00:16<00:04, 272MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  74% 3.70G/5.02G [00:16<00:04, 276MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  74% 3.73G/5.02G [00:16<00:04, 277MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  75% 3.76G/5.02G [00:16<00:04, 275MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  76% 3.81G/5.02G [00:16<00:04, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  76% 3.84G/5.02G [00:17<00:04, 274MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  77% 3.88G/5.02G [00:17<00:03, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  78% 3.91G/5.02G [00:17<00:03, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  79% 3.94G/5.02G [00:17<00:03, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  79% 3.97G/5.02G [00:17<00:03, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  80% 4.01G/5.02G [00:17<00:03, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  80% 4.04G/5.02G [00:17<00:03, 276MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  81% 4.07G/5.02G [00:17<00:03, 272MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  82% 4.10G/5.02G [00:17<00:03, 266MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  82% 4.13G/5.02G [00:18<00:03, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  83% 4.16G/5.02G [00:18<00:03, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  84% 4.19G/5.02G [00:18<00:03, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  84% 4.23G/5.02G [00:18<00:03, 254MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  85% 4.26G/5.02G [00:18<00:02, 259MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  85% 4.29G/5.02G [00:18<00:02, 253MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  86% 4.32G/5.02G [00:18<00:03, 218MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  87% 4.35G/5.02G [00:19<00:02, 225MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  87% 4.38G/5.02G [00:19<00:03, 210MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  88% 4.41G/5.02G [00:19<00:02, 218MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  89% 4.45G/5.02G [00:19<00:02, 216MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  89% 4.48G/5.02G [00:19<00:02, 205MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  90% 4.51G/5.02G [00:19<00:02, 220MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  90% 4.54G/5.02G [00:19<00:02, 219MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  91% 4.57G/5.02G [00:20<00:02, 217MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  92% 4.60G/5.02G [00:20<00:01, 223MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  92% 4.63G/5.02G [00:20<00:01, 215MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  93% 4.67G/5.02G [00:20<00:01, 223MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  94% 4.70G/5.02G [00:20<00:02, 160MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  94% 4.73G/5.02G [00:20<00:01, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  95% 4.76G/5.02G [00:21<00:01, 205MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  96% 4.79G/5.02G [00:21<00:01, 225MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  96% 4.82G/5.02G [00:21<00:00, 226MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  97% 4.85G/5.02G [00:21<00:00, 226MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  97% 4.89G/5.02G [00:21<00:00, 202MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  98% 4.92G/5.02G [00:21<00:00, 204MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  99% 4.95G/5.02G [00:21<00:00, 205MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  99% 4.98G/5.02G [00:22<00:00, 222MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf: 100% 5.02G/5.02G [00:22<00:00, 226MB/s]\n",
            "Download complete. Moving file to llama-3-Korean-Bllossom-8B-Q4_K_M.gguf\n",
            "Fetching 6 files: 100% 6/6 [00:22<00:00,  3.79s/it]\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.40.0 accelerate\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python #양자화 구동을 위한 Llama C++ 설치\n",
        "!huggingface-cli download MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M --local-dir='./' #Bllossom모델 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.40.0 accelerate\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "T0ykhq0lQN82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da9ebbc3-a419-4c29-f21b-2b025034a901"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.40.0 in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.79)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "1jKCKrFTxG3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "e823fc784cf64a0fb9fb47d1818011be",
            "96dba2fdf569486c8224953ecd0c5e70",
            "5b807b083c8a4d76bfe5db09326f008d",
            "654f8002e1104b3da8ca51b3e18ae03f",
            "bfac115e9e9a49f282241fb0060eac44",
            "2e8fb9fb1d404d3db8247be78211d34c",
            "7250e7dfea86462ebe9ef8d978cd02a9",
            "5dae105029f142fbbf2b9e74515f2108",
            "822c26a0398545d68caddcd0670e21d0",
            "bc28e7f3c7e34767977e3df1410e1a02",
            "7ad98a118928406dafaf69f7d79df16c"
          ]
        },
        "outputId": "d0f2a79e-ad09-44de-d535-5aa81470f59c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e823fc784cf64a0fb9fb47d1818011be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irNZYVFGnZEs"
      },
      "source": [
        "## 02. 모델준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XbOA7HcbirUG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2d8b832e57e04b0c933acec5dbc4a292",
            "074fdfa86e9c4ba1a2f930247a392e26",
            "a804a2d29d654aceb08c4c52fb12c85a",
            "1388f51cefd84128a9dca3b0f09d3cc4",
            "791c13cd51c94f3ebbd7dc606e05126f",
            "5044916065f645238366487de5b1f9e0",
            "763ffd41777c461bb12efc31490630a0",
            "52303616d85641739bfa8c6043c36824",
            "31fc3c63801d42178c55be9962b6317f",
            "2ef1a2028e4242a8a6aa5c25acad097e",
            "86ae0c98cbcf4d2c8b699e4146a3fec1",
            "d0c29e23f37b4eafa2b1c0bb684da4ff",
            "85bf9e08e2394684a05ff9d0b8f0d6db",
            "1aa2c657f3334d249ce779530680ace5",
            "36cf0abf82874cd4b666b86e809887ff",
            "2bf5d6be73ca4ff1aa5173c49d5b0d8a",
            "a1128ef618c24e2694dc951bcc01a969",
            "5713dc6a3c744c61a7dbafb25a0869fa",
            "6fe119c27c7d4a88b7112c5fd0c50e32",
            "73c00ba9ff2b48c29bca0042ce01456c",
            "79cc20b38b5745c092f5b3c2dc8e762b",
            "1c51e75de67f4649adfd3c647b67ccf4",
            "0fcb8078d31a411baecb19b54d19128e",
            "18977678045342d291d529c34cc878bb",
            "aac60e0a7361480c8432d159848f1795",
            "4219bb7da97943f5bcc66f2048e1f1f3",
            "cd508c8745da4cc28085cc609b251421",
            "83321b4304db433499f64f81267550ab",
            "1f9a51011b694a2f8e5a62cff952cb85",
            "c9412ed7eb144aeb8bb37e3ce38338e9",
            "2d7682123ae0458aa2b6a5cbe87d1d98",
            "1a57db9f89b74c3d95edc24033ef2c83",
            "07f9c07a2f5c4ea29faa80a8230fcb9f"
          ]
        },
        "outputId": "3a003943-3d62-4eaa-b561-ab6952a48308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d8b832e57e04b0c933acec5dbc4a292"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/10.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0c29e23f37b4eafa2b1c0bb684da4ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fcb8078d31a411baecb19b54d19128e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 145088\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,145088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,145088]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,145088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,296982]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 144782\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 144783\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens cache size = 306\n",
            "llm_load_vocab: token to piece cache size = 0.9314 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 145088\n",
            "llm_load_print_meta: n_merges         = 296982\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.17 B\n",
            "llm_load_print_meta: model size       = 4.66 GiB (4.91 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 144782 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 144783 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 144791 '<|eot_id|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   318.80 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4457.43 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 10016\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1252.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1252.00 MiB, K (f16):  626.00 MiB, V (f16):  626.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.55 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   677.57 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    27.57 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '144783', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '145088', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '144782', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ],
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = Llama(\n",
        "    model_path='./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=10000,\n",
        "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHcxdlTTnhqs"
      },
      "source": [
        "## 03. 추론"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \\\n",
        "'''\n",
        "Role: \"You are a teacher running a college entrance essay interview academy.\"\n",
        "Task: \"Your main job is to generate example questions from a set of example passages. The number of passages in the set and the number of questions are not fixed. I will show you an example set of passages and example questions. Based on this, you will generate questions from a new set of passages in a similar manner. Your task is to generate only the questions.\"\n",
        "'''\n",
        "\n",
        "instruction = 'Did you understand what I said?'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":512,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":0.2,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42tc6c73ZaB-",
        "outputId": "3f6d9e9e-5f00-40f2-b28a-2448d9e6c1e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     559.31 ms\n",
            "llama_print_timings:      sample time =     174.61 ms /    79 runs   (    2.21 ms per token,   452.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.55 ms /   111 tokens (    2.55 ms per token,   392.85 tokens per second)\n",
            "llama_print_timings:        eval time =    2309.65 ms /    78 runs   (   29.61 ms per token,    33.77 tokens per second)\n",
            "llama_print_timings:       total time =    2849.59 ms /   189 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I understood that you have a job as a teacher running a college entrance essay interview academy. Your main job is to generate example questions from a set of example passes. You will show me an example set of passes and example questions, and then I will help you generate questions from a new set of passes in a similar manner. I will only provide the questions and not the answers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \\\n",
        "'''\n",
        "Here is a sample set of prompts given as an example. This set consists of four prompts.\n",
        "\n",
        "[제시문 (가)]\n",
        "예술은 새롭게 느끼고 생각하게 함으로써 인간과 현실과 세계를 신선하게 경험하도록 한다. 예술의 언어는\n",
        "인간을 폭넓게 이해하며, 현실의 숨은 모습을 드러내고, 세계의 깊이를 자각하게 한다. 기존의 것을 있는 그\n",
        "대로 받아들이지 않고 새롭게 이해하는 것은 대상의 변형 가능성을 찾아내는 것이다. 이러한 변형 가능성은\n",
        "새 세계를 개시(開示)할 수 있는데, 바로 이 점이 보편성에 대한 예술의 대응 방식이 철학, 사회과학 혹은\n",
        "자연과학과는 다르다는 것을 보여준다. 예술은 처음부터 보편성이나 전체성을 말하지 않는다. 그렇듯이 ‘미\n",
        "(美)’ 혹은 ‘인간성’을 직접 말하지 않는다. 프로파간다 예술1)이나 사회주의 리얼리즘2)에서는 이를 직접 말\n",
        "하기도 하지만, 이런 종류의 작품이 갖는 한계는 분명하다. 옳고 선하고 아름다운 일과 좋은 이념은 실현되\n",
        "기 어렵기 때문에 버거운 것이다. 자유와 정의, 평등과 인권 같은 보편적인 가치들은 시종일관 반성되지 않\n",
        "으면 거짓이기 쉽다. 검토와 비판을 허락하지 않는 보편성은 가짜이다. 참된 보편성은 개별적 경험과 특수한\n",
        "사례의 지속적 검토를 견뎌낼 수 있어야 한다. 처음부터 보편타당한 원리를 상정하는 것이 아니라, 이 원리\n",
        "의 갱신 가능성을 구체적 경험 속에서 늘 다시 검토해야 한다. 보편성의 검토 작업에서 예술의 위치, 역할,\n",
        "메커니즘은 독특하다. 그 이유는 첫째, 예술은 개념적인 추상이 아니라 구체적인 경험에 천착하고, 둘째, 이\n",
        "를 보편성의 지평 안으로 유도하여 재해석하는 활동이기 때문이다. 예술은 구체적 보편성 혹은 개별적 일반\n",
        "성의 차원으로 나아간다. 특히 감각적인 경험과 구체적인 현실에 대한 밀착이야말로 예술의 제1덕목이다. 이\n",
        "러한 밀착을 통해 예술은 흩어지고 무너지고 짓밟히고 외면된 것을 새롭게 읽어내고자 한다. 예술의 저항은\n",
        "선언의 낭독이나 거리에서의 궐기 같은 직접적인 방식이 아니며, 설명이나 진단 혹은 분석과 같은 사회과학\n",
        "적 방식 또한 아니다.\n",
        "그러나 분명한 것은 예술이 단순히 운명의 필연성에 복속되지 않고 그 나름으로 대응하고 있다는 점이다.\n",
        "예술은 드러나지 않을 수 있지만 미세하고도 오랜 파문을 일으킨다는 점에서 가장 강력한 저항매체라고 할\n",
        "수 있다. 예술은 실천적이지만 행동주의적인 것이 아니며, 성찰적이지만 사변적인 것은 아니다. 심미적 개입\n",
        "은 사유와 실천 사이에서 감각적인 경험과 구체적인 현실에 밀착하는 가운데 부단히 이루어진다. 그런 점에\n",
        "서 예술은 운명의 악순환 고리를 끊으면서 우리를 더 넓고 깊은 행복으로 이끌고 나아간다고 말할 수 있다.\n",
        "예술작품을 놓고 그것이 뜻하는 바에 대한 진위나 가치를 따질 수는 없다. 그런 방식으로 예술작품을 대한\n",
        "다면, 그것은 이미 예술작품이 아닌 것이다. 시나 소설과 같은 예술작품은 일상적으로 사용되는 언어와 달\n",
        "리, 객관적 사실을 표상하거나 구체적인 감정이나 생각을 표현하는 것도 아니고, 행위를 유도하기 위해 쓰이\n",
        "지도 않는다. 따라서 한 예술작품에 대해 말할 때, 예술가의 의도나 생각과 같은 비언어적 맥락은 예술작품\n",
        "의 의미를 결정하는 데 도움이 되지 않는다. 한마디로 말해 예술작품의 의미는 예술가의 의도일 수 없다.\n",
        "\n",
        "[제시문 (나)]\n",
        "예술작품이란 인간의 가치를 표현하고 형상화한 하나의 형식이다. 이 추상화된 형식은 관객에게 즐거움을\n",
        "줄 수 있지만, 형식을 단순히 추상적 도안으로 해석해서는 안 된다. 형식은 표상적이고 재현적이며 함축적인\n",
        "요소의 결합체이다. 형식을 이렇게 해석할 때 예술에 적용되는 의미가 풍부해지고 인간의 가치 위계에서 당\n",
        "연히 높은 위치를 점하게 된다. 많은 예술가가 자신을 위해 작품을 만든다고 하지만, 사실은 자신의 규범을\n",
        "설정하고 있는 것이다. 예술작품은 사회적인 반응과 수렴을 요구하는 관계 속에서 비로소 의미를 가진다. 예\n",
        "술가는 예술 경향의 여러 갈래 속에서 자신만의 고유한 작업을 통해 인성을 창조하며, 인간적 고뇌와 고통\n",
        "을 풀어보려 노력한다. 이를 통해 예술가는 현세의 이상향을 찾아 나서기도 한다. 실제로 모든 종류의 가치,\n",
        "이를테면 도덕적, 종교적, 경제적, 과학적 가치 등은 적절한 환경 아래에서 미적으로 관조된다. 이러한 기초\n",
        "위에서 예술이 인간의 모든 가치 영역을 표현하거나 포함한다고 볼 수 있다. 예술이 직접적인 혹은 특별한\n",
        "의무를 명시하지는 않더라도, 그 영향력으로 우리가 언제나 고귀한 존재로 남을 수 있게 해준다.\n",
        "예술은 인간을 감성적, 지적, 도덕적으로 성장시킨다는 측면에서 교육적 기능을 담당한다고 볼 수 있다. 시\n",
        "나 소설과 같은 문자 예술은 독자에게 과학으로는 결코 도달할 수 없는 인간에 대한 진실성을 보여주고, 사\n",
        "회나 역사에 대한 의식과 지식의 폭을 넓혀주며, 도덕적 감수성을 고양하는 데 크게 기여한다. 위대한 소설\n",
        "을 읽었을 때, 훌륭한 연극을 보았을 때, 우리는 흔히 커다란 정신적 충격을 경험한다. 도스토옙스키의 「죄\n",
        "와 벌」을 읽거나 셰익스피어의 「리어왕」을 관람한 후에 우리는 그동안 알아왔던 세계가 뒤바뀌고 뒤집히\n",
        "는 느낌을 가질 수 있다. 이런 작품들은 우리가 지금 살고 있는 세계 속에 안주하는 것을 용납하지 않는다.\n",
        "또한 세계를 새롭게 볼 수 있게 하고, 새로운 감각으로 우리 자신을 느끼게 하며, 행위의 의미를 새로운 시\n",
        "각에서 조명하게 한다. 예술을 통한 이러한 경험은 비단 문학작품에서만 나타나는 것은 아니다. 위대한 그림\n",
        "을 보았을 때, 멋진 무용을 구경했을 때, 훌륭한 음악을 들었을 때도 마찬가지이다. 예술작품과의 접촉을 통\n",
        "해 우리는 사물 현상을 다른 차원에서 신선하게 느끼며, 우리의 행위를 새로운 도덕적 척도에서 평가하게\n",
        "되는 것이다. 이를 통해 우리는 세계와 우리 자신이 달라진 것을 발견할 수 있다.\n",
        "\n",
        "[제시문 (다)]\n",
        "음악은 인간의 마음을 선하게 한다. 평화로운 음악은 사람으로 하여금 화목하고 상호 공경하게 하여 방탕\n",
        "에 빠지지 않도록 하며, 엄숙하고 장엄한 음악은 마음을 가지런하게 하여 어지럽지 않게 한다. 그러므로 마\n",
        "음을 감동시키고 풍속을 바꾸는 데에는 음악보다 더 좋은 것이 없다. 음악은 사람 각자의 심성을 교화시키\n",
        "고, 나아가 사회 구성원 간의 조화로움을 지향한다. 이를 유가(儒家)에서는 예교(禮敎)와 악교(樂敎)를 통합\n",
        "하여 ‘예악교화(禮樂敎化)’라고 이른다. 예악(禮樂)은 사람들이 도덕적 선(善)을 성취하는 도덕 수양에서 중\n",
        "요한 수단으로 간주된다. 여기서 예(禮)는 이지(理智)의 측면에서 말한 것이고, 악(樂)은 감정의 측면에서\n",
        "말한 것이다. 이지는 차이를 구별하는 것이고, 감정은 화합하는 것이다. 그러므로 예의는 인륜 관계를 구별\n",
        "하여 각자 자신의 위치를 편안히 여기고 그 직분을 담당하게 하며, 음악은 집단의 감정을 조화롭게 하고 의\n",
        "사소통을 통하여 마음을 융합하게 만든다. 어느 학자는 “음악은 인간의 감정을 화합하며, 예는 인륜 관계의\n",
        "차이를 구별한다”고 지적했는데, 이것은 예악의 상대적 역할과 조화를 말한 것이다. 또 『예기(禮記)』에서\n",
        "는 “음악은 안에서부터 나오며, 예는 밖에서부터 생겨난 것이다”라고 하였다. 이처럼 예와 악이 하나는 안에\n",
        "서, 하나는 밖에서 상호 조절하여 이지와 감정 모두 평정을 얻게 된다. “음악이 잘 통하면 원망이 없고 예가\n",
        "잘 통하면 다툼이 없으니, 서로 양보하여 천하를 다스리는 것은 예악을 두고 한 말이다”라는 구문에서 이를\n",
        "확인할 수 있다. “예악을 통합하여 인간의 마음을 다스린다”는 것은 예악의 효용을 의미한다. ‘마음을 다하여\n",
        "변화를 극진하게 하는 것’은 음악이 마음 깊숙한 곳을 울려 감정의 변화를 극진히 하는 것을 뜻하며, ‘성실\n",
        "함을 밝히고 허위를 제거하는 것’은 예가 공경의 마음을 표현하고 거짓 행위를 제거하는 것을 말한다. 그래\n",
        "서 “음악이 행해지면 뜻이 맑아지고, 예를 닦으면 덕행이 이루어진다”, “예에서 사람이 서고, 악에서 사람이\n",
        "완성된다”는 등의 말은 예술과 도덕의 조화로운 관계 속에서 도덕적으로 완성된 인격체로서 거듭날 수 있음\n",
        "을 강조한 것이다. 이와 더불어, “음악이 나쁜 풍속을 좋은 방향으로 변화시키니 천하가 모두 태평해진다”라\n",
        "는 말은 예술이 결코 인간의 공동체적 삶과 분리될 수 없으며, 구성원의 삶을 인격적으로 고양하고 공동체\n",
        "의 안녕과 발전에 기여해야 한다는 것이다. 결국 ‘예술을 위한 예술’은 존재할 수 없으며, 개인적 차원이나\n",
        "공동체 차원에서 선을 추구하고 이를 지향하는 경우에만 진정한 예술이 된다.\n",
        "\n",
        "[제시문 (라)]\n",
        "예술은 의미의 문제가 아니라 감각적인 질과 구상(構想)의 문제이다. 예술의 재료가 의미중립적인 것이든\n",
        "인간적인 것이든, 우리는 예술의 재료와 이 재료로 만들어진 예술작품을 구분해야 한다. 예술작품의 창조나\n",
        "감상에 대한 관심은 재료 자체에 있는 것이 아니라 재료로 만들어진 결과물에 있다. 그러나 그 모두가 바로\n",
        "예술작품이 될 수는 없으며, 사람들이 흥미나 감흥을 느낄 때에만 비로소 ‘미적’이라고 할 수 있다. 예술의\n",
        "관심이 사회의 규칙과 원리에 포함된다는 시각이나 이러한 원리를 예술에 적용하려는 시도는 미적 창조와\n",
        "감상 모두에 유해한 것이다. 미적 대상은 인식적인 가치로부터 분리되어 있으며, 감각적이거나 형식적인 성\n",
        "질 자체를 깨달았을 때의 순수한 기쁨에서 정당성을 얻는다. 아름다운 사물에서 ‘아름다움’의 의미를 찾아내\n",
        "고 받아들이는 것은 교양 있고 선택된 사람의 특권이다. 셰익스피어의 작품은 운문(韻文)의 수려함, 비유적\n",
        "표현의 다양함과 풍부함, 그리고 구성의 탁월함 때문에 상찬을 받는다. 예술가의 창조성은 그 자체로 목적이 된다.\n",
        "그렇다면 예술은 무엇에 도움을 줄 수 있는가? 상업주의는 도구적 문화를 요구하며 예술의 유용성만을 강\n",
        "조하기 때문에 비판에 직면하고 있다. 음미되는 가치 자체만으로 예술은 존재의 이유가 있다. 예술은 절대로\n",
        "어떤 것에도 도움을 주지 않으며, 역사, 사회, 그리고 개인의 맥락과는 단절된 자율적 유기체이다. 계산될 수\n",
        "있는 한에서 세계는 과학의 지배를 받기 마련이지만, 예술의 왕국은 ‘계산될 수 없는 곳’에서 시작된다. 예술\n",
        "은 매 순간 자기 충만성을 향유하게 하며, 경험을 생기 있게 하고, 경험이 지닌 상상적이고 정서적인 기쁨을\n",
        "즐기게 한다. 좋은 삶이란 경험의 매 순간을 풍부하고 정열적으로 사는 것이다. 그러므로 예술을 사랑하는\n",
        "것이야 말로 좋은 삶의 원천이 된다.\n",
        "\n",
        "\n",
        "Next, I will show you the example question. This question was generated from the sample set of prompts above.\n",
        "\n",
        "\n",
        "[예시 질문]\n",
        "제시문(가) ∼ 제시문(라)는 예술에 관한 견해를 담고 있다. 제시문들을 상반된 두 입장으로 분류하고 각 입장을 요약하시오.\n",
        "\n",
        "In this way, the example question references the content of the sample prompts, summarizing them or asking about topics that can be considered from them. When creating question, it's important to consider the relationships between the contents of each prompt.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Please create one question and one answer based on the following prompts. You need to generate questions from the [new prompts]. When generating question, the content should not imply or suggest the answers. Please delete any unnecessary sentences. The format should be as follows:\n",
        "\n",
        "[\"Question 1\"\n",
        "\"Question 2\"\n",
        "\"Question 3\"\n",
        "\"Answer 1\"\n",
        "\"Answer 2\"\n",
        "'Answer 3\"]\n",
        "\n",
        "Please create one question and one answer. As I said, when generating question, the content should not imply or suggest the answer. When creating question, it's important to consider the relationships between the contents of each prompt. The question should be in Korean, and concise, consisting of 1-2 sentences.\n",
        "\n",
        "\n",
        "[new prompts (가)]\n",
        "협력이 가능한 일반적인 요건은 두 가지이다. 첫째, 협력은 호혜주의를 바탕으로 한다. 둘째, 호혜주의가 오\n",
        "랜 기간 동안 안정적으로 유지될 수 있다는 믿음이 있어야 한다. 결국 서로가 상대방을 도울 수 있을 때 협\n",
        "력이 가능하다. 그런데 도울 때 대가를 치러야 한다면 딜레마가 생길 수 있다. 협력 당사자의 입장에서 보면\n",
        "상대방의 협력 덕택에 얻는 이득이 내가 협력할 때 치러야 할 비용보다 더 커야만 협력할 이유가 생겨서 상\n",
        "호 이득의 기회를 현실화할 수 있다. 이 경우 협력 당사자 모두가 상호 협력을 상호 배반보다 더 선호하게\n",
        "되는 것은 당연하다. 그러나 내가 원하는 것을 얻기가 쉽지는 않다. 다음 두 이유 때문이다. 첫째, 나는 도움\n",
        "을 주지 않는 것이 단기적으로 더 이득이지만 그럼에도 불구하고 상대방은 나를 돕도록 유도하고 싶어 한\n",
        "다. 둘째, 나는 남에게 큰 비용이 드는 도움을 제공하지 않으면서도 받을 수 있는 도움은 모두 받고 싶은 유\n",
        "혹을 느낀다. 하지만 일단 한 집단 안에서 호혜주의를 바탕으로 협력이 자리를 잡으면 어떤 비협력적인 전\n",
        "략도 침범하기 어려워서 스스로를 지켜낼 수 있다.\n",
        "협력이 배태될 수 있고, 온갖 다양한 전략이 뒤섞여 있는 환경에서도 번성할 수 있으며, 또 일단 자리를 잡\n",
        "은 뒤에는 스스로를 보호할 수 있다는 사실은 매우 고무적이다. 하지만 무엇보다 흥미로운 사실은 이러한\n",
        "결과를 얻는데 개인이나 사회적인 환경의 특성은 거의 영향을 미치지 못한다는 점이다. 개인은 논리적일 필\n",
        "요가 없고 어떻게 하고 왜 그렇게 해야 하는지 알지 못해도 된다. 진화 과정은 성공적인 전략들이 자연적으\n",
        "로 번성하게 해 준다. 또한 개인은 서로 메시지나 약속을 주고받을 필요도 없다. 말도 필요하지 않다. 행동\n",
        "이 말을 대신하기 때문이다. 마찬가지로 개인 사이에 어떤 신뢰도 필요하지 않다. 호혜주의만으로 충분히 배\n",
        "반을 비생산적이 되게 할 수 있기 때문이다. 이타주의도 필요하지 않다. 성공적인 전략은 이기주의자한테서\n",
        "도 협력을 이끌어내기 때문이다.\n",
        "협력의 창발과 성장, 유지에 꼭 필요한 개인과 사회 환경에 대한 가정이 몇 가지 있기는 하다. 우선 예전에\n",
        "상호 작용했던 협력자를 알아볼 수 있어야 하며 상대와의 과거 상호 작용 내력을 기억할 수 있어야 한다. 그\n",
        "래야 그에 따라 반응할 수 있다. 하지만 실제로는 인식과 기억의 요건을 맞추는 것이 그렇게 어려운 것은 아\n",
        "니다. 박테리아조차도 단 하나의 상대하고만 상호 작용한다거나, 단 하나의 전략을 가지고 상대방의 가장 최\n",
        "근 행위에 대해서만 반응하는 식으로 이런 요건을 충족시킨다. 박테리아가 할 수 있으면 사람도 할 수 있다.\n",
        "\n",
        "[new prompts (나)]\n",
        "인간의 노력을 조정하는 수단으로 경쟁의 힘을 최대한 잘 활용해야 한다. 이는 유효한 경쟁이 창출될 수\n",
        "있는 곳에서는 그 어떤 방법보다도 이것이 개별적 노력의 좋은 길잡이가 된다는 확신에 기초한 것이다. 나\n",
        "는 경쟁이 유익하게 작동하려면 세심하게 배려된 법적 틀이 필요하다는 사실을 부인하지 않는다. 또한 만약\n",
        "경쟁이 유효하게 이루어지도록 하는 조건을 충족시키기 어려운 경우에는 다른 방법을 경제활동의 길잡이로\n",
        "삼아야 한다는 것도 부정하지 않는다. 그러나 개인의 개별적 노력을 조정하는 방법으로 더욱 열등한 방법이\n",
        "경쟁을 대체하는 것에는 반대한다.\n",
        "그리고 알려진 방법 중 가장 효율적이라는 이유뿐만 아니라 더 크게는 권력의 강제적이고도 자의적인 간섭\n",
        "없이도 우리의 행위를 조정할 수 있는 유일한 방법이기 때문에 나는 경쟁을 우월한 방법으로 간주한다. 사\n",
        "실, 경쟁을 선호하는 핵심 이유 중 하나는 의식적인 사회적 통제가 필요하지 않다는 점이며, 특정한 직업이\n",
        "그 직업과 연관된 불리한 점과 위험 요소를 상쇄하고도 남을 만큼 전망이 있는지 스스로 판가름할 기회를\n",
        "각자에게 부여한다는 점이다. 경쟁을 사회 조직의 원칙으로 잘 활용하기 위해서는 강제적 간섭을 배제해야\n",
        "하지만 그 작동을 도와줄 수 있는 특정한 방식의 간섭은 인정할 수 있으며 정부의 적절한 조치가 필요한 경\n",
        "우도 있다. 그러나 강제력을 사용해서는 안 된다는 점은 특별히 강조해야 한다. 시장 참여자는 거래 상대방\n",
        "을 찾을 수 있는 한 어떤 가격에서건 자유롭게 팔고 살 수 있어야 하고, 누구든 자유롭게 생산할 수 있고,\n",
        "팔릴 수 있는 어떤 것도 생산하여 팔고 살 수 있어야 한다.\n",
        "업종에 상관없이 모든 사람에게 같은 조건으로 진입을 자유롭게 허용해야 한다는 것은 본질적으로 중요하\n",
        "다. 아울러 개인이나 단체가 공개적인 혹은 드러나지 않은 힘을 이용하여 이러한 진입을 제한하려는 것을\n",
        "법이 용인하지 않아야 한다는 것 또한 본질적으로 중요하다. 특정 상품에 대해 가격이나 물량을 통제하게\n",
        "되면 각자의 노력을 유효하게 조정하는 경쟁의 능력은 박탈된다. 왜냐하면 이 경우 가격의 변화가 변화된\n",
        "모든 상황을 더 이상 반영하지 못하게 되며, 그 결과 가격의 변화가 더 이상 개인의 행위를 나타내는 믿을\n",
        "만한 길잡이가 될 수 없기 때문이다.\n",
        "\n",
        "[new prompts (다)]\n",
        "전체 사회 구성원이 공동으로 소유한 산림과 바다 같은 공유 자원은 배제성은 없지만 경합성이 있다. 배제\n",
        "성이 없으므로 대가를 지불하지 않고도 누구나 공유 자원을 소비할 수 있는 반면, 경합성이 있으므로 누군\n",
        "가가 공유 자원을 소비하면 다른 사람의 소비 기회는 감소한다. 따라서 우리는 공유 자원을 다른 사람보다\n",
        "먼저 더 많이 소비하려고 경쟁한다. 이와 같은 특성 때문에 공유 자원은 쉽게 남용되어 필요한 양보다 과다\n",
        "하게 소비하게 되고 고갈될 가능성이 커진다. 이에 대해 일부 공동체주의자는 공동체적 해법이야말로 인간\n",
        "의 본성에 가장 걸맞은 자연스러운 해법이라고 주장한다. 이들에게 자연은 “적자생존” 혹은 “피로 물든 이빨\n",
        "과 발톱” 등으로 묘사되어온 생존 경쟁의 장이 아니다. 오히려 이들은 상호 부조의 감정이야말로 수십만 년\n",
        "에 걸친 집단생활을 통해 그리고 지난 수천 년의 사회생활을 통해 배양된 것이며, “전쟁터에서처럼 사람들이\n",
        "미쳐 돌아가는 상태가 아니라면” 상호 부조의 감정은 거스를 수 없는 법이라고 주장한다.\n",
        "그러나 인간은 언제나 공공의 이득을 위해서 행동할 준비가 되어 있는 존재는 아니다. 그렇기 때문에 인간\n",
        "본성에 기댈 수만은 없으며 적절한 유인이 필요하고 적절한 제도가 필요하다. 적절한 제도 하에서는 자신의\n",
        "이익과 공동의 이익을 조화시킬 수 있을 것이기 때문이다. 이때 적절한 제도란 물질적 이득을 추구하는 개\n",
        "인을 외적으로 그리고 위에서 아래로 강제하는 장치가 아니라 상호 신뢰를 통해 서로를 규제할 수 있는 견\n",
        "제 장치이다. 이것이 가능한 것은 인간은 협조하려는 상대방의 의지가 확인되면 언제든지 이에 협조로 응답\n",
        "하고, 자신의 행동이 타인에게 미치는 영향을 고려하며, 다른 누군가의 행동이 타인에게 해를 입히는 경우\n",
        "자기 일처럼 나서서 이를 제어하려는 의향을 가진 존재이기 때문이다.\n",
        "1960년대 오스트레일리아에서 바다가재를 잡는 지역의 어부들은 바다가재의 숫자가 계속 줄어드는 문제에\n",
        "직면해 있었다. 문제를 해결하기 위해 어부들은 좋은 생각을 해냈다. 공동체를 결성해서 설치할 수 있는 어\n",
        "망의 숫자를 제한하기로 한 것이다. 이에 다른 사람이 어망을 더 설치하는 것을 서로 감시하게 되면서 불법\n",
        "적으로 어망을 설치하는 어민이 사라졌고 결국 바다가재의 숫자를 늘리는 데 성공했다. 만약 정부에서 일방\n",
        "적으로 어망을 제한했다면 여러 나라에서 그랬던 것처럼 일부 어부가 경비정 몰래 바다가재를 제한 없이 잡\n",
        "다가 경비정이 쫓아오면 잡히지 않기 위해 그물을 끊고 도망가는 불법 조업이 기승을 부렸을 것이다. 그런\n",
        "데 어부들이 자율적으로 제한을 두면서 자연스럽게 불법 조업이 자취를 감추었다.\n",
        "공동체를 토대로 사회질서를 유지할 수 있다는 생각을 누군가는 낡은 이념으로, 다른 누군가는 유토피아적\n",
        "이념으로 여긴다. 그러나 많은 경우 공동체 구성원이 상호 감시하고 상호 제재하는 것이 공유 자원을 관리\n",
        "하는 효과적인 방법이 된다. 공동체 구성원은 외적 권위체가 갖지 못하는 정보를 갖고 있으며, 공동체 내에\n",
        "서 상호 신뢰를 기초로 서로를 규제해가면서 비극을 피할 수 있기 때문이다.\n",
        "\n",
        "[new prompts (라)]\n",
        "경쟁을 통해 각자의 능력이 최대한 발휘되기도 하지만 경쟁에서 진 쪽은 여러 가지 부작용을 겪기도 하는\n",
        "것이 사실이다. 경쟁이란 본질적으로 ‘차가운 것’이기 때문이다. 그래서 시장에서는 경쟁에서 지지 않기 위해\n",
        "경쟁관계의 두 기업이 일시적으로 협력하거나 다양한 방식의 협력관계를 경쟁 속에 부분적으로 도입하는 역\n",
        "설적인 경우를 볼 수 있다. 예를 들어 오랫동안 경쟁하다가도 경쟁자에게 부품을 공급하는 협력적 공급자로\n",
        "위치가 일시적으로 변화할 수 있다. 협력을 도입하여 경쟁의 형태가 간접적인 것으로 변화할 수도 있다. 예\n",
        "들 들어 한 기업이 다른 두 기업과 각각 독립적으로 파트너십을 맺고 협력하여 양쪽의 파트너 기업들이 서\n",
        "로 경쟁하는 구도가 만들어 질 수도 있다. 판매에서는 경쟁하지만 연구 및 개발에서는 잠정적으로 협력한다거\n",
        "나 아예 부서를 나누어서 어떤 부서에서는 협력을, 다른 부서에서는 경쟁을 진행하는 방식으로 부분적 협력이\n",
        "진행될 수도 있다. 특히 새로운 기술이 등장해서 기술 혹은 산업간 융합이 일어날 때, 그리고 기업 규모가\n",
        "작아서 경쟁력을 높여야 할 때 경쟁자와 협력하는 현상이 일어나기도 한다. 그럼에도 경쟁 메커니즘은 근본\n",
        "적으로 기업들이 경쟁에서 우위를 확보하기 위해 혁신에 더 집중하도록 유도한다. 일반적으로 기업들은 독\n",
        "점보다는 경쟁 상황일 때 연구개발 등 혁신 활동에 더 많이 투자할 유인을 갖기 때문이다.\n",
        "사회 전체 차원에서 보면 경쟁이 심해질수록 사회생활의 접촉대상이 되는 타인은 실재적 혹은 잠재적 경쟁\n",
        "대상으로만 간주될 것이며 이러한 사고가 팽배할 때 휴머니즘을 토대로 한 사회적 친화력을 견지하기는 어\n",
        "렵다. 그래서 경제사상가들은 경쟁이 국부의 증대에 기여하지만 사회적 동질성과 도덕을 잠식한다는 지적을\n",
        "자주 해 왔다. 이런 문제를 고려할 때 제기되는 과제는 한편으로는 경쟁의 기능을 계속 살리면서 다른 한편\n",
        "으로는 경쟁이 유발하는 비인도적인 측면을 약화시킬 수 있는 ‘황금의 중도’를 설정하는 일이다. 2차 세계대\n",
        "전 이후 독일을 위시한 몇몇 서구국가들이 도입한 사회적 시장경제 제도는 바로 그와 같은 사실을 배경으로\n",
        "형성된 것이었다. 효율성을 증대시키는 시장경제의 기능을 토대로 하면서 동시에 자본주의 사회에 내재하는\n",
        "차가움이나 사회·경제적 역기능을 극소화하자는 것이다.\n",
        "최근 들어 과거의 성장제일주의 정책을 수정하여 안정과 형평 내지는 삶의 질을 조화시키겠다고 선언하는\n",
        "국가를 쉽게 찾을 수 있다. 이를 위해서는 무엇보다도 사회의 기본적 가치에 대한 공감이 형성되지 않으면\n",
        "안 된다. 그렇지 않을 경우 표출되는 다양한 갈등은 사회적 제어능력을 벗어날 위험이 있기 때문이다. 공존\n",
        "원리로서의 공정한 경쟁을 통한 진보를 훼손하지 않으면서도 휴머니즘을 기저로 하는 사회적 삶의 실존과\n",
        "조화되는 규범을 마련하는 토론의 장에 모든 사회 구성원 혹은 이익집단의 참여가 필요한 이유도 여기에 있\n",
        "다. 최근 폭발적으로 분출되는 다양한 욕구·견해·주장 등은 아마도 새로운 질서창출을 위한 사회적 공감대\n",
        "형성의 불가피한 진통일 것이다.\n",
        "'''\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":100000,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":0.2,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seQVZNCqaQXD",
        "outputId": "82d88629-9310-4616-8a2e-23f91cdf7692"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     559.31 ms\n",
            "llama_print_timings:      sample time =    1011.34 ms /   417 runs   (    2.43 ms per token,   412.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3827.69 ms /  2410 tokens (    1.59 ms per token,   629.62 tokens per second)\n",
            "llama_print_timings:        eval time =   16850.85 ms /   416 runs   (   40.51 ms per token,    24.69 tokens per second)\n",
            "llama_print_timings:       total time =   22342.36 ms /  2826 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the question and answer in the format that you wanted.\n",
            "\n",
            "[질문 1]\n",
            "제시문(가)와 제시문(라)를 바탕으로, 예술과 경쟁에서의 협력과 경쟁의 장단점을 논해주세요.\n",
            "\n",
            "[답변 1]\n",
            "예술과 경쟁에서는 협력과 경쟁이 동시에 존재합니다. 협력은 서로 도움이 되고 상호 보완성도 존재하지만, 경쟁에서는 각자가 상대방을 이길려고 노력하며 차별화와 경쟁력을 추구하는 경향이 있습니다. 이러한 경쟁과 협상의 장단점을 분석하면, 경쟁에서는 혁신적인 전략과 기술, 그리고 마케팅 등을 통해 더욱 높은 성과를 얻을 수 있지만, 협상은 상호 협조와 협력을 통해 문제를 더욱 효과적으로 해결할 수 있습니다. 따라서, 어떤 상황에서 협력이나 경쟁이 더 유리하게 작용하는지에에 따라 선택해야 합니다.\n",
            "\n",
            "[질문 2]\n",
            "제시문(나)와 제시문(다)를 바탕으로, 공유 자원의 관리와 관련된 경쟁과 협력의 관계를 논의해주세요.\n",
            "\n",
            "[답변 2]\n",
            "공유 자원은 경쟁과 협력의 구도를 변화시키는 중요한 요소입니다. 사람들은 서로 공유된 자원을 활용하기 위해 경쟁적으로 행동하지만, 동시에 그 자원들이 고갈되어 더이상 사용할 수 없게 되면 협력적인 노력으로 이를 대처하려고 합니다. 이러한 증시와 협상의 순환은 자연스러운 인간 본성에 기인하며, 적절한 제도와 유인이 필요한 것입니다. 예를 들어, 어부들의 경우 바다가재의 수를 제한하여 불법적인 조제를 방지하고 자원을 효과적으로 관리했습니다.\n",
            "\n",
            "[질문 3]\n",
            "제시문(라)와 제시문(다)를 바탕으로, 경제 시장에서의 경쟁과 협력의 관계를 논의해주세요.\n",
            "\n",
            "[답변 3]\n",
            "경제 시장에서는 경쟁이 중요한 기능을 하지만, 동시에 협력과 사회적 연대는 필요한 것입니다. 경쟁은 기업들이 혁신적인 전략과 기술, 그리고 마케팅 등을 통해 더욱 높은 성과를 얻을 수 있도록 유도하며, 동시에 사회적 시장경제가 추구하는 것처럼 자본주의 사회의 모순을 완화시키는 역할을 합니다. 이를 위해 경제 주체들은 서로 협동하여 사회적인 문제를 해결하고, 동시에 경쟁적 기능을 통해 경제적인 발전을 이루기도 합니다.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e823fc784cf64a0fb9fb47d1818011be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96dba2fdf569486c8224953ecd0c5e70",
              "IPY_MODEL_5b807b083c8a4d76bfe5db09326f008d",
              "IPY_MODEL_654f8002e1104b3da8ca51b3e18ae03f"
            ],
            "layout": "IPY_MODEL_bfac115e9e9a49f282241fb0060eac44"
          }
        },
        "96dba2fdf569486c8224953ecd0c5e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e8fb9fb1d404d3db8247be78211d34c",
            "placeholder": "​",
            "style": "IPY_MODEL_7250e7dfea86462ebe9ef8d978cd02a9",
            "value": ""
          }
        },
        "5b807b083c8a4d76bfe5db09326f008d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dae105029f142fbbf2b9e74515f2108",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_822c26a0398545d68caddcd0670e21d0",
            "value": 0
          }
        },
        "654f8002e1104b3da8ca51b3e18ae03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc28e7f3c7e34767977e3df1410e1a02",
            "placeholder": "​",
            "style": "IPY_MODEL_7ad98a118928406dafaf69f7d79df16c",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "bfac115e9e9a49f282241fb0060eac44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e8fb9fb1d404d3db8247be78211d34c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7250e7dfea86462ebe9ef8d978cd02a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dae105029f142fbbf2b9e74515f2108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "822c26a0398545d68caddcd0670e21d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc28e7f3c7e34767977e3df1410e1a02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ad98a118928406dafaf69f7d79df16c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d8b832e57e04b0c933acec5dbc4a292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_074fdfa86e9c4ba1a2f930247a392e26",
              "IPY_MODEL_a804a2d29d654aceb08c4c52fb12c85a",
              "IPY_MODEL_1388f51cefd84128a9dca3b0f09d3cc4"
            ],
            "layout": "IPY_MODEL_791c13cd51c94f3ebbd7dc606e05126f"
          }
        },
        "074fdfa86e9c4ba1a2f930247a392e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5044916065f645238366487de5b1f9e0",
            "placeholder": "​",
            "style": "IPY_MODEL_763ffd41777c461bb12efc31490630a0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a804a2d29d654aceb08c4c52fb12c85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52303616d85641739bfa8c6043c36824",
            "max": 51264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31fc3c63801d42178c55be9962b6317f",
            "value": 51264
          }
        },
        "1388f51cefd84128a9dca3b0f09d3cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ef1a2028e4242a8a6aa5c25acad097e",
            "placeholder": "​",
            "style": "IPY_MODEL_86ae0c98cbcf4d2c8b699e4146a3fec1",
            "value": " 51.3k/51.3k [00:00&lt;00:00, 632kB/s]"
          }
        },
        "791c13cd51c94f3ebbd7dc606e05126f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5044916065f645238366487de5b1f9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763ffd41777c461bb12efc31490630a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52303616d85641739bfa8c6043c36824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31fc3c63801d42178c55be9962b6317f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ef1a2028e4242a8a6aa5c25acad097e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ae0c98cbcf4d2c8b699e4146a3fec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0c29e23f37b4eafa2b1c0bb684da4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85bf9e08e2394684a05ff9d0b8f0d6db",
              "IPY_MODEL_1aa2c657f3334d249ce779530680ace5",
              "IPY_MODEL_36cf0abf82874cd4b666b86e809887ff"
            ],
            "layout": "IPY_MODEL_2bf5d6be73ca4ff1aa5173c49d5b0d8a"
          }
        },
        "85bf9e08e2394684a05ff9d0b8f0d6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1128ef618c24e2694dc951bcc01a969",
            "placeholder": "​",
            "style": "IPY_MODEL_5713dc6a3c744c61a7dbafb25a0869fa",
            "value": "tokenizer.json: 100%"
          }
        },
        "1aa2c657f3334d249ce779530680ace5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fe119c27c7d4a88b7112c5fd0c50e32",
            "max": 10121378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73c00ba9ff2b48c29bca0042ce01456c",
            "value": 10121378
          }
        },
        "36cf0abf82874cd4b666b86e809887ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79cc20b38b5745c092f5b3c2dc8e762b",
            "placeholder": "​",
            "style": "IPY_MODEL_1c51e75de67f4649adfd3c647b67ccf4",
            "value": " 10.1M/10.1M [00:00&lt;00:00, 15.3MB/s]"
          }
        },
        "2bf5d6be73ca4ff1aa5173c49d5b0d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1128ef618c24e2694dc951bcc01a969": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5713dc6a3c744c61a7dbafb25a0869fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fe119c27c7d4a88b7112c5fd0c50e32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73c00ba9ff2b48c29bca0042ce01456c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79cc20b38b5745c092f5b3c2dc8e762b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c51e75de67f4649adfd3c647b67ccf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fcb8078d31a411baecb19b54d19128e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18977678045342d291d529c34cc878bb",
              "IPY_MODEL_aac60e0a7361480c8432d159848f1795",
              "IPY_MODEL_4219bb7da97943f5bcc66f2048e1f1f3"
            ],
            "layout": "IPY_MODEL_cd508c8745da4cc28085cc609b251421"
          }
        },
        "18977678045342d291d529c34cc878bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83321b4304db433499f64f81267550ab",
            "placeholder": "​",
            "style": "IPY_MODEL_1f9a51011b694a2f8e5a62cff952cb85",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "aac60e0a7361480c8432d159848f1795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9412ed7eb144aeb8bb37e3ce38338e9",
            "max": 507,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d7682123ae0458aa2b6a5cbe87d1d98",
            "value": 507
          }
        },
        "4219bb7da97943f5bcc66f2048e1f1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a57db9f89b74c3d95edc24033ef2c83",
            "placeholder": "​",
            "style": "IPY_MODEL_07f9c07a2f5c4ea29faa80a8230fcb9f",
            "value": " 507/507 [00:00&lt;00:00, 8.37kB/s]"
          }
        },
        "cd508c8745da4cc28085cc609b251421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83321b4304db433499f64f81267550ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9a51011b694a2f8e5a62cff952cb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9412ed7eb144aeb8bb37e3ce38338e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7682123ae0458aa2b6a5cbe87d1d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a57db9f89b74c3d95edc24033ef2c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f9c07a2f5c4ea29faa80a8230fcb9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}