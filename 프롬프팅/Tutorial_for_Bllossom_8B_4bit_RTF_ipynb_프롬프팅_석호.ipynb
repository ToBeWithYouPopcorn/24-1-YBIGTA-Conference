{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn28qgmjmpwU"
      },
      "source": [
        "# Bllossom-8B 양자화 모델을 이용한 한국어 LLM 튜토리얼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwtKm4_Wmud6"
      },
      "source": [
        "## 01. 활용할 package 설정\n",
        " - GPU사용하기: colab에서 런타임 --> 런타임유형변경 --> T4 선택\n",
        " - 패키지설치: 아래 pip를 이용해 Transformers, accelerate 설치\n",
        " - 런타임재시작: 런타임 --> 세션다시시작  (accelerate설치 시 런타임 다시시작하셔야됩니다!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5xmsZ2fOdAk",
        "outputId": "afd1bb8e-81cc-4c3f-fcfb-66e9e87afaef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.40.0\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 transformers-4.40.0\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=172351186 sha256=87fb6806a770dda0161ff1792ed083cacf7c7bee7e24a40da0126517b3163bcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\n",
            "Fetching 6 files:   0% 0/6 [00:00<?, ?it/s]Downloading 'special_tokens_map.json' to '.huggingface/download/special_tokens_map.json.86e4627c9ec4a278205b4598a034199e721e5bd2.incomplete'\n",
            "Downloading '.gitattributes' to '.huggingface/download/.gitattributes.4eba1088cbec5eae276a6990930ab6a5136103d1.incomplete'\n",
            "Downloading 'README.md' to '.huggingface/download/README.md.f68979286f9cb7e46e1baa6b217248f303f094f0.incomplete'\n",
            "Downloading 'tokenizer_config.json' to '.huggingface/download/tokenizer_config.json.6c407ae86d155b3c4100b980084c5c8105cec980.incomplete'\n",
            "Downloading 'tokenizer.json' to '.huggingface/download/tokenizer.json.1e08eea05dd130df12c7535a8f01dcb5426ea1fc.incomplete'\n",
            "Downloading 'llama-3-Korean-Bllossom-8B-Q4_K_M.gguf' to '.huggingface/download/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf.882dc22ffebe74c225373af08375017c4c529af439ccf6e0b5587188fb43d0bd.incomplete'\n",
            "\n",
            "special_tokens_map.json: 100% 507/507 [00:00<00:00, 3.11MB/s]\n",
            "Download complete. Moving file to special_tokens_map.json\n",
            "\n",
            "README.md:   0% 0.00/7.61k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "README.md: 100% 7.61k/7.61k [00:00<00:00, 6.54MB/s]\n",
            "Download complete. Moving file to README.md\n",
            "tokenizer_config.json: 100% 51.3k/51.3k [00:00<00:00, 65.9MB/s]\n",
            "Download complete. Moving file to tokenizer_config.json\n",
            "\n",
            "tokenizer.json:   0% 0.00/10.1M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            ".gitattributes: 100% 1.59k/1.59k [00:00<00:00, 13.5MB/s]\n",
            "Download complete. Moving file to .gitattributes\n",
            "Fetching 6 files:  17% 1/6 [00:00<00:04,  1.22it/s]\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   0% 0.00/5.02G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   0% 21.0M/5.02G [00:00<00:25, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   1% 62.9M/5.02G [00:00<00:15, 320MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   2% 105M/5.02G [00:00<00:14, 338MB/s] \u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   3% 147M/5.02G [00:00<00:15, 316MB/s]\u001b[A\u001b[A\n",
            "tokenizer.json: 100% 10.1M/10.1M [00:00<00:00, 14.2MB/s]\n",
            "Download complete. Moving file to tokenizer.json\n",
            "\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   4% 189M/5.02G [00:00<00:16, 291MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   4% 220M/5.02G [00:00<00:16, 292MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   5% 252M/5.02G [00:00<00:16, 281MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   6% 294M/5.02G [00:00<00:16, 295MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   6% 325M/5.02G [00:01<00:15, 298MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   7% 357M/5.02G [00:01<00:15, 299MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   8% 398M/5.02G [00:01<00:14, 310MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   9% 440M/5.02G [00:01<00:14, 315MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  10% 482M/5.02G [00:01<00:13, 340MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  10% 524M/5.02G [00:01<00:14, 307MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  11% 566M/5.02G [00:01<00:14, 304MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  12% 598M/5.02G [00:01<00:14, 297MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  13% 629M/5.02G [00:02<00:16, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  13% 661M/5.02G [00:02<00:16, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  14% 692M/5.02G [00:02<00:17, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  14% 724M/5.02G [00:02<00:16, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  15% 765M/5.02G [00:02<00:15, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  16% 797M/5.02G [00:04<01:29, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  17% 849M/5.02G [00:04<00:57, 72.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  18% 891M/5.02G [00:05<00:42, 96.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  19% 933M/5.02G [00:05<00:32, 125MB/s] \u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  19% 965M/5.02G [00:05<00:28, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  20% 996M/5.02G [00:05<00:24, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  20% 1.03G/5.02G [00:05<00:21, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  21% 1.07G/5.02G [00:05<00:18, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  22% 1.11G/5.02G [00:05<00:16, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  23% 1.15G/5.02G [00:05<00:14, 269MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  24% 1.20G/5.02G [00:05<00:13, 285MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  25% 1.24G/5.02G [00:06<00:12, 311MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  25% 1.28G/5.02G [00:06<00:12, 289MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  26% 1.32G/5.02G [00:06<00:12, 294MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  27% 1.36G/5.02G [00:06<00:12, 304MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  28% 1.41G/5.02G [00:06<00:11, 303MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  29% 1.44G/5.02G [00:08<01:09, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  29% 1.48G/5.02G [00:08<00:49, 71.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  30% 1.52G/5.02G [00:09<00:37, 92.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  31% 1.55G/5.02G [00:09<00:32, 108MB/s] \u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  32% 1.60G/5.02G [00:09<00:22, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  33% 1.65G/5.02G [00:09<00:18, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  34% 1.69G/5.02G [00:09<00:17, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  34% 1.72G/5.02G [00:09<00:15, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  35% 1.76G/5.02G [00:09<00:13, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  36% 1.80G/5.02G [00:10<00:12, 266MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  37% 1.85G/5.02G [00:10<00:13, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  37% 1.88G/5.02G [00:10<00:22, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  38% 1.91G/5.02G [00:10<00:21, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  38% 1.93G/5.02G [00:11<00:22, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  39% 1.95G/5.02G [00:11<00:23, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  39% 1.97G/5.02G [00:11<00:22, 133MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  40% 1.99G/5.02G [00:11<00:25, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  40% 2.01G/5.02G [00:11<00:27, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  41% 2.03G/5.02G [00:12<00:28, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  41% 2.06G/5.02G [00:12<00:25, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  41% 2.08G/5.02G [00:12<00:38, 76.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  42% 2.09G/5.02G [00:14<02:16, 21.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  43% 2.14G/5.02G [00:15<01:03, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  44% 2.19G/5.02G [00:15<00:38, 74.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  45% 2.23G/5.02G [00:15<00:27, 101MB/s] \u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  45% 2.26G/5.02G [00:15<00:22, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  46% 2.30G/5.02G [00:15<00:18, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  46% 2.33G/5.02G [00:15<00:16, 164MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  47% 2.36G/5.02G [00:19<01:33, 28.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  48% 2.41G/5.02G [00:19<00:56, 45.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  49% 2.45G/5.02G [00:19<00:40, 63.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  50% 2.49G/5.02G [00:19<00:32, 78.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  50% 2.52G/5.02G [00:19<00:26, 93.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  51% 2.55G/5.02G [00:19<00:22, 112MB/s] \u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  51% 2.58G/5.02G [00:19<00:19, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  52% 2.61G/5.02G [00:19<00:15, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  53% 2.64G/5.02G [00:20<00:14, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  53% 2.67G/5.02G [00:20<00:12, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  54% 2.71G/5.02G [00:20<00:12, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  55% 2.74G/5.02G [00:20<00:10, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  55% 2.77G/5.02G [00:20<00:09, 226MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  56% 2.80G/5.02G [00:20<00:09, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  57% 2.84G/5.02G [00:20<00:08, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  57% 2.87G/5.02G [00:21<00:09, 232MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  58% 2.90G/5.02G [00:21<00:08, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  59% 2.94G/5.02G [00:21<00:08, 256MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  59% 2.97G/5.02G [00:21<00:07, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  60% 3.00G/5.02G [00:21<00:07, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  60% 3.03G/5.02G [00:21<00:07, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  61% 3.07G/5.02G [00:21<00:07, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  62% 3.10G/5.02G [00:22<00:09, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  62% 3.14G/5.02G [00:22<00:19, 96.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  63% 3.16G/5.02G [00:22<00:19, 96.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  63% 3.18G/5.02G [00:23<00:17, 104MB/s] \u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  64% 3.20G/5.02G [00:23<00:17, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  64% 3.22G/5.02G [00:23<00:16, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  65% 3.25G/5.02G [00:23<00:13, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  66% 3.29G/5.02G [00:23<00:09, 175MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  66% 3.33G/5.02G [00:23<00:08, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  67% 3.37G/5.02G [00:24<00:07, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  68% 3.41G/5.02G [00:24<00:06, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  69% 3.44G/5.02G [00:24<00:06, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  69% 3.48G/5.02G [00:24<00:05, 289MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  70% 3.51G/5.02G [00:24<00:05, 291MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  71% 3.54G/5.02G [00:24<00:05, 288MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  71% 3.58G/5.02G [00:24<00:05, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  72% 3.62G/5.02G [00:24<00:04, 287MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  73% 3.65G/5.02G [00:24<00:04, 289MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  73% 3.68G/5.02G [00:25<00:04, 290MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  74% 3.71G/5.02G [00:25<00:05, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  75% 3.74G/5.02G [00:25<00:05, 214MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  75% 3.77G/5.02G [00:25<00:05, 233MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  76% 3.81G/5.02G [00:25<00:05, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  76% 3.84G/5.02G [00:25<00:05, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  77% 3.88G/5.02G [00:25<00:04, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  78% 3.91G/5.02G [00:26<00:05, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  79% 3.94G/5.02G [00:26<00:04, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  79% 3.98G/5.02G [00:26<00:04, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  80% 4.02G/5.02G [00:26<00:04, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  81% 4.06G/5.02G [00:26<00:03, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  82% 4.09G/5.02G [00:26<00:04, 224MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  82% 4.12G/5.02G [00:27<00:05, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  83% 4.15G/5.02G [00:27<00:04, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  83% 4.18G/5.02G [00:27<00:03, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  84% 4.23G/5.02G [00:27<00:03, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  85% 4.26G/5.02G [00:27<00:03, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  85% 4.29G/5.02G [00:27<00:03, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  86% 4.32G/5.02G [00:27<00:02, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  87% 4.36G/5.02G [00:28<00:02, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  88% 4.39G/5.02G [00:28<00:02, 280MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  88% 4.44G/5.02G [00:28<00:01, 297MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  89% 4.48G/5.02G [00:28<00:01, 305MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  90% 4.51G/5.02G [00:28<00:01, 284MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  91% 4.55G/5.02G [00:28<00:01, 301MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  91% 4.58G/5.02G [00:28<00:01, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  92% 4.61G/5.02G [00:29<00:01, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  93% 4.65G/5.02G [00:29<00:01, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  93% 4.68G/5.02G [00:29<00:01, 256MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  94% 4.71G/5.02G [00:29<00:01, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  94% 4.74G/5.02G [00:29<00:01, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  95% 4.77G/5.02G [00:32<00:06, 36.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  96% 4.79G/5.02G [00:33<00:07, 28.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  96% 4.82G/5.02G [00:33<00:04, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  97% 4.84G/5.02G [00:33<00:03, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  97% 4.87G/5.02G [00:33<00:02, 54.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  97% 4.89G/5.02G [00:34<00:02, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  98% 4.91G/5.02G [00:34<00:01, 70.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  98% 4.93G/5.02G [00:34<00:01, 73.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  99% 4.95G/5.02G [00:34<00:00, 80.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  99% 4.97G/5.02G [00:35<00:00, 86.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  99% 4.99G/5.02G [00:35<00:00, 84.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf: 100% 5.02G/5.02G [00:35<00:00, 142MB/s]\n",
            "Download complete. Moving file to llama-3-Korean-Bllossom-8B-Q4_K_M.gguf\n",
            "Fetching 6 files: 100% 6/6 [00:36<00:00,  6.04s/it]\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.40.0 accelerate\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python #양자화 구동을 위한 Llama C++ 설치\n",
        "!huggingface-cli download MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M --local-dir='./' #Bllossom모델 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.40.0 accelerate\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "T0ykhq0lQN82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb7a9fe-4bde-49a7-dba8-5f5176fc4f81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.40.0 in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.79)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "1jKCKrFTxG3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "a1caedb649a74137a76831fbcd08917c",
            "281046f3d7e04738ba0f3ba848fe512e",
            "ef0fea2b89904f38b2e69badaadfa858",
            "a5686137df784ed3b2fe39864eebb68e",
            "ae5b408552304b65aa1d4b6481bbc9d2",
            "d5947710c6d5489e8bfad5900ef80cd5",
            "c4968b7b40d24bf3b655e9b217f368e8",
            "bda2f2c14f3741769e5000e3c622b917",
            "4fdef88d01f6428baec65bc1f85dde7a",
            "455ee57f45944392bd90ab253f338991",
            "cad665c2c785409f866fca5e763377a6"
          ]
        },
        "outputId": "109e9097-508f-48eb-9489-4debc85b4696"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1caedb649a74137a76831fbcd08917c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irNZYVFGnZEs"
      },
      "source": [
        "## 02. 모델준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XbOA7HcbirUG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8f8d457476ed47f8b4128d4f79ecb73e",
            "a48e2a2a6b0e4723925394ca83a90795",
            "09ff9735e27a4cbab9ae404287dc94ce",
            "875507e10d784070a6ab3e2dc809e04b",
            "f0e4749785dd4f13b1cbcccca9e0d2a7",
            "17b10d84897d47e6b64d0e21424fbec9",
            "896ac4bbc3174d9da88c058fbc9b93ac",
            "51d9357e72434fbd9a9b624ce08540fb",
            "e2b916a5c73e40988f784b2a947ae07f",
            "404224048d5e4ed8a793acfaa9c37c5f",
            "c703b895f851440b8c7de8b09a638725",
            "de9ec131673148d8b58bf3bfba0ceb9c",
            "4b60ee19679146a3a767c0a8d9fe4624",
            "3d0566084a514ed0b77bc7f4d9e384e3",
            "7ac867e5fb98481aaa1238fafc8d3f3d",
            "cab462a7c2294ac18dff3bdb501e63b7",
            "29caafd86e274aaeb98acc0ec7676bd7",
            "9149994e17cd47aab0e6d6af02a62aad",
            "b716e339d38e4702a463db7029e4ee83",
            "236097df2bbc42aab85ec675fa036e87",
            "0e4a2c4cc5324d4697a755d496fbd886",
            "bf951aeed3994398909ff61064227706",
            "dcd68f28a70a426fabcc9c5bb68cf57d",
            "fb517459702c4831a4316c949845ab91",
            "0dce29e0c05d4236808f5cf0c448f799",
            "6b73d1e8b17142eca4e4742df587c2c4",
            "fe65e1401c8442f9b21fc3e0b1e45e36",
            "eceeb2adf2464eb68fbbead89a342cea",
            "82de6ea5dacf41478fa972d91b35b08e",
            "b2a13a6d4b7943b4a239542412b42d7f",
            "8fb8e679225545a1b052b9f1a0cf1d0a",
            "85cc4fa45a8447da8fb62417d281176a",
            "2ac16fee916743b4ba868be43a1202f0"
          ]
        },
        "outputId": "c7cda588-b5ce-4038-9604-846178ce57ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f8d457476ed47f8b4128d4f79ecb73e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/10.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de9ec131673148d8b58bf3bfba0ceb9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcd68f28a70a426fabcc9c5bb68cf57d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 145088\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,145088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,145088]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,145088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,296982]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 144782\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 144783\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens cache size = 306\n",
            "llm_load_vocab: token to piece cache size = 0.9314 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 145088\n",
            "llm_load_print_meta: n_merges         = 296982\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.17 B\n",
            "llm_load_print_meta: model size       = 4.66 GiB (4.91 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 144782 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 144783 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 144791 '<|eot_id|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   318.80 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4457.43 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 10016\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1252.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1252.00 MiB, K (f16):  626.00 MiB, V (f16):  626.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.55 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   677.57 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    27.57 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '144783', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '145088', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '144782', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ],
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = Llama(\n",
        "    model_path='./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=10000,\n",
        "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHcxdlTTnhqs"
      },
      "source": [
        "## 03. 추론"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ucuz86xhnhEd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \\\n",
        "'''\n",
        "Role: \"You are a teacher running a college entrance essay interview academy.\"\n",
        "Task: \"Your main job is to generate example questions from a set of example passages. The number of passages in the set and the number of questions are not fixed. I will show you an example set of passages and example questions. Based on this, you will generate questions from a new set of passages in a similar manner. Your task is to generate only the questions.\"\n",
        "'''\n",
        "\n",
        "instruction = 'Did you understand what I said?'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":512,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":0.2,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42tc6c73ZaB-",
        "outputId": "ae77c223-8c06-4e17-b6ae-a3ccadbff86a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     538.81 ms\n",
            "llama_print_timings:      sample time =     111.77 ms /    51 runs   (    2.19 ms per token,   456.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     262.38 ms /   111 tokens (    2.36 ms per token,   423.05 tokens per second)\n",
            "llama_print_timings:        eval time =    1483.32 ms /    50 runs   (   29.67 ms per token,    33.71 tokens per second)\n",
            "llama_print_timings:       total time =    1905.36 ms /   161 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I understand that you want me to generate example questions from a set of example passes, based on the style and format of the questions shown to me. I will do my best to come up with new questions for a similar set of passes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \\\n",
        "'''\n",
        "Here is a sample set of prompts given as an example. This set consists of three prompts.\n",
        "[제시문 (가)]\n",
        "인간은 타인의 시선에서 벗어나 있을 때 과연 윤리적일 수 있는가? 칸다울레스 왕이\n",
        "다스리는 리디아 왕국에 기게스라는 목동이 살았다. 기게스가 양을 치고 있던 어느 날 갑자기\n",
        "커다란 지진이 일어났다. 지진이 일어난 자리에는 땅이 갈라져 동굴이 생겼고, 기게스는\n",
        "호기심이 생겨 갈라진 동굴 안으로 들어갔다. 그는 동굴 안에서 거인의 시체가 놓여 있는 것을\n",
        "발견하였다. 시체의 손가락에는 금반지가 끼워져 있었다. 기게스는 그 반지를 빼 들고 밖으로\n",
        "나왔다. 그러다 우연히 자신이 끼고 있는 반지의 흠집 난 곳을 안쪽으로 돌리면 자신은 투명\n",
        "인간이 되고 바깥쪽으로 돌리면 자기 모습이 다시 보인다는 사실을 알게 되었다. 이제 남들의\n",
        "시선에서 벗어나 보이지 않는 힘을 갖게 된 기게스는 자연스럽게 나쁜 마음을 먹게 되었다.\n",
        "가축의 상태를 왕에게 보고하는 전령으로서 궁전에 들어간 기게스는 자신의 새로운 힘인 마법\n",
        "반지를 이용하여 모습을 감춘 후, 왕비를 겁탈하고 그녀를 자기 편으로 끌어들여 왕을 암살한\n",
        "뒤 스스로 왕이 되었다.\n",
        "\n",
        "[제시문 (나)]\n",
        "서로 잘 아는 사람들이 소규모로 모여 사는 마을 공동체에서 개인은 대체로 합리적이고\n",
        "도덕적인 성향을 보인다. 그는 다른 사람들의 말에 크게 영향받기보다는, 이성과 주관에 따라\n",
        "판단하고 규범에 맞게 행동한다. 혼자 있을 때도, 여럿이 있을 때와 크게 다를 바 없이\n",
        "처신한다. 그런데 19세기 이후 대도시에 인구가 밀집하고 대량생산과 소비, 그리고 대중문화가\n",
        "발전하면서 대중사회가 등장한다. 이제 대중 속에서 이름 없는 한 명이 된 개인은 집단적인\n",
        "분위기에 복종하고 전체의 결정을 따르라는 무언의 압력에 쉽사리 굴복한다. 대중의\n",
        "일원으로서 그는 익명성 아래 자신의 욕망, 정열, 관심을 분출하고 실현한다. 이때 권력을 가진\n",
        "지도자의 역할은 결정적이다. 권위적 지도자는 단순하고 선동적인 말로 대중에게 행동 방향을\n",
        "제시하며, 대중은 지도자의 말을 마치 절대적인 진리인 것처럼 이해한다. 대중은 직관과 감정에\n",
        "따라 권위적 지도자의 말을 무비판적으로 수용한다. 무리 속의 익명적 개인들은 쉽게 흥분하고\n",
        "변덕을 부리며 열정을 드러낸다. 그러한 감정 에너지는 때로 대중이 난폭하게 폭력을\n",
        "행사한다든지, 용감하게 순교를 불사하는 등의 행동을 하도록 만든다.\n",
        "\n",
        "\n",
        "[제시문 (다)]\n",
        "도시 문명의 발전은 현대 사회의 주요 특징이다. 과학과 기술의 진전을 통해 이루어진 도시\n",
        "화는 익명성이 전통에 따르는 도덕규범과 오랜 대면 관계를 대신하도록 만들었다. 그런데 우리\n",
        "는 도시인의 생존이 도시의 잔인한 익명성 탓에 소모되고 훼손된다는 말을 자주 듣는다. 개인\n",
        "은 작은 시골 마을에 있을 때 이미 정해진 행동규범을 따르며, 스스로 그것을 의무로 여겼다.\n",
        "그러한 규범을 어길 때 마을에서 평판이 나빠지기 때문이다. 하지만 그는 누가 누군지 알 수\n",
        "없는 대도시로 나오면서부터 깊은 인간관계를 쌓을 수 없게 된다는 것이다. 이런 까닭에\n",
        "대중사회 속의 도시인은 마치 정체성을 상실하고 자아를 잃어버린 채로 살아간다고\n",
        "비판받기까지 한다.\n",
        "그러나 이처럼 도시의 익명성이 과거 마을 공동체 시절의 인간적 교류를 사라지게 했다는\n",
        "비판은 그 익명성이 지니는 독특한 이점을 보지 못한 데서 나온다. 도시의 익명성은 마을\n",
        "공동체의 넌더리 나는 속박에서 벗어나는 자유의 가능성을 제공하므로 위협적이고 피폐한 것이\n",
        "아니라 훨씬 더 인간적이고 해방적인 현상이다. 왜냐하면 도시 생활의 익명성 형태는 인간 삶에\n",
        "필수적인 사생활을 보호하는 데 도움을 주며 도시인을 마을 생활의 부담스러운 도덕규범과\n",
        "강요된 인습이라는 족쇄에서 벗어나도록 만들기 때문이다. 따라서 도시인은 이런 익명성 덕택에\n",
        "과거와는 비교할 수 없을 만큼 많은 사람과 다양한 교류를 할 수 있게 되었을 뿐만 아니라\n",
        "자유롭고 창의적인 생각을 펼칠 수 있게 되었다.\n",
        "\n",
        "\n",
        "Next, I will show you the example questions. These questions were generated from the sample set of prompts above.\n",
        "\n",
        "\n",
        "[예시 질문]\n",
        "제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.\n",
        "\n",
        "\n",
        "In this way, the example questions reference the content of the sample prompts, summarizing them or asking about topics that can be considered from them. When creating questions, it's important to consider the relationships between the contents of each prompt.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Please create questions and answers based on the following prompts. You need to generate questions from the [new prompts]. When generating questions, the content should not imply or suggest the answers. Please delete any unnecessary sentences. The format should be as follows:\n",
        "\n",
        "[\"Question 1\"\n",
        "\"Question 2\"\n",
        "\"Question 3\"\n",
        "\"Answer 1\"\n",
        "\"Answer 2\"\n",
        "'Answer 3\"]\n",
        "\n",
        "Please create two questions and two answers. As I said, when generating questions, the content should not imply or suggest the answers. When creating questions, it's important to consider the relationships between the contents of each prompt. The questions should be in Korean, and concise, consisting of 1-2 sentences.\n",
        "\n",
        "\n",
        "[new prompts(가)]\n",
        "이타주의란 자신에게 발생할 손해를 감수하고 타인에게 이득을 주려는 생각 또는 행동을\n",
        "뜻한다. 이러한 이타주의에 대해서 다양한 논의가 이루어졌다. 이타주의는 거대한 규모의\n",
        "현대 자본주의 사회에는 더 이상 어울리지 않는 낡은 덕목에 지나지 않는다는 (a)견해가 있다. 이에 따르면 현대 사회에서는 누군가를 위해서가 아니라 자신만을 위해서 행동한 결과 대규모 협력이 이루어진다. 우리가 시장에서 구매해서 사용하고 있는 물건을 보자. 이들\n",
        "대부분은 살아가면서 한 번도 마주칠 일이 없는 저 어딘가에 있는 누군가가 ‘자신의 이익을\n",
        "위해서’ 만든 것이다. 이처럼 시장이라는 메커니즘은 우리 눈에 보이지 않는 누군가의 필요도 충족시켜 주고, 자신의 이익만을 추구하면서 행동하더라도 결과적으로는 사회 전체의 복지를 증진시키는 대규모 협력체계를 만들어 낸다. 따라서 이기심이라는 강력한 충동 앞에서 무너지기 쉬운 이타주의에 기초를 둘 것이 아니라, 이기심만으로도 시장이 잘\n",
        "작동할 수 있게 하는 법적·제도적 장치를 고안하는 것이 더 중요하다. 한편 생물학자들은 우리의 이기심이 유전자에 각인된 것이라고 본다. 이 (b)견해에 따르면 유전자가 살아남기 위한 자기복제의 메커니즘이 모든 생명체가 가진 생족 본능, 즉\n",
        "이기심의 뿌리다. 하지만 그들은 이기심이 이타주의와 모순된다고 보는 것은 아니며 진화를 통해 이타주의가 형성되었다는 점을 인정한다. 어떤 성향이 진화한다는 것은 그 성향을 가짐으로써 그렇지 않은 경우에 비해, 자손의 수든 물질적 이득의 크기든, 더 큰\n",
        "보상을 받게 된다는 것을 의미한다. 이타주의는 사회에서 수행해야 할 역할이 있기 때문에 진화 과정 속에서 살아남을 수 있었다.\n",
        "\n",
        "\n",
        "[new prompts(나)]\n",
        "최근 미국 캘리포니아 대학의 한 연구팀은 취학 전인 네 살 남녀 어린이 총 74명을 대상으로 실험을 실시했다. 먼저 아이들이 다양한 게임을 통해 총 20개의 교환권을 획득할수 있게 했다. 게임이 끝나 갈 때 쯤 교환권을 선물로 바꿀 수 있다는 것을 알려 준 후, 아파서 게임에 참여하지 못한 친구들에게 교환권을 기부할 수 있다고 말해 주었다. 이후\n",
        "연구팀은 비밀 공간에 마련된 상자에 어린이들이 각자 알아서 교환권을 넣도록 했다. 그 결과 총 74명의 어린이 중 40명이 적어도 한 개 이상의 교환권을 기부했으며 한 개도\n",
        "넣지 않은 아이도 34명에 달했다. 이번 연구는 흥미로운 점이 있다. 실험 전에 어린이들에게 부착한 전극으로 미주신경(vagus nerve)을 모니터한 결과, 기부를 많이 한 어린이의 경우 미주신경이 더욱 활성화되는\n",
        "것으로 나타난 것이다. 연구를 이끈 밀러 박사는 “미주신경은 신체에서 가장 광범위하게 부교감신경을 지배하는데, 심장 박동과 스트레스를 안정적으로 제어한다.”면서 “우리 감정을편안하고 안정적으로 이끌어 건강을 유지하고 원활한 사회적 관계를 맺는 데 도움이 된다.” 고\n",
        "설명했다.\n",
        "'''\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":512,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":0.2,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seQVZNCqaQXD",
        "outputId": "55cc88eb-9c3a-48e0-b705-a3e35af6a59c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     538.81 ms\n",
            "llama_print_timings:      sample time =     633.11 ms /   266 runs   (    2.38 ms per token,   420.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     942.99 ms /   611 tokens (    1.54 ms per token,   647.94 tokens per second)\n",
            "llama_print_timings:        eval time =    9344.81 ms /   265 runs   (   35.26 ms per token,    28.36 tokens per second)\n",
            "llama_print_timings:       total time =   11253.26 ms /   876 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1]\n",
            "이타주의에 대한 다양한 견해가 있다. 이타주의는 어떤 사회적 현상이라고 볼 수 있고, 어떤 문제점들을 가지고 있나요? \n",
            "\n",
            "[답변 1]\n",
            "이타주의는 현대 사회에서 존재하며, 우리 눈에 보이지 않는 다른 사람(특히 자본주의 사회에서 기업 등)의 이익을 위해서 행동하는 것을 의미한다. 이러한 현상은 큰 규모의 협력이 이루어지며, 개인의 이기심 때문에 발생한다고 볼 수 있다. 이는 도시생활과 같은 환경에서는 익명성과 비인가족 관계가 더욱 뚜렷하게 나타나며, 이로 인해 불만족감과 외로움을 느끼게 된다.\n",
            "\n",
            "[질문 2]\n",
            "이타주의와 관련된 다양한 관점을 고려할 때, 어떻게 이를 해결할 수 있다고 생각하시나요? \n",
            "\n",
            "[답변 2]\n",
            "이를 해결하기 위해서는 이기심 자체가 아닌 다른 것을 고려해야 한다. 예를 들어, 시장이라는 메커니즘에서 자신에게 이익을 가져다 주는 것이 아니라 타인을 위해서 행동하게 되면 사회 전체의 복지를 증진시키는 대규모 협력체제를 만들 수 있다. 또한, 유전자에 각인된 것이라는 생물학자들의 견해도 있는데, 이타주의는 생존을 위한 자기복제의 메커니즘이며, 이를 이해하는 것이 필요하다. 이러한 해결책들은 이타주의에 대한 새로운 시각과 함께, 사회적 발전과 협력으로 이어질 수 있다.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1caedb649a74137a76831fbcd08917c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_281046f3d7e04738ba0f3ba848fe512e",
              "IPY_MODEL_ef0fea2b89904f38b2e69badaadfa858",
              "IPY_MODEL_a5686137df784ed3b2fe39864eebb68e"
            ],
            "layout": "IPY_MODEL_ae5b408552304b65aa1d4b6481bbc9d2"
          }
        },
        "281046f3d7e04738ba0f3ba848fe512e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5947710c6d5489e8bfad5900ef80cd5",
            "placeholder": "​",
            "style": "IPY_MODEL_c4968b7b40d24bf3b655e9b217f368e8",
            "value": ""
          }
        },
        "ef0fea2b89904f38b2e69badaadfa858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bda2f2c14f3741769e5000e3c622b917",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fdef88d01f6428baec65bc1f85dde7a",
            "value": 0
          }
        },
        "a5686137df784ed3b2fe39864eebb68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_455ee57f45944392bd90ab253f338991",
            "placeholder": "​",
            "style": "IPY_MODEL_cad665c2c785409f866fca5e763377a6",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "ae5b408552304b65aa1d4b6481bbc9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5947710c6d5489e8bfad5900ef80cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4968b7b40d24bf3b655e9b217f368e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bda2f2c14f3741769e5000e3c622b917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4fdef88d01f6428baec65bc1f85dde7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "455ee57f45944392bd90ab253f338991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad665c2c785409f866fca5e763377a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f8d457476ed47f8b4128d4f79ecb73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a48e2a2a6b0e4723925394ca83a90795",
              "IPY_MODEL_09ff9735e27a4cbab9ae404287dc94ce",
              "IPY_MODEL_875507e10d784070a6ab3e2dc809e04b"
            ],
            "layout": "IPY_MODEL_f0e4749785dd4f13b1cbcccca9e0d2a7"
          }
        },
        "a48e2a2a6b0e4723925394ca83a90795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b10d84897d47e6b64d0e21424fbec9",
            "placeholder": "​",
            "style": "IPY_MODEL_896ac4bbc3174d9da88c058fbc9b93ac",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "09ff9735e27a4cbab9ae404287dc94ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51d9357e72434fbd9a9b624ce08540fb",
            "max": 51264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2b916a5c73e40988f784b2a947ae07f",
            "value": 51264
          }
        },
        "875507e10d784070a6ab3e2dc809e04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_404224048d5e4ed8a793acfaa9c37c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_c703b895f851440b8c7de8b09a638725",
            "value": " 51.3k/51.3k [00:00&lt;00:00, 926kB/s]"
          }
        },
        "f0e4749785dd4f13b1cbcccca9e0d2a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17b10d84897d47e6b64d0e21424fbec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896ac4bbc3174d9da88c058fbc9b93ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51d9357e72434fbd9a9b624ce08540fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b916a5c73e40988f784b2a947ae07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "404224048d5e4ed8a793acfaa9c37c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c703b895f851440b8c7de8b09a638725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de9ec131673148d8b58bf3bfba0ceb9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b60ee19679146a3a767c0a8d9fe4624",
              "IPY_MODEL_3d0566084a514ed0b77bc7f4d9e384e3",
              "IPY_MODEL_7ac867e5fb98481aaa1238fafc8d3f3d"
            ],
            "layout": "IPY_MODEL_cab462a7c2294ac18dff3bdb501e63b7"
          }
        },
        "4b60ee19679146a3a767c0a8d9fe4624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29caafd86e274aaeb98acc0ec7676bd7",
            "placeholder": "​",
            "style": "IPY_MODEL_9149994e17cd47aab0e6d6af02a62aad",
            "value": "tokenizer.json: 100%"
          }
        },
        "3d0566084a514ed0b77bc7f4d9e384e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b716e339d38e4702a463db7029e4ee83",
            "max": 10121378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_236097df2bbc42aab85ec675fa036e87",
            "value": 10121378
          }
        },
        "7ac867e5fb98481aaa1238fafc8d3f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e4a2c4cc5324d4697a755d496fbd886",
            "placeholder": "​",
            "style": "IPY_MODEL_bf951aeed3994398909ff61064227706",
            "value": " 10.1M/10.1M [00:00&lt;00:00, 11.5MB/s]"
          }
        },
        "cab462a7c2294ac18dff3bdb501e63b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29caafd86e274aaeb98acc0ec7676bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9149994e17cd47aab0e6d6af02a62aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b716e339d38e4702a463db7029e4ee83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "236097df2bbc42aab85ec675fa036e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e4a2c4cc5324d4697a755d496fbd886": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf951aeed3994398909ff61064227706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcd68f28a70a426fabcc9c5bb68cf57d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb517459702c4831a4316c949845ab91",
              "IPY_MODEL_0dce29e0c05d4236808f5cf0c448f799",
              "IPY_MODEL_6b73d1e8b17142eca4e4742df587c2c4"
            ],
            "layout": "IPY_MODEL_fe65e1401c8442f9b21fc3e0b1e45e36"
          }
        },
        "fb517459702c4831a4316c949845ab91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eceeb2adf2464eb68fbbead89a342cea",
            "placeholder": "​",
            "style": "IPY_MODEL_82de6ea5dacf41478fa972d91b35b08e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "0dce29e0c05d4236808f5cf0c448f799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2a13a6d4b7943b4a239542412b42d7f",
            "max": 507,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fb8e679225545a1b052b9f1a0cf1d0a",
            "value": 507
          }
        },
        "6b73d1e8b17142eca4e4742df587c2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85cc4fa45a8447da8fb62417d281176a",
            "placeholder": "​",
            "style": "IPY_MODEL_2ac16fee916743b4ba868be43a1202f0",
            "value": " 507/507 [00:00&lt;00:00, 9.78kB/s]"
          }
        },
        "fe65e1401c8442f9b21fc3e0b1e45e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eceeb2adf2464eb68fbbead89a342cea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82de6ea5dacf41478fa972d91b35b08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2a13a6d4b7943b4a239542412b42d7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fb8e679225545a1b052b9f1a0cf1d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85cc4fa45a8447da8fb62417d281176a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac16fee916743b4ba868be43a1202f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}