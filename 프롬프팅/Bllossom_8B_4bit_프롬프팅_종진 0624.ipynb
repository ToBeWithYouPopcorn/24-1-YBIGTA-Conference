{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn28qgmjmpwU"
      },
      "source": [
        "# Bllossom-8B 양자화 모델을 이용한 한국어 LLM 튜토리얼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwtKm4_Wmud6"
      },
      "source": [
        "## 01. 활용할 package 설정\n",
        " - GPU사용하기: colab에서 런타임 --> 런타임유형변경 --> T4 선택\n",
        " - 패키지설치: 아래 pip를 이용해 Transformers, accelerate 설치\n",
        " - 런타임재시작: 런타임 --> 세션다시시작  (accelerate설치 시 런타임 다시시작하셔야됩니다!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5xmsZ2fOdAk",
        "outputId": "14baebae-38b6-409e-e946-df277803ea1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.40.0\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 transformers-4.40.0\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=172352912 sha256=29c60b6cfd82870298a55ed4f4cd2246a34bace058a461a54c99c742d98dc494\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\n",
            "Fetching 6 files:   0% 0/6 [00:00<?, ?it/s]Downloading 'llama-3-Korean-Bllossom-8B-Q4_K_M.gguf' to '.huggingface/download/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf.882dc22ffebe74c225373af08375017c4c529af439ccf6e0b5587188fb43d0bd.incomplete'\n",
            "Downloading 'tokenizer_config.json' to '.huggingface/download/tokenizer_config.json.6c407ae86d155b3c4100b980084c5c8105cec980.incomplete'\n",
            "Downloading '.gitattributes' to '.huggingface/download/.gitattributes.4eba1088cbec5eae276a6990930ab6a5136103d1.incomplete'\n",
            "Downloading 'README.md' to '.huggingface/download/README.md.f68979286f9cb7e46e1baa6b217248f303f094f0.incomplete'\n",
            "Downloading 'tokenizer.json' to '.huggingface/download/tokenizer.json.1e08eea05dd130df12c7535a8f01dcb5426ea1fc.incomplete'\n",
            "Downloading 'special_tokens_map.json' to '.huggingface/download/special_tokens_map.json.86e4627c9ec4a278205b4598a034199e721e5bd2.incomplete'\n",
            "\n",
            "tokenizer_config.json:   0% 0.00/51.3k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "README.md: 100% 7.61k/7.61k [00:00<00:00, 37.7MB/s]\n",
            "Download complete. Moving file to README.md\n",
            "\n",
            "\n",
            ".gitattributes: 100% 1.59k/1.59k [00:00<00:00, 15.4MB/s]\n",
            "Download complete. Moving file to .gitattributes\n",
            "Fetching 6 files:  17% 1/6 [00:00<00:02,  2.23it/s]\n",
            "\n",
            "tokenizer_config.json: 100% 51.3k/51.3k [00:00<00:00, 917kB/s]\n",
            "Download complete. Moving file to tokenizer_config.json\n",
            "\n",
            "special_tokens_map.json: 100% 507/507 [00:00<00:00, 3.93MB/s]\n",
            "Download complete. Moving file to special_tokens_map.json\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   0% 0.00/5.02G [00:00<?, ?B/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   0% 21.0M/5.02G [00:00<00:31, 159MB/s]\u001b[A\n",
            "\n",
            "tokenizer.json: 100% 10.1M/10.1M [00:00<00:00, 21.7MB/s]\n",
            "Download complete. Moving file to tokenizer.json\n",
            "\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   1% 52.4M/5.02G [00:00<00:20, 240MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   2% 83.9M/5.02G [00:00<00:18, 270MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   2% 115M/5.02G [00:00<00:17, 285MB/s] \u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   3% 157M/5.02G [00:00<00:16, 302MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   4% 189M/5.02G [00:00<00:15, 303MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   5% 231M/5.02G [00:00<00:15, 306MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   5% 273M/5.02G [00:00<00:15, 313MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   6% 315M/5.02G [00:01<00:14, 317MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   7% 357M/5.02G [00:01<00:14, 315MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   8% 398M/5.02G [00:01<00:15, 301MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   9% 440M/5.02G [00:01<00:15, 302MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:   9% 472M/5.02G [00:01<00:14, 304MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  10% 503M/5.02G [00:01<00:15, 299MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  11% 535M/5.02G [00:01<00:15, 297MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  11% 566M/5.02G [00:01<00:15, 295MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  12% 598M/5.02G [00:02<00:14, 299MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  13% 629M/5.02G [00:02<00:14, 303MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  13% 661M/5.02G [00:02<00:14, 306MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  14% 703M/5.02G [00:02<00:13, 311MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  15% 734M/5.02G [00:02<00:13, 309MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  15% 765M/5.02G [00:02<00:13, 305MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  16% 797M/5.02G [00:02<00:14, 297MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  17% 828M/5.02G [00:02<00:14, 294MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  17% 860M/5.02G [00:02<00:14, 295MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  18% 891M/5.02G [00:02<00:13, 300MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  18% 923M/5.02G [00:03<00:13, 297MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  19% 954M/5.02G [00:03<00:13, 302MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  20% 986M/5.02G [00:03<00:13, 301MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  20% 1.03G/5.02G [00:03<00:12, 309MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  21% 1.07G/5.02G [00:03<00:12, 317MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  22% 1.11G/5.02G [00:03<00:12, 316MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  23% 1.15G/5.02G [00:03<00:12, 313MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  24% 1.18G/5.02G [00:03<00:12, 312MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  24% 1.22G/5.02G [00:04<00:12, 312MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  25% 1.25G/5.02G [00:04<00:12, 305MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  25% 1.28G/5.02G [00:04<00:12, 302MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  26% 1.31G/5.02G [00:04<00:12, 305MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  27% 1.34G/5.02G [00:04<00:11, 308MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  27% 1.37G/5.02G [00:04<00:11, 306MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  28% 1.41G/5.02G [00:04<00:11, 308MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  29% 1.45G/5.02G [00:04<00:11, 314MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  30% 1.49G/5.02G [00:04<00:11, 315MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  31% 1.53G/5.02G [00:05<00:10, 319MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  31% 1.57G/5.02G [00:05<00:10, 319MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  32% 1.61G/5.02G [00:05<00:10, 322MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  33% 1.66G/5.02G [00:05<00:10, 320MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  34% 1.70G/5.02G [00:05<00:10, 321MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  35% 1.74G/5.02G [00:05<00:10, 327MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  36% 1.78G/5.02G [00:05<00:09, 325MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  36% 1.82G/5.02G [00:05<00:09, 326MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  37% 1.87G/5.02G [00:06<00:09, 324MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  38% 1.91G/5.02G [00:06<00:09, 324MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  39% 1.95G/5.02G [00:06<00:09, 324MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  40% 1.99G/5.02G [00:06<00:09, 327MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  41% 2.03G/5.02G [00:06<00:09, 329MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  41% 2.08G/5.02G [00:06<00:08, 330MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  42% 2.12G/5.02G [00:06<00:08, 328MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  43% 2.16G/5.02G [00:06<00:08, 327MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  44% 2.20G/5.02G [00:07<00:08, 329MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  45% 2.24G/5.02G [00:07<00:08, 331MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  46% 2.29G/5.02G [00:07<00:08, 334MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  46% 2.33G/5.02G [00:07<00:08, 328MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  47% 2.37G/5.02G [00:07<00:08, 328MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  48% 2.41G/5.02G [00:07<00:07, 327MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  49% 2.45G/5.02G [00:07<00:07, 327MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  50% 2.50G/5.02G [00:07<00:07, 328MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  51% 2.54G/5.02G [00:08<00:07, 315MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  51% 2.58G/5.02G [00:08<00:07, 313MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  52% 2.61G/5.02G [00:08<00:07, 308MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  53% 2.64G/5.02G [00:08<00:07, 308MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  53% 2.68G/5.02G [00:08<00:07, 312MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  54% 2.73G/5.02G [00:08<00:07, 317MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  55% 2.77G/5.02G [00:08<00:07, 318MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  56% 2.81G/5.02G [00:08<00:06, 320MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  57% 2.85G/5.02G [00:09<00:06, 326MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  58% 2.89G/5.02G [00:09<00:06, 325MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  59% 2.94G/5.02G [00:09<00:06, 331MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  59% 2.98G/5.02G [00:09<00:06, 331MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  60% 3.02G/5.02G [00:09<00:06, 330MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  61% 3.06G/5.02G [00:09<00:05, 332MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  62% 3.10G/5.02G [00:09<00:05, 328MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  63% 3.15G/5.02G [00:10<00:05, 329MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  64% 3.19G/5.02G [00:10<00:05, 329MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  64% 3.23G/5.02G [00:10<00:05, 331MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  65% 3.27G/5.02G [00:10<00:05, 332MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  66% 3.31G/5.02G [00:10<00:05, 329MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  67% 3.36G/5.02G [00:10<00:05, 326MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  68% 3.40G/5.02G [00:10<00:05, 322MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  69% 3.44G/5.02G [00:10<00:04, 323MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  69% 3.48G/5.02G [00:11<00:04, 325MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  70% 3.52G/5.02G [00:11<00:04, 321MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  71% 3.57G/5.02G [00:11<00:04, 320MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  72% 3.61G/5.02G [00:11<00:04, 317MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  73% 3.65G/5.02G [00:11<00:04, 306MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  73% 3.68G/5.02G [00:11<00:04, 307MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  74% 3.71G/5.02G [00:11<00:04, 306MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  75% 3.74G/5.02G [00:11<00:04, 303MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  75% 3.77G/5.02G [00:11<00:04, 305MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  76% 3.81G/5.02G [00:12<00:03, 306MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  76% 3.84G/5.02G [00:12<00:03, 304MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  77% 3.87G/5.02G [00:12<00:03, 303MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  78% 3.90G/5.02G [00:12<00:03, 302MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  79% 3.94G/5.02G [00:12<00:03, 306MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  79% 3.98G/5.02G [00:12<00:03, 311MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  80% 4.03G/5.02G [00:12<00:03, 314MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  81% 4.07G/5.02G [00:12<00:02, 318MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  82% 4.11G/5.02G [00:13<00:02, 314MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  83% 4.15G/5.02G [00:13<00:02, 320MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  84% 4.19G/5.02G [00:13<00:02, 318MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  84% 4.24G/5.02G [00:13<00:02, 315MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  85% 4.28G/5.02G [00:13<00:02, 317MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  86% 4.32G/5.02G [00:13<00:02, 320MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  87% 4.36G/5.02G [00:13<00:02, 321MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  88% 4.40G/5.02G [00:13<00:01, 323MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  89% 4.45G/5.02G [00:14<00:01, 320MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  89% 4.49G/5.02G [00:14<00:01, 323MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  90% 4.53G/5.02G [00:14<00:01, 326MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  91% 4.57G/5.02G [00:14<00:01, 326MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  92% 4.61G/5.02G [00:14<00:01, 331MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  93% 4.66G/5.02G [00:14<00:01, 329MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  94% 4.70G/5.02G [00:14<00:00, 334MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  94% 4.74G/5.02G [00:15<00:00, 331MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  95% 4.78G/5.02G [00:15<00:00, 329MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  96% 4.82G/5.02G [00:15<00:00, 317MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  97% 4.87G/5.02G [00:15<00:00, 311MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  98% 4.91G/5.02G [00:15<00:00, 317MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf:  99% 4.95G/5.02G [00:15<00:00, 319MB/s]\u001b[A\n",
            "llama-3-Korean-Bllossom-8B-Q4_K_M.gguf: 100% 5.02G/5.02G [00:15<00:00, 315MB/s]\n",
            "Download complete. Moving file to llama-3-Korean-Bllossom-8B-Q4_K_M.gguf\n",
            "Fetching 6 files: 100% 6/6 [00:16<00:00,  2.77s/it]\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.40.0 accelerate\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python #양자화 구동을 위한 Llama C++ 설치\n",
        "!huggingface-cli download MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M --local-dir='./' #Bllossom모델 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MblLMViWihFO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "b0dcfdbc280148c1936dfd7a6777cb42",
            "4fd957d4fc8b4109a5590e5dca3467da",
            "e565e1fbf1924ccda28c80c6c49dc4b5",
            "c9211a3f5b244d3eadcf424b3155ede3",
            "5ce85ae253db4912a68a315c1e95c21b",
            "4340ddc0bc1f468fbf988e53ed4139f4",
            "2417fc8d480849d08360c33795e8c983",
            "15b6f0b490e244bda4e9611de3647bba",
            "450665a289604343bb6a2cd2ce356ba4",
            "f44c9551314046afb56f20b337b1f239",
            "4c9a8dd3016e44149f114e11990c89c6"
          ]
        },
        "outputId": "4f987879-5eb8-4715-a262-5ee2e5d229e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0dcfdbc280148c1936dfd7a6777cb42"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irNZYVFGnZEs"
      },
      "source": [
        "## 02. 모델준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHcxdlTTnhqs"
      },
      "source": [
        "## 03. 추론- RTF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ_NWGG_gKHx"
      },
      "outputs": [],
      "source": [
        "PROMPT = '''Role : \"당신은 대입 논술 면접 학원을 운영하고 있는 선생님입니다.\"\n",
        "Task : \"당신의 주 업무는 예시 제시문 세트로부터 예시 질문을 생성하는 것입니다. 제시문 세트 안의 제시문 개수와, 질문의 개수는 정해져 있지 않습니다.\n",
        "당신에게 예시 제시문 세트와 예시 질문을 보여드리겠습니다. 이를 보고, 유사하게 새로운 제시문 세트로부터 질문을 생성하면 됩니다. 질문만 생성하는 것입니다. \"\n",
        "\n",
        "Format :\n",
        "\"먼저 예시로 주어진 제시문 세트입니다. 이 제시문 세트는 3개의 제시문으로 이뤄져 있습니다.\n",
        "\n",
        "'\n",
        "[제시문 (가)]\n",
        "인간은 타인의 시선에서 벗어나 있을 때 과연 윤리적일 수 있는가?\n",
        "칸다울레스 왕이 다스리는 리디아 왕국에 기게스라는 목동이 살았다.\n",
        "기게스가 양을 치고 있던 어느 날 갑자기 커다란 지진이 일어났다.\n",
        "지진이 일어난 자리에는 땅이 갈라져 동굴이 생겼고, 기게스는 호기심이 생겨 갈라진 동굴 안으로 들어갔다.\n",
        "그는 동굴 안에서 거인의 시체가 놓여 있는 것을 발견하였다.\n",
        "시체의 손가락에는 금반지가 끼워져 있었다.\n",
        "기게스는 그 반지를 빼 들고 밖으로 나왔다.\n",
        "그러다 우연히 자신이 끼고 있는 반지의 흠집 난 곳을 안쪽으로 돌리면 자신은 투명 인간이 되고 바깥쪽으로 돌리면 자기 모습이 다시 보인다는 사실을 알게 되었다.\n",
        "이제 남들의 시선에서 벗어나 보이지 않는 힘을 갖게 된 기게스는 자연스럽게 나쁜 마음을 먹게 되었다.\n",
        "가축의 상태를 왕에게 보고하는 전령으로서 궁전에 들어간 기게스는 자신의 새로운 힘인 마법 반지를 이용하여 모습을 감춘 후, 왕비를 겁탈하고 그녀를 자기 편으로 끌어들여 왕을 암살한 뒤 스스로 왕이 되었다.\n",
        "\n",
        "[제시문 (나)]\n",
        "서로 잘 아는 사람들이 소규모로 모여 사는 마을 공동체에서 개인은 대체로 합리적이고 도덕적인 성향을 보인다.\n",
        "그는 다른 사람들의 말에 크게 영향받기보다는, 이성과 주관에 따라 판단하고 규범에 맞게 행동한다.\n",
        "혼자 있을 때도, 여럿이 있을 때와 크게 다를 바 없이 처신한다.\n",
        "그런데 19세기 이후 대도시에 인구가 밀집하고 대량생산과 소비, 그리고 대중문화가 발전하면서 대중사회가 등장한다.\n",
        "이제 대중 속에서 이름 없는 한 명이 된 개인은 집단적인 분위기에 복종하고 전체의 결정을 따르라는 무언의 압력에 쉽사리 굴복한다.\n",
        "대중의 일원으로서 그는 익명성 아래 자신의 욕망, 정열, 관심을 분출하고 실현한다. 이때 권력을 가진 지도자의 역할은 결정적이다.\n",
        "권위적 지도자는 단순하고 선동적인 말로 대중에게 행동 방향을 제시하며, 대중은 지도자의 말을 마치 절대적인 진리인 것처럼 이해한다.\n",
        "대중은 직관과 감정에 따라 권위적 지도자의 말을 무비판적으로 수용한다.\n",
        "무리 속의 익명적 개인들은 쉽게 흥분하고 변덕을 부리며 열정을 드러낸다.\n",
        "그러한 감정 에너지는 때로 대중이 난폭하게 폭력을 행사한다든지, 용감하게 순교를 불사하는 등의 행동을 하도록 만든다.\n",
        "\n",
        "[제시문 (다)]\n",
        "도시 문명의 발전은 현대 사회의 주요 특징이다.\n",
        "과학과 기술의 진전을 통해 이루어진 도시화는 익명성이 전통에 따르는 도덕규범과 오랜 대면 관계를 대신하도록 만들었다.\n",
        "그런데 우리는 도시인의 생존이 도시의 잔인한 익명성 탓에 소모되고 훼손된다는 말을 자주 듣는다.\n",
        "개인은 작은 시골 마을에 있을 때 이미 정해진 행동규범을 따르며, 스스로 그것을 의무로 여겼다.\n",
        "그러한 규범을 어길 때 마을에서 평판이 나빠지기 때문이다.\n",
        "하지만 그는 누가 누군지 알 수 없는 대도시로 나오면서부터 깊은 인간관계를 쌓을 수 없게 된다는 것이다.\n",
        "이런 까닭에 대중사회 속의 도시인은 마치 정체성을 상실하고 자아를 잃어버린 채로 살아간다고 비판받기까지 한다.\n",
        "그러나 이처럼 도시의 익명성이 과거 마을 공동체 시절의 인간적 교류를 사라지게 했다는비판은 그 익명성이 지니는 독특한 이점을 보지 못한 데서 나온다.\n",
        "도시의 익명성은 마을 공동체의 넌더리 나는 속박에서 벗어나는 자유의 가능성을 제공하므로 위협적이고 피폐한 것이 아니라 훨씬 더 인간적이고 해방적인 현상이다.\n",
        "왜냐하면 도시 생활의 익명성 형태는 인간 삶에 필수적인 사생활을 보호하는 데 도움을 주며 도시인을 마을 생활의 부담스러운 도덕규범과 강요된 인습이라는 족쇄에서 벗어나도록 만들기 때문이다.\n",
        "따라서 도시인은 이런 익명성 덕택에 과거와는 비교할 수 없을 만큼 많은 사람과 다양한 교류를 할 수 있게 되었을 뿐만 아니라 자유롭고 창의적인 생각을 펼칠 수 있게 되었다.\n",
        "'\n",
        "\n",
        "\n",
        "다음으로 예시 질문을 보여드리겠습니다. 위의 예시 제시문 세트로부터 생성된 질문입니다.\n",
        "\n",
        "[예시 질문]\n",
        "'제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.'\n",
        "\n",
        "이처럼 예시 질문들은 예시 제시문의 내용을 참고하여, 이를 요약하거나 이들로부터 생각해볼 수 있는 내용들을 질문합니다. 이때 각 제시문 내용간의 관계를 고려해, 이를 물어보는 문제를 출제해야 합니다.\"\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QspS4rolcz1O"
      },
      "outputs": [],
      "source": [
        "test_text = '''다음 3개의 제시문을 보고 질문을 생성해주세요. [새로운 제시문]으로부터 질문을 생성하는 것입니다.\n",
        "질문만 생성해주세요. 다른 불필요한 문장은 삭제해 주세요.\n",
        "생성 형식은\n",
        "[\"질문 1\"\n",
        "\"질문 2\"\n",
        "\"질문 3\"]\n",
        "으로 부탁드립니다.\n",
        "질문을 3개 생성해주세요. 질문만 생성하면 됩니다. 질문은 한국어로, 1~2문장의 간결한 질문이 필요합니다. 답변은 필요 없습니다.\n",
        "\n",
        "[새로운 제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에\n",
        "충족시켜주는 매우 독특한 현상이다. 유행을 따름으로써 자신도 주변 사람들과 똑같이\n",
        "행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을\n",
        "얻으려는 심리가 복합적으로 얽혀 있는 것이다. 남을 따르고자 하는 욕구나 소망이 결여되는\n",
        "경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상\n",
        "존재하지 않게 된다. 예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을\n",
        "기피하는 집단에서는 유행이 발생하지 않는다. 1390년경 피렌체에서 남성 복장의 뚜렷한\n",
        "유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이\n",
        "모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다. 유행은\n",
        "결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[새로운 제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고\n",
        "동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다. 이제 사람들은 선조를\n",
        "닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다. 변화에 대한 애정과\n",
        "동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다. 이 원리들에는\n",
        "선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다. 옛것은 더 이상\n",
        "존경해야 하는 것으로 여겨지지 않는다. 사람들은 끊임없이 새것을 탐닉하며, 같은 시대에\n",
        "사는 사람들을 따라 한다. 예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀\n",
        "있었다. 그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다. 이탈리아, 스페인,\n",
        "프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다. 그들은 새로운\n",
        "것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다. 유행은 이런 식으로 사회 전체에\n",
        "확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서\n",
        "변화하였다.\n",
        "\n",
        "[새로운 제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을\n",
        "하게 된다. 그 행동양식 가운데 하나가 언어 사용이다. 가령, ‘엄마, 아빠’와 같은 말은\n",
        "어린이들의 말이고, ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적\n",
        "행동 양식이 반영된 것이라고 할 수 있다. 이런 ‘연령 단계’에 의한 언어 차이 외에도,\n",
        "기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도\n",
        "있을 수 있다. 젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를\n",
        "쉽게 받아들인다. 언어에서도 젊은 세대의 이런 특성이 반영된다. 젊은 세대는 ‘반모’(반말\n",
        "모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와\n",
        "신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는\n",
        "젊은 세대를 중심으로 사용된다. 이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "                '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = Llama(\n",
        "    model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5cd69d040dd9479aa53cc8177981f559",
            "b541d24c191a473db213535554f28577",
            "70f3fbac817945f387a6c9edb275b0b4",
            "4c6e3dec9eef45e48328c2adcd2ebe68",
            "7ca548e4b40b481a96e1d5b793bdd625",
            "50c9fce6869b4616ada71b3802813439",
            "1f4e691f7db64e3a90b497363f8353bf",
            "9622e8c7fe7d47eeabe4b8e2dd06bcdf",
            "488f53e9c9a149cfb72cd96b0f601dba",
            "ec580b1a7e3646949b4a37c73be1358d",
            "585a8c25882f40bfb437f7dce836801e",
            "25ea18b3265b42678f55ac2c15d1d099",
            "8ecbe3ed83ef4979987137f9ff5e2336",
            "a27b7d2e610f4345b52f915e61b6a215",
            "4db8025ad73744e7b7712f0184e01aa2",
            "19d83363ee6042f1a4f5125ed0f3706f",
            "807b7ab0cf5e4e8f9cf85bbbfe04333e",
            "dcacf042ac844c66ada43b3b6d58ce4c",
            "6ed433a493804b62b9b02396ad9c2839",
            "d16311ef3fba48fb96e117f0af72f8b6",
            "d8db6c26f95f4b68a693933118ce2ef4",
            "0de0af0c1da44744859d3ab0311691c1",
            "173d1bcfb4234ca99f80c6d3252b3e44",
            "88282786833c4f508ca883f5f9fd2b1f",
            "e265311a6a614da7a6034dcee867d9cc",
            "ccdf21bbb6c04ae8a9eea1abb20904c4",
            "909527dbdf864dbbb6556ac3147fb79b",
            "7cb6e3de109a461487af25cc138ac69f",
            "8de0cc772525478c8c7b6f054a7fb794",
            "c5e571f3e4a243b7acc67926f2c28320",
            "84b03ce9a2a442cab58058f47d31b556",
            "eee2b6a0adf343b8a703a41a11aa97f0",
            "6b68720baa83471cb6c13f9ccf4982d0"
          ]
        },
        "id": "OMjiUwAuOcHS",
        "outputId": "16508411-df36-4c63-a5b8-6702b5528aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cd69d040dd9479aa53cc8177981f559"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25ea18b3265b42678f55ac2c15d1d099"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "173d1bcfb4234ca99f80c6d3252b3e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 145088\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,145088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,145088]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,145088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,296982]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 144782\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 144783\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens cache size = 306\n",
            "llm_load_vocab: token to piece cache size = 0.9314 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 145088\n",
            "llm_load_print_meta: n_merges         = 296982\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.17 B\n",
            "llm_load_print_meta: model size       = 4.66 GiB (4.91 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 144782 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 144783 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 144791 '<|eot_id|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   318.80 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4457.43 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 5024\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   628.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  628.00 MiB, K (f16):  314.00 MiB, V (f16):  314.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.55 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   355.82 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    17.82 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '144783', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '145088', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '144782', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlSSOAUuOUmh",
        "outputId": "d5831363-987e-4f15-e412-0057aff9714a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   10726.30 ms\n",
            "llama_print_timings:      sample time =     310.83 ms /   123 runs   (    2.53 ms per token,   395.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11691.76 ms /  2035 tokens (    5.75 ms per token,   174.05 tokens per second)\n",
            "llama_print_timings:        eval time =    2886.64 ms /   122 runs   (   23.66 ms per token,    42.26 tokens per second)\n",
            "llama_print_timings:       total time =   15031.19 ms /  2157 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1] 전통적인 가치에 대한 거부과 개인적인 자유에 대한 강조가 현대 사회의 특징이라는데 동의하느냐? [예를 들어, 유행] \n",
            "\n",
            "[질문 2] 유행은 누구의 주도적인 역할을 통해 형성되는지 궁금합니다. 예를 들어, 피렌체에서 남성 복장의 뚜렷한 유행이 탄생한 이유는 어떤 요인이 작용한 것일까요? \n",
            "\n",
            "[질문 3] 변화 속도가 빠른 현대 사회에서 유행이 가지는 의미가 어떻게 되는지 봅니다. 유행이 모든 분야에 영향을 미치는 경향이 있는 이유에 대해서 논의해주세요.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":500,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":0.6,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fl0g5kep2rRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4nulKujOakM"
      },
      "source": [
        "## 03. 추론- CARE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_care = Llama(\n",
        "    model_path='./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=10000,\n",
        "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu9Jr6Z1Oji8",
        "outputId": "66b8fc4e-b246-4d40-b5e5-5c039f8d76eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 145088\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,145088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,145088]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,145088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,296982]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 144782\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 144783\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens definition check successful ( 306/145088 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 145088\n",
            "llm_load_print_meta: n_merges         = 296982\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.17 B\n",
            "llm_load_print_meta: model size       = 4.66 GiB (4.91 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 144782 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 144783 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 144791 '<|eot_id|>'\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   318.80 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4457.43 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 10016\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1252.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1252.00 MiB, K (f16):  626.00 MiB, V (f16):  626.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.55 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   677.57 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    27.57 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '144783', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '145088', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '144782', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBohDZKOOakO"
      },
      "outputs": [],
      "source": [
        "prompt_care = '''\n",
        "Context : \"당신은 대입 논술 면접 학원을 운영하고 있는 선생님입니다. 당신의 주 업무는 예시 제시문 세트로부터 예시 질문을 생성하는 것입니다.\"\n",
        "Action : \"당신에게 예시 제시문 세트와 예시 질문을 보여드리겠습니다. 이를 보고, 새로운 제시문 세트로부터 당신에게 예시 제시문 세트와 예시 질문을 보여드리겠습니다. 이를 보고, 새로운 유사하게 제시문 세트로부터 질문을 생성하면 됩니다.질문을 생성하면 됩니다. 제시문 세트 안의 제시문 개수와, 질문의 개수는 정해져 있지 않습니다. \"\n",
        "\n",
        "Example : \"예시 제시문 세트는 3개의 제시문으로 이뤄져 있습니다.\n",
        "\n",
        "'\n",
        "[제시문 (가)]\n",
        "인간은 타인의 시선에서 벗어나 있을 때 과연 윤리적일 수 있는가?\n",
        "칸다울레스 왕이 다스리는 리디아 왕국에 기게스라는 목동이 살았다.\n",
        "기게스가 양을 치고 있던 어느 날 갑자기 커다란 지진이 일어났다.\n",
        "지진이 일어난 자리에는 땅이 갈라져 동굴이 생겼고, 기게스는 호기심이 생겨 갈라진 동굴 안으로 들어갔다.\n",
        "그는 동굴 안에서 거인의 시체가 놓여 있는 것을 발견하였다.\n",
        "시체의 손가락에는 금반지가 끼워져 있었다.\n",
        "기게스는 그 반지를 빼 들고 밖으로 나왔다.\n",
        "그러다 우연히 자신이 끼고 있는 반지의 흠집 난 곳을 안쪽으로 돌리면 자신은 투명 인간이 되고 바깥쪽으로 돌리면 자기 모습이 다시 보인다는 사실을 알게 되었다.\n",
        "이제 남들의 시선에서 벗어나 보이지 않는 힘을 갖게 된 기게스는 자연스럽게 나쁜 마음을 먹게 되었다.\n",
        "가축의 상태를 왕에게 보고하는 전령으로서 궁전에 들어간 기게스는 자신의 새로운 힘인 마법 반지를 이용하여 모습을 감춘 후, 왕비를 겁탈하고 그녀를 자기 편으로 끌어들여 왕을 암살한 뒤 스스로 왕이 되었다.\n",
        "\n",
        "[제시문 (나)]\n",
        "서로 잘 아는 사람들이 소규모로 모여 사는 마을 공동체에서 개인은 대체로 합리적이고 도덕적인 성향을 보인다.\n",
        "그는 다른 사람들의 말에 크게 영향받기보다는, 이성과 주관에 따라 판단하고 규범에 맞게 행동한다.\n",
        "혼자 있을 때도, 여럿이 있을 때와 크게 다를 바 없이 처신한다.\n",
        "그런데 19세기 이후 대도시에 인구가 밀집하고 대량생산과 소비, 그리고 대중문화가 발전하면서 대중사회가 등장한다.\n",
        "이제 대중 속에서 이름 없는 한 명이 된 개인은 집단적인 분위기에 복종하고 전체의 결정을 따르라는 무언의 압력에 쉽사리 굴복한다.\n",
        "대중의 일원으로서 그는 익명성 아래 자신의 욕망, 정열, 관심을 분출하고 실현한다. 이때 권력을 가진 지도자의 역할은 결정적이다.\n",
        "권위적 지도자는 단순하고 선동적인 말로 대중에게 행동 방향을 제시하며, 대중은 지도자의 말을 마치 절대적인 진리인 것처럼 이해한다.\n",
        "대중은 직관과 감정에 따라 권위적 지도자의 말을 무비판적으로 수용한다.\n",
        "무리 속의 익명적 개인들은 쉽게 흥분하고 변덕을 부리며 열정을 드러낸다.\n",
        "그러한 감정 에너지는 때로 대중이 난폭하게 폭력을 행사한다든지, 용감하게 순교를 불사하는 등의 행동을 하도록 만든다.\n",
        "\n",
        "[제시문 (다)]\n",
        "도시 문명의 발전은 현대 사회의 주요 특징이다.\n",
        "과학과 기술의 진전을 통해 이루어진 도시화는 익명성이 전통에 따르는 도덕규범과 오랜 대면 관계를 대신하도록 만들었다.\n",
        "그런데 우리는 도시인의 생존이 도시의 잔인한 익명성 탓에 소모되고 훼손된다는 말을 자주 듣는다.\n",
        "개인은 작은 시골 마을에 있을 때 이미 정해진 행동규범을 따르며, 스스로 그것을 의무로 여겼다.\n",
        "그러한 규범을 어길 때 마을에서 평판이 나빠지기 때문이다.\n",
        "하지만 그는 누가 누군지 알 수 없는 대도시로 나오면서부터 깊은 인간관계를 쌓을 수 없게 된다는 것이다.\n",
        "이런 까닭에 대중사회 속의 도시인은 마치 정체성을 상실하고 자아를 잃어버린 채로 살아간다고 비판받기까지 한다.\n",
        "그러나 이처럼 도시의 익명성이 과거 마을 공동체 시절의 인간적 교류를 사라지게 했다는비판은 그 익명성이 지니는 독특한 이점을 보지 못한 데서 나온다.\n",
        "도시의 익명성은 마을 공동체의 넌더리 나는 속박에서 벗어나는 자유의 가능성을 제공하므로 위협적이고 피폐한 것이 아니라 훨씬 더 인간적이고 해방적인 현상이다.\n",
        "왜냐하면 도시 생활의 익명성 형태는 인간 삶에 필수적인 사생활을 보호하는 데 도움을 주며 도시인을 마을 생활의 부담스러운 도덕규범과 강요된 인습이라는 족쇄에서 벗어나도록 만들기 때문이다.\n",
        "따라서 도시인은 이런 익명성 덕택에 과거와는 비교할 수 없을 만큼 많은 사람과 다양한 교류를 할 수 있게 되었을 뿐만 아니라 자유롭고 창의적인 생각을 펼칠 수 있게 되었다.\n",
        "'\n",
        "\n",
        "위의 예시 제시문 세트로부터 생성된 질문입니다.\n",
        "\n",
        "[예시 질문]\n",
        "'제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.'\n",
        "\"\n",
        "\n",
        "Result : \"이처럼 질문들은 제시문의 내용을 참고하여, 이를 요약하거나 이들로부터 생각해볼 수 있는 내용들을 질문합니다. 이때 각 제시문 내용간의 관계를 고려해, 이를 물어보는 문제를 출제해야 합니다. 당신은 이와 유사하게 새로운 제시문으로부터 질문을 생성하면 됩니다.\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CJXgma8OakO"
      },
      "outputs": [],
      "source": [
        "test_text_care = '''다음 3개의 제시문을 보고 질문을 생성해주세요.\n",
        "질문은 한국어로, 1~2문장의 간결한 질문이 필요합니다. [새로운 제시문]으로부터 질문을 생성하는 것입니다.\n",
        "질문만 생성해주세요. 다른 불필요한 문장은 삭제해 주세요. 답변은 생성하지 않습니다.\n",
        "\n",
        "생성 형식은\n",
        "[\"질문 1\"\n",
        "\"질문 2\"\n",
        "\"질문 3\"] 으로 해야 합니다.\n",
        "질문을 3개 생성해주세요.\n",
        "\n",
        "[새로운 제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 충족시켜주는 매우 독특한 현상이다.\n",
        "유행을 따름으로써 자신도 주변 사람들과 똑같이 행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을 얻으려는 심리가 복합적으로 얽혀 있는 것이다.\n",
        "남을 따르고자 하는 욕구나 소망이 결여되는 경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상 존재하지 않게 된다.\n",
        "예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을 기피하는 집단에서는 유행이 발생하지 않는다.\n",
        "1390년경 피렌체에서 남성 복장의 뚜렷한 유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이 모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다.\n",
        "유행은 결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[새로운 제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고 동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다.\n",
        "이제 사람들은 선조를 닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다.\n",
        "변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다.\n",
        "이 원리들에는 선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다.\n",
        "옛것은 더 이상 존경해야 하는 것으로 여겨지지 않는다.\n",
        "사람들은 끊임없이 새것을 탐닉하며, 같은 시대에사는 사람들을 따라 한다.\n",
        "예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀 있었다.\n",
        "그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다.\n",
        "이탈리아, 스페인, 프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다.\n",
        "그들은 새로운 것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다.\n",
        "유행은 이런 식으로 사회 전체에 확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서 변화하였다.\n",
        "\n",
        "[새로운 제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을 하게 된다.\n",
        "그 행동양식 가운데 하나가 언어 사용이다.\n",
        "가령, ‘엄마, 아빠’와 같은 말은 어린이들의 말이고, ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적 행동 양식이 반영된 것이라고 할 수 있다.\n",
        "이런 ‘연령 단계’에 의한 언어 차이 외에도, 기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도 있을 수 있다.\n",
        "젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를 쉽게 받아들인다.\n",
        "언어에서도 젊은 세대의 이런 특성이 반영된다.\n",
        "젊은 세대는 ‘반모’(반말모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와 신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는 젊은 세대를 중심으로 사용된다.\n",
        "이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBI0TJGWOakP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7b2faa-7e8e-4ce7-aed0-3afd29a32b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     546.35 ms\n",
            "llama_print_timings:      sample time =     199.41 ms /    62 runs   (    3.22 ms per token,   310.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1055.72 ms /   810 tokens (    1.30 ms per token,   767.25 tokens per second)\n",
            "llama_print_timings:        eval time =    1779.16 ms /    61 runs   (   29.17 ms per token,    34.29 tokens per second)\n",
            "llama_print_timings:       total time =    3194.70 ms /   871 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문:\n",
            "1. 유행은 누구의 주도적인 역할을 통해 형성되는지 설명해주세요.\n",
            "2. 어떤 요인들이 유행을 형성하는 데 중요한 역할을 하는지 설명하시오.\n",
            "3. 왜 사람들은 유행에 민감하고, 새로운 변화에 쉽게 적응하는지 설명해주세요.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{prompt_care}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text_care}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":5000,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":0.6,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model_care(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2차 시도"
      ],
      "metadata": {
        "id": "ekxzxSctgP0J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPA6wlWfgMf7"
      },
      "source": [
        "## 03. 추론- RTF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = Llama(\n",
        "    model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8b6fd4-da2e-48e1-f803-afdc77199165",
        "id": "OuBZLzgagMgH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 145088\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,145088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,145088]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,145088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,296982]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 144782\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 144783\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens definition check successful ( 306/145088 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 145088\n",
            "llm_load_print_meta: n_merges         = 296982\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.17 B\n",
            "llm_load_print_meta: model size       = 4.66 GiB (4.91 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 144782 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 144783 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 144791 '<|eot_id|>'\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   318.80 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4457.43 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 5024\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   628.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  628.00 MiB, K (f16):  314.00 MiB, V (f16):  314.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.55 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   355.82 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    17.82 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '144783', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '145088', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '144782', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd_L8KdFgMgG"
      },
      "outputs": [],
      "source": [
        "PROMPT = '''Role : \"당신은 대입 논술 면접 학원을 운영하고 있는 선생님입니다.\"\n",
        "Task : \"당신의 주 업무는 예시 제시문 세트로부터 예시 질문을 생성하는 것입니다. 제시문 세트 안의 제시문 개수와, 질문의 개수는 정해져 있지 않습니다.\n",
        "당신에게 예시 제시문 세트와 예시 질문을 보여드리겠습니다. 이를 보고, 유사하게 새로운 제시문 세트로부터 질문을 생성하면 됩니다. 질문만 생성하는 것입니다. \"\n",
        "\n",
        "Format :\n",
        "\"먼저 예시로 주어진 제시문 세트입니다. 이 제시문 세트는 3개의 제시문으로 이뤄져 있습니다.\n",
        "\n",
        "'\n",
        "[제시문 (가)]\n",
        "인간은 타인의 시선에서 벗어나 있을 때 과연 윤리적일 수 있는가?\n",
        "칸다울레스 왕이 다스리는 리디아 왕국에 기게스라는 목동이 살았다.\n",
        "기게스가 양을 치고 있던 어느 날 갑자기 커다란 지진이 일어났다.\n",
        "지진이 일어난 자리에는 땅이 갈라져 동굴이 생겼고, 기게스는 호기심이 생겨 갈라진 동굴 안으로 들어갔다.\n",
        "그는 동굴 안에서 거인의 시체가 놓여 있는 것을 발견하였다.\n",
        "시체의 손가락에는 금반지가 끼워져 있었다.\n",
        "기게스는 그 반지를 빼 들고 밖으로 나왔다.\n",
        "그러다 우연히 자신이 끼고 있는 반지의 흠집 난 곳을 안쪽으로 돌리면 자신은 투명 인간이 되고 바깥쪽으로 돌리면 자기 모습이 다시 보인다는 사실을 알게 되었다.\n",
        "이제 남들의 시선에서 벗어나 보이지 않는 힘을 갖게 된 기게스는 자연스럽게 나쁜 마음을 먹게 되었다.\n",
        "가축의 상태를 왕에게 보고하는 전령으로서 궁전에 들어간 기게스는 자신의 새로운 힘인 마법 반지를 이용하여 모습을 감춘 후, 왕비를 겁탈하고 그녀를 자기 편으로 끌어들여 왕을 암살한 뒤 스스로 왕이 되었다.\n",
        "\n",
        "[제시문 (나)]\n",
        "서로 잘 아는 사람들이 소규모로 모여 사는 마을 공동체에서 개인은 대체로 합리적이고 도덕적인 성향을 보인다.\n",
        "그는 다른 사람들의 말에 크게 영향받기보다는, 이성과 주관에 따라 판단하고 규범에 맞게 행동한다.\n",
        "혼자 있을 때도, 여럿이 있을 때와 크게 다를 바 없이 처신한다.\n",
        "그런데 19세기 이후 대도시에 인구가 밀집하고 대량생산과 소비, 그리고 대중문화가 발전하면서 대중사회가 등장한다.\n",
        "이제 대중 속에서 이름 없는 한 명이 된 개인은 집단적인 분위기에 복종하고 전체의 결정을 따르라는 무언의 압력에 쉽사리 굴복한다.\n",
        "대중의 일원으로서 그는 익명성 아래 자신의 욕망, 정열, 관심을 분출하고 실현한다. 이때 권력을 가진 지도자의 역할은 결정적이다.\n",
        "권위적 지도자는 단순하고 선동적인 말로 대중에게 행동 방향을 제시하며, 대중은 지도자의 말을 마치 절대적인 진리인 것처럼 이해한다.\n",
        "대중은 직관과 감정에 따라 권위적 지도자의 말을 무비판적으로 수용한다.\n",
        "무리 속의 익명적 개인들은 쉽게 흥분하고 변덕을 부리며 열정을 드러낸다.\n",
        "그러한 감정 에너지는 때로 대중이 난폭하게 폭력을 행사한다든지, 용감하게 순교를 불사하는 등의 행동을 하도록 만든다.\n",
        "\n",
        "[제시문 (다)]\n",
        "도시 문명의 발전은 현대 사회의 주요 특징이다.\n",
        "과학과 기술의 진전을 통해 이루어진 도시화는 익명성이 전통에 따르는 도덕규범과 오랜 대면 관계를 대신하도록 만들었다.\n",
        "그런데 우리는 도시인의 생존이 도시의 잔인한 익명성 탓에 소모되고 훼손된다는 말을 자주 듣는다.\n",
        "개인은 작은 시골 마을에 있을 때 이미 정해진 행동규범을 따르며, 스스로 그것을 의무로 여겼다.\n",
        "그러한 규범을 어길 때 마을에서 평판이 나빠지기 때문이다.\n",
        "하지만 그는 누가 누군지 알 수 없는 대도시로 나오면서부터 깊은 인간관계를 쌓을 수 없게 된다는 것이다.\n",
        "이런 까닭에 대중사회 속의 도시인은 마치 정체성을 상실하고 자아를 잃어버린 채로 살아간다고 비판받기까지 한다.\n",
        "그러나 이처럼 도시의 익명성이 과거 마을 공동체 시절의 인간적 교류를 사라지게 했다는비판은 그 익명성이 지니는 독특한 이점을 보지 못한 데서 나온다.\n",
        "도시의 익명성은 마을 공동체의 넌더리 나는 속박에서 벗어나는 자유의 가능성을 제공하므로 위협적이고 피폐한 것이 아니라 훨씬 더 인간적이고 해방적인 현상이다.\n",
        "왜냐하면 도시 생활의 익명성 형태는 인간 삶에 필수적인 사생활을 보호하는 데 도움을 주며 도시인을 마을 생활의 부담스러운 도덕규범과 강요된 인습이라는 족쇄에서 벗어나도록 만들기 때문이다.\n",
        "따라서 도시인은 이런 익명성 덕택에 과거와는 비교할 수 없을 만큼 많은 사람과 다양한 교류를 할 수 있게 되었을 뿐만 아니라 자유롭고 창의적인 생각을 펼칠 수 있게 되었다.\n",
        "'\n",
        "\n",
        "\n",
        "다음으로 예시 질문을 보여드리겠습니다. 위의 예시 제시문 세트로부터 생성된 질문입니다.\n",
        "\n",
        "[예시 질문]\n",
        "'제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.'\n",
        "\n",
        "이처럼 예시 제시문들은 공통적인 주제를 가지고, 예시 질문들은 예시 제시문의 내용을 참고해 제시문 간의 내용의 연관성을 묻습니다. 당신은 이와 유사하게 새로운 제시문 세트로부터 문제를 출제해야 합니다.\"\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPApR21QgMgH"
      },
      "outputs": [],
      "source": [
        "test_text = '''다음 3개의 제시문을 보고 질문을 생성해주세요. [새로운 제시문]으로부터 질문을 생성하는 것입니다.\n",
        "생성 형식은\n",
        "[\"질문 1\"\n",
        "\"질문 2\"\n",
        "\"질문 3\"]\n",
        "으로 부탁드립니다.\n",
        "\n",
        "질문을 3개 생성해주세요. 질문만 생성하면 되고 답변은 필요 없습니다. 다른 불필요한 문장은 삭제해 주세요. 질문은 한국어로, 1~2문장의 간결한 질문이 필요합니다.\n",
        "\n",
        "[새로운 제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에\n",
        "충족시켜주는 매우 독특한 현상이다. 유행을 따름으로써 자신도 주변 사람들과 똑같이\n",
        "행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을\n",
        "얻으려는 심리가 복합적으로 얽혀 있는 것이다. 남을 따르고자 하는 욕구나 소망이 결여되는\n",
        "경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상\n",
        "존재하지 않게 된다. 예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을\n",
        "기피하는 집단에서는 유행이 발생하지 않는다. 1390년경 피렌체에서 남성 복장의 뚜렷한\n",
        "유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이\n",
        "모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다. 유행은\n",
        "결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[새로운 제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고\n",
        "동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다. 이제 사람들은 선조를\n",
        "닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다. 변화에 대한 애정과\n",
        "동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다. 이 원리들에는\n",
        "선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다. 옛것은 더 이상\n",
        "존경해야 하는 것으로 여겨지지 않는다. 사람들은 끊임없이 새것을 탐닉하며, 같은 시대에\n",
        "사는 사람들을 따라 한다. 예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀\n",
        "있었다. 그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다. 이탈리아, 스페인,\n",
        "프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다. 그들은 새로운\n",
        "것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다. 유행은 이런 식으로 사회 전체에\n",
        "확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서\n",
        "변화하였다.\n",
        "\n",
        "[새로운 제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을\n",
        "하게 된다. 그 행동양식 가운데 하나가 언어 사용이다. 가령, ‘엄마, 아빠’와 같은 말은\n",
        "어린이들의 말이고, ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적\n",
        "행동 양식이 반영된 것이라고 할 수 있다. 이런 ‘연령 단계’에 의한 언어 차이 외에도,\n",
        "기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도\n",
        "있을 수 있다. 젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를\n",
        "쉽게 받아들인다. 언어에서도 젊은 세대의 이런 특성이 반영된다. 젊은 세대는 ‘반모’(반말\n",
        "모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와\n",
        "신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는\n",
        "젊은 세대를 중심으로 사용된다. 이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "                '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb82c6a2-cbdb-4bbe-e7d4-306e69c11208",
        "id": "2TK0St7mgMgI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
            "\n",
            "llama_print_timings:        load time =     546.52 ms\n",
            "llama_print_timings:      sample time =     506.82 ms /   192 runs   (    2.64 ms per token,   378.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2160.73 ms /  2029 tokens (    1.06 ms per token,   939.03 tokens per second)\n",
            "llama_print_timings:        eval time =    5576.66 ms /   191 runs   (   29.20 ms per token,    34.25 tokens per second)\n",
            "llama_print_timings:       total time =    8720.92 ms /  2220 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1] 유행이 모든 세대에 영향을 미치는가요?\n",
            "유행은 시대에 따라 영향을 주기도 하지만, 어떤 시대에는 유행이 지배적일지라도 급격한 변화를 겪을 수 있습니다.\n",
            "\n",
            "[질문 2] 어떤 요인들이 유행을 이끌어내는 역할을 합니다?\n",
            "유행은 크게 남성복장의 경우, 여성복장의 경우, 그리고 디자인의 경우 세 가지 유형으로 나뉩니다. 이 중 어느 것이 가장 큰 영향을 끼치느냐는 시대에 따라 다를 수 있습니다.\n",
            "\n",
            "[질문 3] 유행이 바뀌면 어떤 결과가 나타나는가요?\n",
            "유행이 바뀌면 많은 사람들의 행동과 의식이 함께 변화합니다. 예를 들어, 유행이 바뀌면 사람들은 새로운 스타일을 채택하고, 기존의 스타일은 더 이상 유행하지 않게 됩니다. 따라서, 유행의 변화는 사회적인 변화를 나타내는 지표로 볼 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":500,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":0.6,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gJyB53LgMgI"
      },
      "source": [
        "## 03. 추론- CARE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_care = Llama(\n",
        "    model_path='./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=10000,\n",
        "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b8fc4e-b246-4d40-b5e5-5c039f8d76eb",
        "id": "2qFt5_kfgMgI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 145088\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,145088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,145088]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,145088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,296982]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 144782\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 144783\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens definition check successful ( 306/145088 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 145088\n",
            "llm_load_print_meta: n_merges         = 296982\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.17 B\n",
            "llm_load_print_meta: model size       = 4.66 GiB (4.91 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 144782 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 144783 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 144791 '<|eot_id|>'\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   318.80 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4457.43 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 10016\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1252.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1252.00 MiB, K (f16):  626.00 MiB, V (f16):  626.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.55 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   677.57 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    27.57 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '144783', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '145088', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '144782', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTr1SQwtgMgJ"
      },
      "outputs": [],
      "source": [
        "prompt_care = '''\n",
        "Context : \"당신은 대입 논술 면접 학원을 운영하고 있는 선생님입니다. 당신의 주 업무는 예시 제시문 세트로부터 예시 질문을 생성하는 것입니다.\"\n",
        "Action : \"당신에게 예시 제시문 세트와 예시 질문을 보여드리겠습니다. 이를 보고, 새로운 제시문 세트로부터 당신에게 예시 제시문 세트와 예시 질문을 보여드리겠습니다. 이를 보고, 새로운 유사하게 제시문 세트로부터 질문을 생성하면 됩니다.질문을 생성하면 됩니다. 제시문 세트 안의 제시문 개수와, 질문의 개수는 정해져 있지 않습니다. \"\n",
        "\n",
        "Example : \"예시 제시문 세트는 3개의 제시문으로 이뤄져 있습니다.\n",
        "\n",
        "'\n",
        "[제시문 (가)]\n",
        "인간은 타인의 시선에서 벗어나 있을 때 과연 윤리적일 수 있는가?\n",
        "칸다울레스 왕이 다스리는 리디아 왕국에 기게스라는 목동이 살았다.\n",
        "기게스가 양을 치고 있던 어느 날 갑자기 커다란 지진이 일어났다.\n",
        "지진이 일어난 자리에는 땅이 갈라져 동굴이 생겼고, 기게스는 호기심이 생겨 갈라진 동굴 안으로 들어갔다.\n",
        "그는 동굴 안에서 거인의 시체가 놓여 있는 것을 발견하였다.\n",
        "시체의 손가락에는 금반지가 끼워져 있었다.\n",
        "기게스는 그 반지를 빼 들고 밖으로 나왔다.\n",
        "그러다 우연히 자신이 끼고 있는 반지의 흠집 난 곳을 안쪽으로 돌리면 자신은 투명 인간이 되고 바깥쪽으로 돌리면 자기 모습이 다시 보인다는 사실을 알게 되었다.\n",
        "이제 남들의 시선에서 벗어나 보이지 않는 힘을 갖게 된 기게스는 자연스럽게 나쁜 마음을 먹게 되었다.\n",
        "가축의 상태를 왕에게 보고하는 전령으로서 궁전에 들어간 기게스는 자신의 새로운 힘인 마법 반지를 이용하여 모습을 감춘 후, 왕비를 겁탈하고 그녀를 자기 편으로 끌어들여 왕을 암살한 뒤 스스로 왕이 되었다.\n",
        "\n",
        "[제시문 (나)]\n",
        "서로 잘 아는 사람들이 소규모로 모여 사는 마을 공동체에서 개인은 대체로 합리적이고 도덕적인 성향을 보인다.\n",
        "그는 다른 사람들의 말에 크게 영향받기보다는, 이성과 주관에 따라 판단하고 규범에 맞게 행동한다.\n",
        "혼자 있을 때도, 여럿이 있을 때와 크게 다를 바 없이 처신한다.\n",
        "그런데 19세기 이후 대도시에 인구가 밀집하고 대량생산과 소비, 그리고 대중문화가 발전하면서 대중사회가 등장한다.\n",
        "이제 대중 속에서 이름 없는 한 명이 된 개인은 집단적인 분위기에 복종하고 전체의 결정을 따르라는 무언의 압력에 쉽사리 굴복한다.\n",
        "대중의 일원으로서 그는 익명성 아래 자신의 욕망, 정열, 관심을 분출하고 실현한다. 이때 권력을 가진 지도자의 역할은 결정적이다.\n",
        "권위적 지도자는 단순하고 선동적인 말로 대중에게 행동 방향을 제시하며, 대중은 지도자의 말을 마치 절대적인 진리인 것처럼 이해한다.\n",
        "대중은 직관과 감정에 따라 권위적 지도자의 말을 무비판적으로 수용한다.\n",
        "무리 속의 익명적 개인들은 쉽게 흥분하고 변덕을 부리며 열정을 드러낸다.\n",
        "그러한 감정 에너지는 때로 대중이 난폭하게 폭력을 행사한다든지, 용감하게 순교를 불사하는 등의 행동을 하도록 만든다.\n",
        "\n",
        "[제시문 (다)]\n",
        "도시 문명의 발전은 현대 사회의 주요 특징이다.\n",
        "과학과 기술의 진전을 통해 이루어진 도시화는 익명성이 전통에 따르는 도덕규범과 오랜 대면 관계를 대신하도록 만들었다.\n",
        "그런데 우리는 도시인의 생존이 도시의 잔인한 익명성 탓에 소모되고 훼손된다는 말을 자주 듣는다.\n",
        "개인은 작은 시골 마을에 있을 때 이미 정해진 행동규범을 따르며, 스스로 그것을 의무로 여겼다.\n",
        "그러한 규범을 어길 때 마을에서 평판이 나빠지기 때문이다.\n",
        "하지만 그는 누가 누군지 알 수 없는 대도시로 나오면서부터 깊은 인간관계를 쌓을 수 없게 된다는 것이다.\n",
        "이런 까닭에 대중사회 속의 도시인은 마치 정체성을 상실하고 자아를 잃어버린 채로 살아간다고 비판받기까지 한다.\n",
        "그러나 이처럼 도시의 익명성이 과거 마을 공동체 시절의 인간적 교류를 사라지게 했다는비판은 그 익명성이 지니는 독특한 이점을 보지 못한 데서 나온다.\n",
        "도시의 익명성은 마을 공동체의 넌더리 나는 속박에서 벗어나는 자유의 가능성을 제공하므로 위협적이고 피폐한 것이 아니라 훨씬 더 인간적이고 해방적인 현상이다.\n",
        "왜냐하면 도시 생활의 익명성 형태는 인간 삶에 필수적인 사생활을 보호하는 데 도움을 주며 도시인을 마을 생활의 부담스러운 도덕규범과 강요된 인습이라는 족쇄에서 벗어나도록 만들기 때문이다.\n",
        "따라서 도시인은 이런 익명성 덕택에 과거와는 비교할 수 없을 만큼 많은 사람과 다양한 교류를 할 수 있게 되었을 뿐만 아니라 자유롭고 창의적인 생각을 펼칠 수 있게 되었다.\n",
        "'\n",
        "\n",
        "위의 예시 제시문 세트로부터 생성된 질문입니다.\n",
        "\n",
        "[예시 질문]\n",
        "'제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.'\n",
        "\"\n",
        "\n",
        "Result : \"이처럼 질문들은 제시문의 내용을 참고하여, 이를 요약하거나 이들로부터 생각해볼 수 있는 내용들을 질문합니다. 이때 각 제시문 내용간의 관계를 고려해, 이를 물어보는 문제를 출제해야 합니다. 당신은 이와 유사하게 새로운 제시문으로부터 질문을 생성하면 됩니다.\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7-TMaAcgMgJ"
      },
      "outputs": [],
      "source": [
        "test_text_care = '''다음 3개의 제시문을 보고 질문을 생성해주세요.\n",
        "질문은 한국어로, 1~2문장의 간결한 질문이 필요합니다. [새로운 제시문]으로부터 질문을 생성하는 것입니다.\n",
        "질문만 생성해주세요. 다른 불필요한 문장은 삭제해 주세요. 답변은 생성하지 않습니다.\n",
        "\n",
        "생성 형식은\n",
        "[\"질문 1\"\n",
        "\"질문 2\"\n",
        "\"질문 3\"] 으로 해야 합니다.\n",
        "질문을 3개 생성해주세요.\n",
        "\n",
        "[새로운 제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 충족시켜주는 매우 독특한 현상이다.\n",
        "유행을 따름으로써 자신도 주변 사람들과 똑같이 행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을 얻으려는 심리가 복합적으로 얽혀 있는 것이다.\n",
        "남을 따르고자 하는 욕구나 소망이 결여되는 경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상 존재하지 않게 된다.\n",
        "예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을 기피하는 집단에서는 유행이 발생하지 않는다.\n",
        "1390년경 피렌체에서 남성 복장의 뚜렷한 유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이 모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다.\n",
        "유행은 결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[새로운 제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고 동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다.\n",
        "이제 사람들은 선조를 닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다.\n",
        "변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다.\n",
        "이 원리들에는 선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다.\n",
        "옛것은 더 이상 존경해야 하는 것으로 여겨지지 않는다.\n",
        "사람들은 끊임없이 새것을 탐닉하며, 같은 시대에사는 사람들을 따라 한다.\n",
        "예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀 있었다.\n",
        "그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다.\n",
        "이탈리아, 스페인, 프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다.\n",
        "그들은 새로운 것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다.\n",
        "유행은 이런 식으로 사회 전체에 확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서 변화하였다.\n",
        "\n",
        "[새로운 제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을 하게 된다.\n",
        "그 행동양식 가운데 하나가 언어 사용이다.\n",
        "가령, ‘엄마, 아빠’와 같은 말은 어린이들의 말이고, ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적 행동 양식이 반영된 것이라고 할 수 있다.\n",
        "이런 ‘연령 단계’에 의한 언어 차이 외에도, 기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도 있을 수 있다.\n",
        "젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를 쉽게 받아들인다.\n",
        "언어에서도 젊은 세대의 이런 특성이 반영된다.\n",
        "젊은 세대는 ‘반모’(반말모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와 신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는 젊은 세대를 중심으로 사용된다.\n",
        "이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7b2faa-7e8e-4ce7-aed0-3afd29a32b1e",
        "id": "a5tzXgOQgMgK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     546.35 ms\n",
            "llama_print_timings:      sample time =     199.41 ms /    62 runs   (    3.22 ms per token,   310.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1055.72 ms /   810 tokens (    1.30 ms per token,   767.25 tokens per second)\n",
            "llama_print_timings:        eval time =    1779.16 ms /    61 runs   (   29.17 ms per token,    34.29 tokens per second)\n",
            "llama_print_timings:       total time =    3194.70 ms /   871 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문:\n",
            "1. 유행은 누구의 주도적인 역할을 통해 형성되는지 설명해주세요.\n",
            "2. 어떤 요인들이 유행을 형성하는 데 중요한 역할을 하는지 설명하시오.\n",
            "3. 왜 사람들은 유행에 민감하고, 새로운 변화에 쉽게 적응하는지 설명해주세요.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{prompt_care}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text_care}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":5000,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":0.6,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model_care(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.31 시도"
      ],
      "metadata": {
        "id": "9_1uhN8qaNdW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8i1eKaZaMZR"
      },
      "source": [
        "## 03. 추론- RTF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFEiqc3SaMZS"
      },
      "outputs": [],
      "source": [
        "PROMPT = '''Role : \"You are a Korean teacher who runs a Korean college essay interview academy\"\n",
        "Task : \"Your main task is to generate example questions from the set of example statements. The number of statements in the set and the number of questions are not determined.\n",
        "Let me show you an example set of statements and an example question, which you can look at, and generate a question from a new set of statements similarly.\n",
        "It's just creating questions. The Questions must be generated in Korean, and concise in one or two sentences\"\n",
        "\n",
        "Format :\n",
        "\"First, I'll give you a set of statements as example. This set consists of 3 statements. All of statements are written in Korean.\n",
        "\n",
        "'\n",
        "[제시문 (가)]\n",
        "인간은 타인의 시선에서 벗어나 있을 때 과연 윤리적일 수 있는가?\n",
        "칸다울레스 왕이 다스리는 리디아 왕국에 기게스라는 목동이 살았다.\n",
        "기게스가 양을 치고 있던 어느 날 갑자기 커다란 지진이 일어났다.\n",
        "지진이 일어난 자리에는 땅이 갈라져 동굴이 생겼고, 기게스는 호기심이 생겨 갈라진 동굴 안으로 들어갔다.\n",
        "그는 동굴 안에서 거인의 시체가 놓여 있는 것을 발견하였다.\n",
        "시체의 손가락에는 금반지가 끼워져 있었다.\n",
        "기게스는 그 반지를 빼 들고 밖으로 나왔다.\n",
        "그러다 우연히 자신이 끼고 있는 반지의 흠집 난 곳을 안쪽으로 돌리면 자신은 투명 인간이 되고 바깥쪽으로 돌리면 자기 모습이 다시 보인다는 사실을 알게 되었다.\n",
        "이제 남들의 시선에서 벗어나 보이지 않는 힘을 갖게 된 기게스는 자연스럽게 나쁜 마음을 먹게 되었다.\n",
        "가축의 상태를 왕에게 보고하는 전령으로서 궁전에 들어간 기게스는 자신의 새로운 힘인 마법 반지를 이용하여 모습을 감춘 후, 왕비를 겁탈하고 그녀를 자기 편으로 끌어들여 왕을 암살한 뒤 스스로 왕이 되었다.\n",
        "\n",
        "[제시문 (나)]\n",
        "서로 잘 아는 사람들이 소규모로 모여 사는 마을 공동체에서 개인은 대체로 합리적이고 도덕적인 성향을 보인다.\n",
        "그는 다른 사람들의 말에 크게 영향받기보다는, 이성과 주관에 따라 판단하고 규범에 맞게 행동한다.\n",
        "혼자 있을 때도, 여럿이 있을 때와 크게 다를 바 없이 처신한다.\n",
        "그런데 19세기 이후 대도시에 인구가 밀집하고 대량생산과 소비, 그리고 대중문화가 발전하면서 대중사회가 등장한다.\n",
        "이제 대중 속에서 이름 없는 한 명이 된 개인은 집단적인 분위기에 복종하고 전체의 결정을 따르라는 무언의 압력에 쉽사리 굴복한다.\n",
        "대중의 일원으로서 그는 익명성 아래 자신의 욕망, 정열, 관심을 분출하고 실현한다. 이때 권력을 가진 지도자의 역할은 결정적이다.\n",
        "권위적 지도자는 단순하고 선동적인 말로 대중에게 행동 방향을 제시하며, 대중은 지도자의 말을 마치 절대적인 진리인 것처럼 이해한다.\n",
        "대중은 직관과 감정에 따라 권위적 지도자의 말을 무비판적으로 수용한다.\n",
        "무리 속의 익명적 개인들은 쉽게 흥분하고 변덕을 부리며 열정을 드러낸다.\n",
        "그러한 감정 에너지는 때로 대중이 난폭하게 폭력을 행사한다든지, 용감하게 순교를 불사하는 등의 행동을 하도록 만든다.\n",
        "\n",
        "[제시문 (다)]\n",
        "도시 문명의 발전은 현대 사회의 주요 특징이다.\n",
        "과학과 기술의 진전을 통해 이루어진 도시화는 익명성이 전통에 따르는 도덕규범과 오랜 대면 관계를 대신하도록 만들었다.\n",
        "그런데 우리는 도시인의 생존이 도시의 잔인한 익명성 탓에 소모되고 훼손된다는 말을 자주 듣는다.\n",
        "개인은 작은 시골 마을에 있을 때 이미 정해진 행동규범을 따르며, 스스로 그것을 의무로 여겼다.\n",
        "그러한 규범을 어길 때 마을에서 평판이 나빠지기 때문이다.\n",
        "하지만 그는 누가 누군지 알 수 없는 대도시로 나오면서부터 깊은 인간관계를 쌓을 수 없게 된다는 것이다.\n",
        "이런 까닭에 대중사회 속의 도시인은 마치 정체성을 상실하고 자아를 잃어버린 채로 살아간다고 비판받기까지 한다.\n",
        "그러나 이처럼 도시의 익명성이 과거 마을 공동체 시절의 인간적 교류를 사라지게 했다는비판은 그 익명성이 지니는 독특한 이점을 보지 못한 데서 나온다.\n",
        "도시의 익명성은 마을 공동체의 넌더리 나는 속박에서 벗어나는 자유의 가능성을 제공하므로 위협적이고 피폐한 것이 아니라 훨씬 더 인간적이고 해방적인 현상이다.\n",
        "왜냐하면 도시 생활의 익명성 형태는 인간 삶에 필수적인 사생활을 보호하는 데 도움을 주며 도시인을 마을 생활의 부담스러운 도덕규범과 강요된 인습이라는 족쇄에서 벗어나도록 만들기 때문이다.\n",
        "따라서 도시인은 이런 익명성 덕택에 과거와는 비교할 수 없을 만큼 많은 사람과 다양한 교류를 할 수 있게 되었을 뿐만 아니라 자유롭고 창의적인 생각을 펼칠 수 있게 되었다.\n",
        "'\n",
        "\n",
        "\n",
        "Next, let me show you an example question: this is generated from the above set of example statements.\n",
        "\n",
        "[예시 질문]\n",
        "'제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.'\n",
        "\n",
        "As such, example questions refer to the contents of the example statements and ask questions that can be summarized or thought about from them. At this time, it is necessary to consider the relationship between the contents of each presentation and ask questions that ask this.\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJori9bgaMZT"
      },
      "outputs": [],
      "source": [
        "test_text = '''다음 3개의 제시문을 보고 질문을 생성해주세요. [새로운 제시문]으로부터 질문을 생성하는 것입니다.\n",
        "질문만 생성해주세요. 다른 불필요한 문장은 삭제해 주세요.\n",
        "생성 형식은\n",
        "[\"질문 1\"\n",
        "\"질문 2\"\n",
        "\"질문 3\"]\n",
        "으로 부탁드립니다.\n",
        "질문을 3개 생성해주세요. 질문만 생성하면 됩니다. 질문은 한국어로, 1~2문장의 간결한 질문이 필요합니다. 답변은 필요 없습니다.\n",
        "\n",
        "[새로운 제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에\n",
        "충족시켜주는 매우 독특한 현상이다. 유행을 따름으로써 자신도 주변 사람들과 똑같이\n",
        "행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을\n",
        "얻으려는 심리가 복합적으로 얽혀 있는 것이다. 남을 따르고자 하는 욕구나 소망이 결여되는\n",
        "경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상\n",
        "존재하지 않게 된다. 예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을\n",
        "기피하는 집단에서는 유행이 발생하지 않는다. 1390년경 피렌체에서 남성 복장의 뚜렷한\n",
        "유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이\n",
        "모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다. 유행은\n",
        "결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[새로운 제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고\n",
        "동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다. 이제 사람들은 선조를\n",
        "닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다. 변화에 대한 애정과\n",
        "동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다. 이 원리들에는\n",
        "선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다. 옛것은 더 이상\n",
        "존경해야 하는 것으로 여겨지지 않는다. 사람들은 끊임없이 새것을 탐닉하며, 같은 시대에\n",
        "사는 사람들을 따라 한다. 예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀\n",
        "있었다. 그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다. 이탈리아, 스페인,\n",
        "프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다. 그들은 새로운\n",
        "것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다. 유행은 이런 식으로 사회 전체에\n",
        "확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서\n",
        "변화하였다.\n",
        "\n",
        "[새로운 제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을\n",
        "하게 된다. 그 행동양식 가운데 하나가 언어 사용이다. 가령, ‘엄마, 아빠’와 같은 말은\n",
        "어린이들의 말이고, ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적\n",
        "행동 양식이 반영된 것이라고 할 수 있다. 이런 ‘연령 단계’에 의한 언어 차이 외에도,\n",
        "기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도\n",
        "있을 수 있다. 젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를\n",
        "쉽게 받아들인다. 언어에서도 젊은 세대의 이런 특성이 반영된다. 젊은 세대는 ‘반모’(반말\n",
        "모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와\n",
        "신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는\n",
        "젊은 세대를 중심으로 사용된다. 이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "                '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = Llama(\n",
        "    model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dbef0dd-031f-48fc-ef9b-4256fef3e485",
        "id": "_UBbg3Y9aMZT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 145088\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,145088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,145088]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,145088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,296982]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 144782\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 144783\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens definition check successful ( 306/145088 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 145088\n",
            "llm_load_print_meta: n_merges         = 296982\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.17 B\n",
            "llm_load_print_meta: model size       = 4.66 GiB (4.91 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 144782 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 144783 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 144791 '<|eot_id|>'\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   318.80 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4457.43 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 5024\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   628.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  628.00 MiB, K (f16):  314.00 MiB, V (f16):  314.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.55 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   355.82 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    17.82 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '144783', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '145088', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '144782', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c5f684-cf9f-4746-fe07-6d00093aa8e9",
        "id": "3dBQOaSiaMZT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     539.71 ms\n",
            "llama_print_timings:      sample time =     258.62 ms /   109 runs   (    2.37 ms per token,   421.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3303.24 ms /   109 runs   (   30.30 ms per token,    33.00 tokens per second)\n",
            "llama_print_timings:       total time =    3866.21 ms /   109 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are three sample questions which can be generated from the above new set of example statements.\n",
            "\n",
            "[질문 1]\n",
            "유행에 대한 새로운 경향이 나타나는 경우, 어떤 원인들이 이를 이끌어나가는지 설명해주세요?\n",
            "\n",
            "[질문 2]\n",
            "최근에는 다양한 유행이 나타나면서 사람들 사이에서 어떤 요소가 중요한 역할을 하는지 설명해주세요.\n",
            "\n",
            "[질문 3]\n",
            "어떤 분야에서 유행이 생겨난다면, 해당 분야에서 유행을 따르는 사람들의 행동 양식은 어떻게 형성되는지 설명해주세요.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":500,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":1,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 새로운 제시문에 접목시켜보기"
      ],
      "metadata": {
        "id": "yvY5414ZtAvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_2 = '''Role : \"You are a Korean teacher who runs a Korean college essay interview academy\"\n",
        "Task : \"Your main task is to generate example questions from the set of new statements.\n",
        "Let me show you an example set of statements and an example question, which you can look at, and generate a question from a new set of statements similarly.\n",
        "This set of presentations contains content on one common topic. You should generate questions related to this common topic.\n",
        "\"\n",
        "\n",
        "Format :\n",
        "\"First, I'll give you a set of statements as example. This set consists of 3 statements. All of statements are written in Korean.\n",
        "\n",
        "'\n",
        "[제시문 (가)]\n",
        "인간은 타인의 시선에서 벗어나 있을 때 과연 윤리적일 수 있는가?\n",
        "칸다울레스 왕이 다스리는 리디아 왕국에 기게스라는 목동이 살았다.\n",
        "기게스가 양을 치고 있던 어느 날 갑자기 커다란 지진이 일어났다.\n",
        "지진이 일어난 자리에는 땅이 갈라져 동굴이 생겼고, 기게스는 호기심이 생겨 갈라진 동굴 안으로 들어갔다.\n",
        "그는 동굴 안에서 거인의 시체가 놓여 있는 것을 발견하였다.\n",
        "시체의 손가락에는 금반지가 끼워져 있었다.\n",
        "기게스는 그 반지를 빼 들고 밖으로 나왔다.\n",
        "그러다 우연히 자신이 끼고 있는 반지의 흠집 난 곳을 안쪽으로 돌리면 자신은 투명 인간이 되고 바깥쪽으로 돌리면 자기 모습이 다시 보인다는 사실을 알게 되었다.\n",
        "이제 남들의 시선에서 벗어나 보이지 않는 힘을 갖게 된 기게스는 자연스럽게 나쁜 마음을 먹게 되었다.\n",
        "가축의 상태를 왕에게 보고하는 전령으로서 궁전에 들어간 기게스는 자신의 새로운 힘인 마법 반지를 이용하여 모습을 감춘 후, 왕비를 겁탈하고 그녀를 자기 편으로 끌어들여 왕을 암살한 뒤 스스로 왕이 되었다.\n",
        "\n",
        "[제시문 (나)]\n",
        "서로 잘 아는 사람들이 소규모로 모여 사는 마을 공동체에서 개인은 대체로 합리적이고 도덕적인 성향을 보인다.\n",
        "그는 다른 사람들의 말에 크게 영향받기보다는, 이성과 주관에 따라 판단하고 규범에 맞게 행동한다.\n",
        "혼자 있을 때도, 여럿이 있을 때와 크게 다를 바 없이 처신한다.\n",
        "그런데 19세기 이후 대도시에 인구가 밀집하고 대량생산과 소비, 그리고 대중문화가 발전하면서 대중사회가 등장한다.\n",
        "이제 대중 속에서 이름 없는 한 명이 된 개인은 집단적인 분위기에 복종하고 전체의 결정을 따르라는 무언의 압력에 쉽사리 굴복한다.\n",
        "대중의 일원으로서 그는 익명성 아래 자신의 욕망, 정열, 관심을 분출하고 실현한다. 이때 권력을 가진 지도자의 역할은 결정적이다.\n",
        "권위적 지도자는 단순하고 선동적인 말로 대중에게 행동 방향을 제시하며, 대중은 지도자의 말을 마치 절대적인 진리인 것처럼 이해한다.\n",
        "대중은 직관과 감정에 따라 권위적 지도자의 말을 무비판적으로 수용한다.\n",
        "무리 속의 익명적 개인들은 쉽게 흥분하고 변덕을 부리며 열정을 드러낸다.\n",
        "그러한 감정 에너지는 때로 대중이 난폭하게 폭력을 행사한다든지, 용감하게 순교를 불사하는 등의 행동을 하도록 만든다.\n",
        "\n",
        "[제시문 (다)]\n",
        "도시 문명의 발전은 현대 사회의 주요 특징이다.\n",
        "과학과 기술의 진전을 통해 이루어진 도시화는 익명성이 전통에 따르는 도덕규범과 오랜 대면 관계를 대신하도록 만들었다.\n",
        "그런데 우리는 도시인의 생존이 도시의 잔인한 익명성 탓에 소모되고 훼손된다는 말을 자주 듣는다.\n",
        "개인은 작은 시골 마을에 있을 때 이미 정해진 행동규범을 따르며, 스스로 그것을 의무로 여겼다.\n",
        "그러한 규범을 어길 때 마을에서 평판이 나빠지기 때문이다.\n",
        "하지만 그는 누가 누군지 알 수 없는 대도시로 나오면서부터 깊은 인간관계를 쌓을 수 없게 된다는 것이다.\n",
        "이런 까닭에 대중사회 속의 도시인은 마치 정체성을 상실하고 자아를 잃어버린 채로 살아간다고 비판받기까지 한다.\n",
        "그러나 이처럼 도시의 익명성이 과거 마을 공동체 시절의 인간적 교류를 사라지게 했다는비판은 그 익명성이 지니는 독특한 이점을 보지 못한 데서 나온다.\n",
        "도시의 익명성은 마을 공동체의 넌더리 나는 속박에서 벗어나는 자유의 가능성을 제공하므로 위협적이고 피폐한 것이 아니라 훨씬 더 인간적이고 해방적인 현상이다.\n",
        "왜냐하면 도시 생활의 익명성 형태는 인간 삶에 필수적인 사생활을 보호하는 데 도움을 주며 도시인을 마을 생활의 부담스러운 도덕규범과 강요된 인습이라는 족쇄에서 벗어나도록 만들기 때문이다.\n",
        "따라서 도시인은 이런 익명성 덕택에 과거와는 비교할 수 없을 만큼 많은 사람과 다양한 교류를 할 수 있게 되었을 뿐만 아니라 자유롭고 창의적인 생각을 펼칠 수 있게 되었다.\n",
        "'\n",
        "\n",
        "\n",
        "Next, let me show you an example question: this is generated from the above set of example statements.\n",
        "\n",
        "[예시 질문]\n",
        "'제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.'\n",
        "\n",
        "As such, example questions refer to the contents of the example statements and ask questions that can be summarized or thought about from them.\n",
        "It's just creating questions. The Questions must be generated in Korean, and concise in one or two sentences\n",
        "At this time, it is necessary to consider the relationship between the contents of each presentation and ask questions that ask this.\"\n",
        "'''"
      ],
      "metadata": {
        "id": "3rvWtkqys9zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_2 = '''다음 3개의 제시문을 보고 질문을 생성해주세요. 이때 유의사항이 몇 가지 있습니다.\n",
        "첫 번째, [새로운 제시문]으로부터 질문을 생성하는 것입니다. 질문만 생성하고, 다른 불필요한 문장은 삭제해 주세요. 존대어를 사용하지 마세요.\n",
        "두 번째, 생성 형식은\n",
        "'\n",
        "[질문 1]\n",
        "\"첫 번째 질문\"\n",
        "[질문 2]\n",
        "\"두 번째 질문\"\n",
        "[질문 3]\n",
        "\"세 번째 질문\"\n",
        "'\n",
        "으로 부탁드립니다.\n",
        "세 번째, 질문을 3개 생성해주세요. 질문은 한국어로, 1~2문장의 간결한 질문이 필요합니다. 답변은 필요 없습니다.\n",
        "마지막으로, 질문은 진중한 어투로 제공되어야 합니다.\n",
        "\n",
        "그렇다면 이제 제시문을 드리겠습니다.\n",
        "\n",
        "[새로운 제시문 (가)]\n",
        "진실을 추구하지만 이야기라는 틀을 벗어날 수 없는 혼종 학문인 역사학은 인문학의 경계에 위치하면서 다른 학문보다 더 어렵기도 하고 더 쉽기도 하다.\n",
        "역사가들은 원하는 정보 모두를 획득할 때까지 사료를 끊임없이 파헤치고, ‘사실’을 다루는 자신들의 깊이를 앞세워 여타 학문의 동료들을 괴롭히는 콧대 높은 경험주의자들이다.\n",
        "이와 동시에 역사책은 흔히 이야기를 중심으로 전개되며, 가장 성공적인 역사서들은 대체로 훌륭한 소설의 속성을 일정하게 갖고 있다.\n",
        "역사학의 본질적 혼종성은 과거를 재구성하는 데 있어서 사실성과 허구성 사이의 경계에 관한 논쟁의 핵심적 이유이다.\n",
        "\n",
        "[새로운 제시문 (나)]\n",
        "크리스토퍼 브라우닝(Christopher Browning)은 1942~1943년에 걸쳐 약 38,000명의 유대인 학살 명령을 수행한 독일 101예비경찰대의 재판 기록을 통해 ‘평범한 사람들’이 학살에 가담했던 이유를 설명한다.\n",
        "유대인을 죽이라는 명령을 받고 당황한 대원들에게 상관은 나이가 좀 더 많은 사람들은 임무를 수행하지 못할 것 같으면 빠져도 좋다고 말했지만, 선택의 가능성에도 불구하고 80~90%의 대원들이 대량 학살에 가담했다.\n",
        "브라우닝은 사회적 관계로 인해 나약한 인간이 부당한 일을 행할 수 있다고 보았다.\n",
        "순응주의, 권위에 대한 복종, 임무를 거부할 때 동료들로부터 따돌림을 당할지도 모른다는 두려움이 학살 가담의 결정적 원인이라는 것이다.\n",
        "브라우닝은 무엇이 보통 사람들을 그토록 잔혹한 범죄에 가담하도록 이끌었는가를 이해하려 했던 것이고 그의 결론은 집단적 순응성의 압도적인 영향\n",
        "이었다.\n",
        "\n",
        "[새로운 제시문 (다)]\n",
        "대니얼 골드하겐(Daniel Goldhagen)은 브라우닝과 동일한 사료를 검토하고 정반대의 결론을 내렸다.\n",
        "그의 결론은 101예비경찰대의 압도적 다수가 동료들의 압력, 복종, 혹은 자신들의 경력 때문에 학살에 가담했던 것이 아니라, 섬뜩할 정도로 냉담하고 잔인한 행동을 묘사한 기록들에서 드러나듯 유대인 학살의 적극적 욕망을 가지고 행동했기 때문이라는 것이다.\n",
        "골드하겐은, 학살 가담이 내키지 않았고 자신들의 행동을 혐오했다는 대원들의 진술이 자기 변호에 불과하며, 그들은 ‘평범한 보통 사람들’이 아니라 ‘비정상적인 정치문화의 보통 사람들’이라고 보았다.\n",
        "그의 명제는 단순하고 명확하다.\n",
        "“독일인의 반유대주의적 신념이 홀로코스트를 유발한 핵심 동인이다.”\n",
        "골드하겐은 사회적 관계에 초점을 맞추기보다는 반유대주의라는 당시 독일 사회의 특수성을 문제시했다.\n",
        "그의 자명한 주장은 앞선 역사가들과 달랐지만, 상당한 대중적 찬사를 받았다.\n",
        "                '''"
      ],
      "metadata": {
        "id": "B0KriW4ZnwUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT_2}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":500,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":1,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dtfyG7Qooc0",
        "outputId": "9c25668f-63f4-4e6c-91d8-08f4917fae14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     790.26 ms\n",
            "llama_print_timings:      sample time =     340.12 ms /   126 runs   (    2.70 ms per token,   370.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2316.05 ms /  2054 tokens (    1.13 ms per token,   886.86 tokens per second)\n",
            "llama_print_timings:        eval time =    3695.48 ms /   125 runs   (   29.56 ms per token,    33.83 tokens per second)\n",
            "llama_print_timings:       total time =    6551.85 ms /  2179 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1]\n",
            "유행에 대한 주제와 그의 형태에 대해 설명해주세요. 어떤 원인들이 유행을 형성하는 것이죠?\n",
            "\n",
            "[질문 2]\n",
            "유행이 모든 사회적 층을 관통하는 것이 아니라 일부 사람들 사이에서만 유행하는 경우가 있을 수 있습니다. 어떤 사회적 요인 때문에 일부 사람들 사이에서 유행이 형성되는 것일까요? 예를 들어 설명해주세요.\n",
            "\n",
            "[질문 3]\n",
            "유행은 왜 항상 변화할까요? 어떤 요인들이 유행의 변화를 일으킵니다?\n",
            "유행이 멈추는 이유는 무엇인가요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YNIv_PhfuB5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiBl_Ri021C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새로운 시도"
      ],
      "metadata": {
        "id": "7QCFrjlF211e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for 문으로 질문 독립적으로 생성"
      ],
      "metadata": {
        "id": "bzo-1DoZHm3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, verbose = 0)\n",
        "model = Llama(\n",
        "    model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=-1, verbose = 0        # Number of model layers to offload to GPU\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4a8363-f18a-4884-9f29-20f8a35651e0",
        "id": "_e0-ueme21IJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OLQ2ct93K2z"
      },
      "outputs": [],
      "source": [
        "PROMPT = '''Role : \"You are a Korean teacher who runs a Korean college essay interview academy\"\n",
        "Task : \"Your main task is to generate example questions from the set of example statements.\n",
        "Let me show you an example set of statements and an example question, which you can look at, and generate a question from a new set of statements similarly.\n",
        "It's just creating questions. The Questions must be generated in Korean, and concise in one or two sentences\"\n",
        "\n",
        "Format :\n",
        "\"First, I'll give you a set of statements as example. This set consists of 3 statements. All of statements are written in Korean.\n",
        "\n",
        "'\n",
        "[제시문 (가)]\n",
        "인간은 타인의 시선에서 벗어나 있을 때 과연 윤리적일 수 있는가?\n",
        "칸다울레스 왕이 다스리는 리디아 왕국에 기게스라는 목동이 살았다.\n",
        "기게스가 양을 치고 있던 어느 날 갑자기 커다란 지진이 일어났다.\n",
        "지진이 일어난 자리에는 땅이 갈라져 동굴이 생겼고, 기게스는 호기심이 생겨 갈라진 동굴 안으로 들어갔다.\n",
        "그는 동굴 안에서 거인의 시체가 놓여 있는 것을 발견하였다.\n",
        "시체의 손가락에는 금반지가 끼워져 있었다.\n",
        "기게스는 그 반지를 빼 들고 밖으로 나왔다.\n",
        "그러다 우연히 자신이 끼고 있는 반지의 흠집 난 곳을 안쪽으로 돌리면 자신은 투명 인간이 되고 바깥쪽으로 돌리면 자기 모습이 다시 보인다는 사실을 알게 되었다.\n",
        "이제 남들의 시선에서 벗어나 보이지 않는 힘을 갖게 된 기게스는 자연스럽게 나쁜 마음을 먹게 되었다.\n",
        "가축의 상태를 왕에게 보고하는 전령으로서 궁전에 들어간 기게스는 자신의 새로운 힘인 마법 반지를 이용하여 모습을 감춘 후, 왕비를 겁탈하고 그녀를 자기 편으로 끌어들여 왕을 암살한 뒤 스스로 왕이 되었다.\n",
        "\n",
        "[제시문 (나)]\n",
        "서로 잘 아는 사람들이 소규모로 모여 사는 마을 공동체에서 개인은 대체로 합리적이고 도덕적인 성향을 보인다.\n",
        "그는 다른 사람들의 말에 크게 영향받기보다는, 이성과 주관에 따라 판단하고 규범에 맞게 행동한다.\n",
        "혼자 있을 때도, 여럿이 있을 때와 크게 다를 바 없이 처신한다.\n",
        "그런데 19세기 이후 대도시에 인구가 밀집하고 대량생산과 소비, 그리고 대중문화가 발전하면서 대중사회가 등장한다.\n",
        "이제 대중 속에서 이름 없는 한 명이 된 개인은 집단적인 분위기에 복종하고 전체의 결정을 따르라는 무언의 압력에 쉽사리 굴복한다.\n",
        "대중의 일원으로서 그는 익명성 아래 자신의 욕망, 정열, 관심을 분출하고 실현한다. 이때 권력을 가진 지도자의 역할은 결정적이다.\n",
        "권위적 지도자는 단순하고 선동적인 말로 대중에게 행동 방향을 제시하며, 대중은 지도자의 말을 마치 절대적인 진리인 것처럼 이해한다.\n",
        "대중은 직관과 감정에 따라 권위적 지도자의 말을 무비판적으로 수용한다.\n",
        "무리 속의 익명적 개인들은 쉽게 흥분하고 변덕을 부리며 열정을 드러낸다.\n",
        "그러한 감정 에너지는 때로 대중이 난폭하게 폭력을 행사한다든지, 용감하게 순교를 불사하는 등의 행동을 하도록 만든다.\n",
        "\n",
        "[제시문 (다)]\n",
        "도시 문명의 발전은 현대 사회의 주요 특징이다.\n",
        "과학과 기술의 진전을 통해 이루어진 도시화는 익명성이 전통에 따르는 도덕규범과 오랜 대면 관계를 대신하도록 만들었다.\n",
        "그런데 우리는 도시인의 생존이 도시의 잔인한 익명성 탓에 소모되고 훼손된다는 말을 자주 듣는다.\n",
        "개인은 작은 시골 마을에 있을 때 이미 정해진 행동규범을 따르며, 스스로 그것을 의무로 여겼다.\n",
        "그러한 규범을 어길 때 마을에서 평판이 나빠지기 때문이다.\n",
        "하지만 그는 누가 누군지 알 수 없는 대도시로 나오면서부터 깊은 인간관계를 쌓을 수 없게 된다는 것이다.\n",
        "이런 까닭에 대중사회 속의 도시인은 마치 정체성을 상실하고 자아를 잃어버린 채로 살아간다고 비판받기까지 한다.\n",
        "그러나 이처럼 도시의 익명성이 과거 마을 공동체 시절의 인간적 교류를 사라지게 했다는비판은 그 익명성이 지니는 독특한 이점을 보지 못한 데서 나온다.\n",
        "도시의 익명성은 마을 공동체의 넌더리 나는 속박에서 벗어나는 자유의 가능성을 제공하므로 위협적이고 피폐한 것이 아니라 훨씬 더 인간적이고 해방적인 현상이다.\n",
        "왜냐하면 도시 생활의 익명성 형태는 인간 삶에 필수적인 사생활을 보호하는 데 도움을 주며 도시인을 마을 생활의 부담스러운 도덕규범과 강요된 인습이라는 족쇄에서 벗어나도록 만들기 때문이다.\n",
        "따라서 도시인은 이런 익명성 덕택에 과거와는 비교할 수 없을 만큼 많은 사람과 다양한 교류를 할 수 있게 되었을 뿐만 아니라 자유롭고 창의적인 생각을 펼칠 수 있게 되었다.\n",
        "'\n",
        "\n",
        "Next, let me show you an example question: this is generated from the above set of example statements.\n",
        "\n",
        "[예시 질문]\n",
        "'제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.'\n",
        "\n",
        "As such, example questions refer to the contents of the example statements and ask questions that can be summarized or thought about from them. At this time, it is necessary to consider the relationship between the contents of each presentation and ask questions that ask this.\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3YuRCHa3K25"
      },
      "outputs": [],
      "source": [
        "test_text = '''다음 3개의 제시문을 보고, 질문과 그에 대한 답변을 생성해주세요. [새로운 제시문]으로부터 질문을 생성하는 것입니다. 다른 불필요한 문장은 삭제해 주세요.\n",
        "질문과 답변은 모두 한국어로, 1~2문장의 간결한 질문이어야 합니다.\n",
        "형식은 다음과 같습니다.\n",
        "\n",
        "\"질문\"\n",
        "\"답변\"\n",
        "\n",
        "이제 새로운 제시문 세트를 보여드리겠습니다.\n",
        "\n",
        "[새로운 제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에\n",
        "충족시켜주는 매우 독특한 현상이다. 유행을 따름으로써 자신도 주변 사람들과 똑같이\n",
        "행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을\n",
        "얻으려는 심리가 복합적으로 얽혀 있는 것이다. 남을 따르고자 하는 욕구나 소망이 결여되는\n",
        "경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상\n",
        "존재하지 않게 된다. 예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을\n",
        "기피하는 집단에서는 유행이 발생하지 않는다. 1390년경 피렌체에서 남성 복장의 뚜렷한\n",
        "유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이\n",
        "모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다. 유행은\n",
        "결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[새로운 제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고\n",
        "동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다. 이제 사람들은 선조를\n",
        "닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다. 변화에 대한 애정과\n",
        "동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다. 이 원리들에는\n",
        "선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다. 옛것은 더 이상\n",
        "존경해야 하는 것으로 여겨지지 않는다. 사람들은 끊임없이 새것을 탐닉하며, 같은 시대에\n",
        "사는 사람들을 따라 한다. 예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀\n",
        "있었다. 그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다. 이탈리아, 스페인,\n",
        "프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다. 그들은 새로운\n",
        "것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다. 유행은 이런 식으로 사회 전체에\n",
        "확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서\n",
        "변화하였다.\n",
        "\n",
        "[새로운 제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을\n",
        "하게 된다. 그 행동양식 가운데 하나가 언어 사용이다. 가령, ‘엄마, 아빠’와 같은 말은\n",
        "어린이들의 말이고, ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적\n",
        "행동 양식이 반영된 것이라고 할 수 있다. 이런 ‘연령 단계’에 의한 언어 차이 외에도,\n",
        "기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도\n",
        "있을 수 있다. 젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를\n",
        "쉽게 받아들인다. 언어에서도 젊은 세대의 이런 특성이 반영된다. 젊은 세대는 ‘반모’(반말\n",
        "모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와\n",
        "신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는\n",
        "젊은 세대를 중심으로 사용된다. 이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "                '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ece449-53ef-40c7-adcd-18dd2e8f7035",
        "id": "hPAknKDa21IK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1]\n",
            "유행에 대한 다양한 관점이 포함되어 있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.\n",
            "\n",
            "[답변 1]\n",
            "제시문 11239년경 피렌체에서 남성 복장의 뚜렷한 유행이 존재하지 않았던 이유는 모두 각각의 독특한 방식으로 차려입기를 원했기 때문이다. \n",
            "유행이 전체로 지배하게 되면, 즉 처음에는 몇몇 사람들이 주도했다고 해서 모두가 따라 하게 되면, 그것이 옷이나 음악이든 더 이상 유행이라고 부르지 않는다. 유행은 곧 소멸된다.\n",
            "유행의 특성은 변화를 끊임없이 따르는 것이다. 예를 들어, 근대 유럽에서 상류사회는 변화에 대한 열정에 사로잡혀 있었다. 이탈리아, 스페인, 프랑스가 이끄는 유행을 다른 국가의 상류층 사람들이 앞다냥으로 모방했다. 이들은 새로이 생겨난 것에 대해 무조건적으로 뒤쫓아가려고 했으며, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
            "따라서, 유행은 어떤 시기에는 유행이 되는 것이 다음에는 유행이 되지 않는 경향이 있다. 이는 사회적인 관심사와 사람들의 욕구가 변함에 따라 유행이 변화하기 때문이다.\n",
            "\n",
            "[질문 2]\n",
            "피렌체에서 남성 복장의 뚜렷하고 뚜렷한 유행이 없었던 이유는 모두 각각의 독특한 방식으로 차려입기를 원했기 때문입니다. \n",
            "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주체를 만들었고, 그 후에 모두 따라 하게 됩니다. 따라서, 유행은 사람들마다 서로 다른 모습을 가지도록 해줍니다.\n",
            "\n",
            "[답변 2]\n",
            "네, 맞습니다. 유행이 모든 것을 통제하고 지배하게 되면, 사람들은 자신의 개성을 표현할 수 없게 됩니다. 예를 들어, 피렌체에서 남성 복장의 뚜렷하고 뚜렷한 유행이 없었던 시기에는, 사람들은 각자의 취향과 개성에 따라 옷을 차려입었기 때문에 서로 다른 모습을 보였습니다. \n",
            "하지만, 유행이 모든 것을 통제하고 지배하게 되면, 사람들은 유행에 따라 옷을 차려입기 때문에 서로 같은 모습을 가지게 됩니다. 따라서, 유행이 모든 것을 지배하면 다양성과 개성이 감소하게 됩니다.\n",
            "\n",
            "[질문 3]\n",
            "제시문 11239년경 피렌체에서 남성 복장의 뚜렷하고 고유한 유행이\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1]\n",
            "유행에 대한 다양한 관점이 포함되어 있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.\n",
            "\n",
            "[답변 1]\n",
            "제시문 (가), (나), (다)는 모두 유행에 대한 다양한 측면을 다루고 있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점은, 둘 다 유행이 인간의 행동과 심리적 욕구, 그리고 사회적 압력에 의해 형성된다는 점입니다. 반면에 제시문 (가)와 (나)의 차이점은, 제시문 (가)는 유행이 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 충족시키는 매우 독특한 현상이라고 말하고, 제시문 (나)는 유행이 새로운 것을 숭배하고 동시대 사람들을 모방하는 경향이 있다는 것을 말합니다. \n",
            "\n",
            "제시문 (다)와 (na)의 공통점은, 둘 다 언어 사용과 관련된 유행이 있으며, 세대 변화에 따라 유행어와 신조어가 변화한다는 점입니다. 제시문 (다)는 특히 언어의 다양성과 유행어에 대한 관심을 강조하며, 제시문 (나)는 젊은 세대가 새로운 변화를 쉽게 받아들여 유행에 민감한 경향이 있다는 것을 설명합니다.\n",
            "\n",
            "따라서, 제시문 (가), (나), (다)는 모두 유행을 다양한 측면에서 바라보며, 공통점과 차이점을 통해 그들의 특징과 변동을 이해할 수 있습니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1]\n",
            "유행에 대한 다양한 관점이 포함되어 있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.\n",
            "\n",
            "[답변 1]\n",
            "제시문 (가), (나), (다)는 모두 유행에 대한 다양한 측면을 다루고 있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점은, 둘 다 유행이 인간의 행동과 관련이 있으며, 개인의 자유로운 선택과 사회적 압력에 의해 영향을 받는다는 것이다. 반면에 제시문 (가)와 (나)의 차이점은, 제시문 (가)는 유행이 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 충족시키는 매우 독특한 현상이라고 말하고, 제시문 (나)는 유행이 새로운 것을 숭배하고 동시대 사람들을 모방하는 경향이 있다는 것이다. \n",
            "\n",
            "제시문 (다)와 (na)의 공통점은, 둘 다 언어 사용과 관련된 내용이 포함되어 있으며, 세대 변화에 따른 언어의 변화가 있다는 것이다. 제시문 (다)는 사람들이 사회적 존재이기 때문에 역할에 따라 행동양식이 달라지며, 이를 언어 사용에도 영향을 미친다고 말하고, 제시문 (나)는 젊은 세대가 새로운 변화를 쉽게 받아들인다는 점에서 세대 변화에 따른 언어 변화의 가능성을 언급하고 있다.\n"
          ]
        }
      ],
      "source": [
        "for i in range(3) :\n",
        "  model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id, verbose = 0)\n",
        "  model = Llama(\n",
        "      model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "      n_ctx=5000,\n",
        "      n_gpu_layers=-1, verbose = 0        # Number of model layers to offload to GPU\n",
        "  )\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{test_text}\"}\n",
        "      ]\n",
        "\n",
        "  prompt = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = False,\n",
        "      add_generation_prompt=True, verbose = 0\n",
        "  )\n",
        "\n",
        "  generation_kwargs = {\n",
        "      \"max_tokens\":500,\n",
        "      \"stop\":[\"<|eot_id|>\"],\n",
        "      \"top_k\" : 50,\n",
        "      \"top_p\":0.9,\n",
        "      \"temperature\":0.1,\n",
        "      \"echo\":True, # Echo the prompt in the output\n",
        "  }\n",
        "\n",
        "  resonse_msg = model(prompt, **generation_kwargs)\n",
        "  print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LluV4OML6qaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 질문 형식의 예시를 여러 개 보여주기"
      ],
      "metadata": {
        "id": "i0yz_JaKH0Lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, verbose = 0)\n",
        "model = Llama(\n",
        "    model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=-1, verbose = 0        # Number of model layers to offload to GPU\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298,
          "referenced_widgets": [
            "31ef79da5c5c4057a89f38c311bbb920",
            "cdae60a81c2d409bbf2784628013a0e0",
            "33a893320f5a40969bb3b23bae2880b3",
            "02e197c1f1514ebdbda102ecde23b4cc",
            "50e7f7042b504b598ec182e80665dabd",
            "6fcaccbadd254e86835979a4072a25c0",
            "82220b67353b47a39d783e8a94cfadc0",
            "fe108b7fdd364dc0b537cab29a612c68",
            "dfc7f0e413704ca1a54023da4a1f123f",
            "2511d5487f56407f92432a3920cb5c74",
            "5dff6eaef64841a186780474a2e566e6",
            "3b0a462efd8d4d04996445e9976c0f38",
            "8bc933052abb4215b259e508906851f5",
            "c13bf3f17c34411893c3deb12dce3568",
            "d71e214e002743fe98290cfec3f9702a",
            "63e2b02c95a6432ca1c43a51b50caba6",
            "a301eb4fd34f422981096fb71beab995",
            "78d383ce021449948a7aa5717f37e075",
            "f9f9a60d072f4374b3c991725c205135",
            "f49b142d432e4c28ba55a996552e7fc9",
            "dff6437605804f6094713a93b8a2dd42",
            "fdf2ed948e8642b1a5f9553fbff42444",
            "74fc111f2c064bcab473f7c2057bd290",
            "7b652bdaca53461a86e5b1f81fcc292f",
            "a2c1003488f845fdbd54f225a99bd0be",
            "f28e01b551fa4c43a89f7de903469669",
            "187818161f6f4b01b2729cdd59f38d7a",
            "28a31174163f4e08896c3b2c14064959",
            "1f9b4a4be2234ce296975cd566c9f6fb",
            "8e7d9ffaba5d43aa9ff560fc225a7682",
            "705c1d122a154927af649b3083b42b24",
            "9744075a51ae455e93e0806de1676b48",
            "8bf2ea552d614197a00d5e54158c3e6f"
          ]
        },
        "outputId": "3dc75d5f-153f-42c9-804a-bfc2072f619d",
        "id": "4ckyZqoiH0Lw"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31ef79da5c5c4057a89f38c311bbb920"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b0a462efd8d4d04996445e9976c0f38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74fc111f2c064bcab473f7c2057bd290"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oIuLP_jRH0Lw"
      },
      "outputs": [],
      "source": [
        "PROMPT = '''Role : \"당신은 대한민국에서 대입 논술 면접 학원을 운영하고 있는 선생님입니다.\"\n",
        "Task : \"당신의 주 업무는 예시 제시문 세트로부터 예시 질문을 생성하는 것입니다.\n",
        "예시 질문을 보여드리겠습니다. 당신은 새로운 제시문들이 주어지면, 제시문들의 내용을 파악한 후 이와 유사한 질문을 생성하면 됩니다.\n",
        "이 때 질문들은 제시문들 간의 연관성을 파악해 이를 물어보는 것입니다.\"\n",
        "\n",
        "Format :\n",
        "\"(가)에서 말한 역사학에서의 허구성을 구체적으로 설명하고, (나)와 (다)에서 발견되는 허구적 요소가 각각 무엇인지 설명하시오.\",\n",
        "\"(가)의 내용에 기반하여 (다)의 상황이 가능한 이유를 설명하시오.\",\n",
        "\"(가), (나)를 바탕으로, (라)의 ‘건강한 사회’가 유지될 수 있는 이유를 설명하시오.\",\n",
        "\"(가) 또는 (나)에서 문제가 된 상황과 유사한 다른 사례를 제시하고 그 이유를 설명하시오.\",\n",
        "\"제시문 (나)를 바탕으로 기술에 대한 제시문 (가)와 제시문 (다)의 주장을 설명하시오.\",\n",
        "\"제시문 (가), (나), (다)에는 익명성에 대한 다양한 관점이 들어있다. 제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.\",\n",
        "\"(가) 싱가포르의 ‘태형’과 (나) 흥보전의 ‘매품’을 서로 비교하시오.\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hl515G9H0Lw"
      },
      "outputs": [],
      "source": [
        "test_text = '''다음 3개의 제시문을 보고, 질문과 그에 대한 답변세트를 총 3개 생성해주세요. 다른 불필요한 문장은 모두 삭제해 주세요.\n",
        "질문과 답변은 모두 한국어로, 1~2문장으로 구성해주세요.\n",
        "각 질문들은 서로 다른 형태와 내용으로, 겹치지 않아야 합니다. 질문 부분에는 질문 자체만 생성해주세요.\n",
        "\n",
        "생성 형식은\n",
        "[\"질문 1\"]\n",
        "[\"답변 1\"]\n",
        "\n",
        "[\"질문 2\"]\n",
        "[\"답변 2\"]\n",
        "\n",
        "[\"질문 3\"]\n",
        "[\"답변 3\"]\n",
        "으로 부탁드립니다.\n",
        "\n",
        "질문과 답변 세트를 3개 생성해주세요.\n",
        "\n",
        "이제 새로운 제시문 세트를 보여드리겠습니다.\n",
        "\n",
        "[제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 충족시켜주는 매우 독특한 현상이다.\n",
        "유행을 따름으로써 자신도 주변 사람들과 똑같이 행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을 얻으려는 심리가 복합적으로 얽혀 있는 것이다.\n",
        "남을 따르고자 하는 욕구나 소망이 결여되는 경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상 존재하지 않게 된다.\n",
        "예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을 기피하는 집단에서는 유행이 발생하지 않는다.\n",
        "1390년경 피렌체에서 남성 복장의 뚜렷한 유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이 모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다.\n",
        "유행은 결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고 동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다.\n",
        "이제 사람들은 선조를 닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다.\n",
        "변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다.\n",
        "이 원리들에는 선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다.\n",
        "옛것은 더 이상 존경해야 하는 것으로 여겨지지 않는다.\n",
        "사람들은 끊임없이 새것을 탐닉하며, 같은 시대에 사는 사람들을 따라 한다.\n",
        "예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀 있었다.\n",
        "그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다. 이탈리아, 스페인, 프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다.\n",
        "그들은 새로운 것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다. 유행은 이런 식으로 사회 전체에 확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서 변화하였다.\n",
        "\n",
        "[제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을 하게 된다. 그 행동양식 가운데 하나가 언어 사용이다.\n",
        "가령, ‘엄마, 아빠’와 같은 말은 어린이들의 말이고, ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적 행동 양식이 반영된 것이라고 할 수 있다.\n",
        "이런 ‘연령 단계’에 의한 언어 차이 외에도, 기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도 있을 수 있다.\n",
        "젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를 쉽게 받아들인다.\n",
        "언어에서도 젊은 세대의 이런 특성이 반영된다. 젊은 세대는 ‘반모’(반말 모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와 신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는 젊은 세대를 중심으로 사용된다.\n",
        "이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "                '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a341834-a96d-4908-d209-45fcaf1c1c63",
        "id": "p03FbUZUH0Lx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문 1:\n",
            "[제시문 (가)]의 내용을 바탕으로, 어떤 이유로 사람들은 유행을 따르고 싶어하는 심리적 동기를 설명해주세요.\n",
            "\n",
            "답변 1:\n",
            "유행은 사람들이 자신들 주변의 다른 사람들과 일치하고 싶어하는 욕구와, 그들만의 개성을 표현하고자 하는 욕구가 복합적으로 얽혀 있는 것입니다. 따라서, 사람들은 상대들에게 인정과 안도감을 느끼기 위해 유행을 따르고 싶어합니다. 이는 그들이 주변 사람들에게 인정받고, 그들만의 스타일이 받아들여지고, 그들만의 개성이 존중받는 것을 의미합니다.\n",
            "\n",
            "질문 2:\n",
            "[제시문 (나)]의 내용을 바탕으로, 어떤 이유로 사람들은 옛 것들을 따라가고, 동시대 사람들을 모방하려는 심리적 동기를 설명해주세요.\n",
            "\n",
            "답변 2:\n",
            "옛것을 따르는 경향은 사람들의 변화를 평가절하하고, 현재의 규범을 중요시 하려는 경향이 함께 동반됩니다. 사람들은 과거를 존경하지 않고, 지금은 지금이 가장 중요한 것으로 여겨집니다. 또한, 사람들은 같은 시대를 사는 사람들과 연결하고자 하며, 동시대 사람들을 모방하려는 경향을 가지고 있습니다. 이는 사람들이 현재와 미래에 대한 불안감을 해소하고, 안정감을 느끼는 것을 의미합니다.\n",
            "\n",
            "질문 3:\n",
            "[제시문 (다)]의 내용을 바탕으로, 어떤 이유로 사람들은 언어 사용에서도 세대 차이와 연관이 있으며, 이를 어떻게 설명할 수 있을까요?\n",
            "\n",
            "답변 3:\n",
            "언어 사용에서도 세대 차이와 연관이 있습니다. 이는 사회적인 존재로서의 역할을 통해 자연스럽게 발생하는데, 예를 들어, '엄마, 아빠'와 같은 말은 어린이들의 말이고, '자네, 댁'과 같은 단어는 어른들의 말이라는 것이 그러한 예입니다. 또한, 새로운 세대가 언어를 만들고 유행어를 만들기도 하는데, 이는 또 다른 세대 차이 중 하나인 젊은 세대의 특성을 대변합니다. 따라서, 언어 사용에서 세대 차이와 연관이 있는 것은 그들이 사회적인 존재로서의 역할을 통해 자연스럽게 발생하는 것입니다.\n"
          ]
        }
      ],
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, verbose = 0)\n",
        "model = Llama(\n",
        "    model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=-1, verbose = 0        # Number of model layers to offload to GPU\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True, verbose = 0\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":500,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":1,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "print(resonse_msg['choices'][0]['text'][len(prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 건님의 instruction과 결합"
      ],
      "metadata": {
        "id": "MsnHqmG0E77M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_list = []"
      ],
      "metadata": {
        "id": "YLqZGw7xGNH_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZRJEekzaE_XU"
      },
      "outputs": [],
      "source": [
        "PROMPT = '''\n",
        "Role : \"당신은 대한민국에서 대입 논술 면접 학원을 운영하고 있는 선생님입니다.\"\n",
        "Task : \"당신의 주 업무는 예시 제시문 세트로부터 예시 질문을 생성입니다.\n",
        "예시 질문을 보여드리겠습니다. 당신은 새로운 제시문들이 주어지면, 제시문들의 내용을 파악한 후 이와 유사한 질문을 생성합니다.\n",
        "이 때 질문들은 제시문들 간의 연관성을 파악해 이를 물어보는 것입니다.\"\n",
        "\n",
        "Format :\n",
        "\"(다)의 사례들을 (가)와 (나)에 비추어 평가하시오.\",\n",
        "\"(가)의 내용에 기반하여 (다)의 상황이 가능한 이유를 설명하시오.\",\n",
        "\"(가), (나)를 바탕으로, (라)의 ‘건강한 사회’가 유지될 수 있는 이유를 설명하시오.\",\n",
        "\"(가) 또는 (나)에서 문제가 된 상황과 유사한 다른 사례를 제시하고 그 이유를 설명하시오.\",\n",
        "\"제시문 (나)를 바탕으로 기술에 대한 제시문 (가)와 제시문 (다)의 주장을 설명하시오.\",\n",
        "\"제시문 (가)와 (나)의 내용을 바탕으로 제시문 (다)의 ᄀ을 평가하시오.\",\n",
        "\"제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.\",\n",
        "\"(가)와 (나)에서 작가가 갖추어야 할 자질들을 찾아 차이점과 공통점을 설명하시오.\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qvMPbSY5E_XV"
      },
      "outputs": [],
      "source": [
        "test_text = '''다음 3개의 제시문을 보고, 질문과 그에 대한 답변세트를 3개 생성해야 합니다. 다른 불필요한 문장은 모두 삭제해 주세요.\n",
        "질문과 답변은 모두 한국어로, 1~2문장으로 구성해주세요.\n",
        "각 질문들은 서로 다른 형태와 내용으로, 겹치지 않아야 합니다. 질문 부분에는 질문 자체만 생성해주세요.\n",
        "\n",
        "생성 형식은\n",
        "[\"질문 1\"\n",
        "\"질문 2\"\n",
        "\"질문 3\"\n",
        "\"답변 1\"\n",
        "\"답변 2\"\n",
        "\"답변 3\"]\n",
        "으로 해야 합니다.\n",
        "\n",
        "질문은 반드시 명령조로 생성되어야 합니다.\n",
        "\n",
        "이제 새로운 제시문 세트를 보여드리겠습니다.\n",
        "\n",
        "[제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 충족시켜주는 매우 독특한 현상이다.\n",
        "유행을 따름으로써 자신도 주변 사람들과 똑같이 행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을 얻으려는 심리가 복합적으로 얽혀 있는 것이다.\n",
        "남을 따르고자 하는 욕구나 소망이 결여되는 경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상 존재하지 않게 된다.\n",
        "예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을 기피하는 집단에서는 유행이 발생하지 않는다.\n",
        "1390년경 피렌체에서 남성 복장의 뚜렷한 유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이 모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다.\n",
        "유행은 결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고 동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다.\n",
        "이제 사람들은 선조를 닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다.\n",
        "변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다.\n",
        "이 원리들에는 선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다.\n",
        "옛것은 더 이상 존경해야 하는 것으로 여겨지지 않는다.\n",
        "사람들은 끊임없이 새것을 탐닉하며, 같은 시대에 사는 사람들을 따라 한다.\n",
        "예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀 있었다.\n",
        "그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다. 이탈리아, 스페인, 프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다.\n",
        "그들은 새로운 것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다. 유행은 이런 식으로 사회 전체에 확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서 변화하였다.\n",
        "\n",
        "[제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을 하게 된다. 그 행동양식 가운데 하나가 언어 사용이다.\n",
        "가령, ‘엄마, 아빠’와 같은 말은 어린이들의 말이고, ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적 행동 양식이 반영된 것이라고 할 수 있다.\n",
        "이런 ‘연령 단계’에 의한 언어 차이 외에도, 기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도 있을 수 있다.\n",
        "젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를 쉽게 받아들인다.\n",
        "언어에서도 젊은 세대의 이런 특성이 반영된다. 젊은 세대는 ‘반모’(반말 모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와 신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는 젊은 세대를 중심으로 사용된다.\n",
        "이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "                '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402114a5-3e8c-43c0-fed8-07d42eb9b07f",
        "id": "iHAWFkbaE_XW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1]\n",
            "제시문 (가)를 기반으로 질문을 만들면 다음과 같습니다.\n",
            "가) 주어진 시기 동안 유행이 변화한 경우, 어떤 요인들이 그 변화를 가져왔을까요?\n",
            "나) 전통적인 가치에 대한 반항과 현대 사회의 변화에 대한 수용성이 강한 시대에서는 유행이 어떻게 형성될 수 있을까요?\n",
            "다) 어떤 집단은 옛 것들에 집착하고, 다른 사람들은 새로운 것에 열중하는 경우, 왜 이렇게 나타나는 것이일까요?\n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)를 기반으로 질문을 만들면 다음과 같습니다.\n",
            "가) 오래된 관습을 따르는 경향이 있는 문화에서는 어떠한 것들이 유행이 될 수 있을까요?\n",
            "나) 어떤 사회에서는 변화에 대한 애정과 동시대인들에 대한 모방이 강력한 영향을 미치는지 분석해보세요.\n",
            "다) 어떤 문화에서는 사람들이 가장 좋아하는 것, 즉 유행에 대해 어떻게 생각하느냐를 살펴봅시다.\n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)를 기반으로 질문을 만들면 다음과 같습니다.\n",
            "가) 언어 사용에서 어떠한 요인들이 유행이 되는 것을 설명하는 역할을 할 수 있을까요?\n",
            "나) 어떤 사회에서는 청소년층과 어른들의 언어 사용할 방식이 다르게 형성되는지 분석해보세요.\n",
            "다) 왜 사람들은 항상 새로운 유행에 관심을 가지며, 어떤 요인들이 유행의 변화가 발생하는지를 설명해주세요.\n"
          ]
        }
      ],
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, verbose = 0)\n",
        "model = Llama(\n",
        "    model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=-1, verbose = 0        # Number of model layers to offload to GPU\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True, verbose = 0\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":500,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.9,\n",
        "    \"temperature\":1,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "generated = resonse_msg['choices'][0]['text'][len(prompt):]\n",
        "print(generated)\n",
        "qa_list.append(generated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for a in range(len(qa_list)):\n",
        "  print(qa_list[a])\n",
        "  print('*'*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNxNaiSuGMBr",
        "outputId": "cf9d3274-9062-4e34-979d-d830bc5635a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1]\n",
            "유행은 어떤 심리적 원리와 사회적인 원리 때문에 생겨나는 것인가?\n",
            "\n",
            "[답안 1]\n",
            "유행은 크게 세 가지 심리적 원리와 사회적 원리로 설명될 수 있습니다. 첫째, 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 만족시키기 위해 나타납니다. 둘째, 다른 사람들과 구별되는 만족감을 얻으려고 하는 심리 때문입니다. 셋째, 변화를 수용할 수 있는 문화적인 요소 때문이죠. 이는 1390년경 피렌체에서 남성 복장의 뚜렷 유행이 나타나 이전과 이후에는 존재하지 않았던 것을 예시 들 수 있습니다.\n",
            "\n",
            "[질문 2]\n",
            "옛것을 따르는 습관은 어떤 원리와 시기에 가장 많이 나타났을까요?\n",
            "\n",
            "[답안 2]\n",
            "옛것을 따라가는 습관은 변화에 대한 애정과 동시대인들에 대한 모방 본능이 결합된 것으로 설명할 수 있습니다. 이 원리는 선조를 따르는 관습의 시대에 지배했던 것입니다. 지금은 주위 사람들을 닮으려 있는 경향이 유행의 시대를 특징 짓습니다. 이러한 원리들은 옛 것을 평가 절하하고 현재의 규범을 중요시 하려는 경향이 함께 동반됩니다. \n",
            "\n",
            "[질문 3]\n",
            "언어는 어떤 사회적, 심리적 요인에 의해 변화할 수 있는지 설명해주세요.\n",
            "\n",
            "[답안 3]\n",
            "언어는 사회적인 존재이기 때문에 위치에 따라 그 행동양식이 달라지게 됩니다. 따라서, 언어 사용 역시 그러한 사회적 행동 양식에서 영향을 받게 됩니다. 예를 들어, '엄마, 아빠'와 같은 말은 어린이들의 말이고, '자네, 댁'과 같은 단어는 어른들의 말이라는 것은 나이에 따른 사회적 행동 양식이 반영된 것입니다. 또한, 젊은 세대는 새로운 변화에 민감하고, 새로운 변화를 쉽게 받아들이는 경향이 있는데, 이러한 특성에서는 '반모', '인싸'와 같은 유행어와 신조어가 만들어지는 것입니다. 이것은 세대 변화에 따른 언어의 유행 현상으로 이해할 수 있습니다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "유행은 모든 연령대의 사람들에게 어떤 영향을 미치는가요?\n",
            "\n",
            "[답변 1]\n",
            "유행은 주로 청소년층에게 크게 영향을 미치며, 그 이후로 어른들 사이에서도 유행이 생겨나 사회적 영향력이 확산됩니다. 유행은 사람들 사이의 경쟁 사회에서 중요한 역할을 하며, 개인의 심리적인 요인도 영향을 미칩니다.\n",
            "\n",
            "[질문 2]\n",
            "유행이 나타나면 어떤 현상은 발생하게나요?\n",
            "\n",
            "[답변 2]\n",
            "유행이 나타나면 많은 사람들이 그 유행을 따라하게 되고, 이로 인해 유행이 확산됩니다. 유행을 따르는 것은 사람들 사이의 동질감을 느끼려는 욕구에서 비롯되며, 이는 더욱 다양한 분야에서 유행이 생겨나게 만듭니다. 이러한 확산 현상은 사회적으로 큰 영향을 미치게 됩니다.\n",
            "\n",
            "[질문 3]\n",
            "유행의 시기적 변화는 어떻게 일어나는 걸까요?\n",
            "\n",
            "[답변 3]\n",
            "유행의 시기적 변화는 세대 교체 현상과 관련이 있습니다. 새로운 세대가 등장하면서 유행이 변화하게 되는데, 이는 세대들의 심리적 특성에서 비롯됩니다. 더 젊은 세대일수록 더 많은 새로운 변화에 민감하고, 따라서 유행에 영향을 주는 경향이 있습니다. 이러한 이유로, 유행은 지속적으로 변화하며, 새로운 세대의 출현과 함께 그 변화가 이루어집니다.\n",
            "**************************************************\n",
            "질문 1:\n",
            "제시문 (가)를 기반으로, 어떤 사람들은 모든 개인이 제각각 특수한 존재라고 생각하기 때문에 유행이 발생하지 않는다고 주장하는지 설명해주세요.\n",
            "\n",
            "질문 2:\n",
            "제시문 (나)를 중심으로, 옛것이나 선조를 따라가는 관습의 시대에 있는 사람들은 어떤 동기를 가지고 새로운 것을 숭배하고 동시대 사람들을 모방하는 경향이 있는지 설명해주세요.\n",
            "\n",
            "질문 3:\n",
            "제시문 (다)를 기반으로, 언어 사용은 사회적인 존재로서 역할에 따라 달라지는 행동양식 중 하나라는 것을 언급하고, 특히 젊은 세대의 유행어와 신조어 사용이 어떤 사회적 변화를 반영하는지를 설명해주세요.\n",
            "**************************************************\n",
            "질문 1:\n",
            "[제시문 (가)] 에서 언급된 내용에 따라, 누구나 모방 욕구와 자신을 표현하고 싶은 성격 욕구를 모두 충족시키는 것이 유행의 특징이라는 것을 강조하실 수 있을까요?\n",
            "\n",
            "답변 1:\n",
            "네, 유행은 누구든 모방을 따른다는 것과 동시에 자신감 있는 개성을 보장해주는 동시에 둘 다 충족되는 특이한 현상입니다. 사람들은 유행을 따르는 과정에서 스스로와 주변 사람들과 차별화하는 자기 발상을 시도하며, 자신의 개성과 주변 사람들의 평소에 충실한 자신을 실현하기 위해 노력하고 있습니다.\n",
            "\n",
            "질문 2:\n",
            "[제시문 (나)] 에서 언급된 내용에 따라, 옛 것을 따르는 경향이 관습의 시절을 지배한다면, 새로운 것을 숭배하고 동시대 사람들을 모방하려는 여유가 유행의 시대를 특징으로 한다는 것을 강조하실 수 있을까요?\n",
            "\n",
            "답변 2:\n",
            "네, 그렇습니다. 사람들은 옛 것을 따라가는 것이 아니라, 새로운 것을 살펴보고 모방합니다. 지금 사람들은 자신들만의 방식으로 새로운 것을 만들고, 그 과정에서 기존의 관습을 버리는 경향이 있습니다. 이러한 변화는 동시대 사람들 사이에서 유행하며, 새로운 것을 만드는 데 있어서도 큰 영향력을 끼치고 있습니다.\n",
            "\n",
            "질문 3:\n",
            "[제시문 (다)] 에서 언급된 내용에 따라, 사람들이 언어를 사용하는 것은 그들이 사회적인 존재이기 때문입니다. 따라서, 나이나 세대별로 차이가 있는 언어 사용은 물론, 사람들끼리 상호작용하는 방식에도 영향을 미치며, 유행이 될 수 있습니다.\n",
            "\n",
            "답변 3:\n",
            "네, 정확한 말입니다. 언어 사용은 바로 사람들이 사회적인 존재임을 보여주는 중요한 증거입니다. 그리고유행은 대대로 전해져 오는 문화적인 관습이나 일반적인 행동 양식입니다. 따라서, 사람들이 어떤 사회적 역할을 수행하는지에 따라 그 역할에 맞는 행동 양식인 유행이 형성되고, 다시 말해 사회적인 존재이기 때문입니다.\n",
            "**************************************************\n",
            "질문 1:\n",
            "[제시문 (가)] 에서 말하는 화제에 대한 질문을 생성해주세요.\n",
            "\n",
            "답변 1:\n",
            "[제시문 (가)]에서 말하는 화제는 사람들이 어떤 것을 모방하려는 욕구와 자신을 표현하고자 하는 개성 욕구가 복합적으로 얽혀 있는 것입니다. 따라서, 화자는 누구의 주도적인 역할을 수행하는지에 대해 묻고 있습니다.\n",
            "\n",
            "질문 2:\n",
            "[제시문 (나)] 에서 말하는 관습의 시대에 대한 질문을 생성해주세요.\n",
            "\n",
            "답변 2:\n",
            "[제시문 (나)]에서 말하는 관습의 시기는 오래된 것을 따라가는 경향이 있었습니다. 이 시기에서는 새로운 것을 숭배하고 동시대 사람들을 모방하는 경향이 유행의 시기를 특징으로 했습니다. \n",
            "\n",
            "질문 3:\n",
            "[제시문 (다)] 에서 말하는 사회적인 존재로서의 언어 사용에 대한 질문을 생성해주세요.\n",
            "\n",
            "답변 3:\n",
            "[제시문 (다)]에서 말하는 사회적인 있는 언어 사용은 그들이 속한 사회적 집단에 따라 다릅니다. 예를 들어, 어린 세대는 아직 독립적이지 않은 상황에서 부모로부터의 보호가 필요한 반면, 노인층 세대는 스스로 독립적이고 자유롭게 행동할 수 있는 상황이 필요합니다. 따라서, 각 세대마다 그에 맞는 행동 양식, 즉 언어 사용이 달라집니다.\n",
            "**************************************************\n",
            "질문 1:\n",
            "제시문 (가)를 기반으로, 어떤 것들이 사람들이 유행을 따르는 욕구를 유발하는 주된 원리인지 설명하시오.\n",
            "\n",
            "답변 1:\n",
            "유행은 사람들 사이에서 어떤 행동이나 취미가 급격한 인기를 끌 때 쓰이는 말로, 주로 패션, 음악, Dance 등에 이용된다. 유행이 생겨나는 이유는 사람들이 자신감과 자아표현 욕구를 충족시키기 위해 모방하려는 욕구가 있기 때문이다. 따라서, 어떤 행동이나 취미가 유행이 될 때는 그 행동을 따라 하는 것이 자랑거리 되고, 다른 사람들과 차별화할 수 있는 방법이라고 생각되어진다. 또한, 유행을 따르는 것은 이미 확립된 가치체계가 개인의 자유로운 선택에 의해 작용한다는 것을 보여줍니다. \n",
            "\n",
            "질문 2:\n",
            "제시문 (나)를 기반으로, 어떤 요인들이 전통을 따르는 욕구를 유발하는 주된 요인이라고 설명하시오.\n",
            "\n",
            "답변 2:\n",
            "전통에 대한 애정과 동시대인들에 대한 모방이 유행의 시기를 특징 짓는다. 사람들은 전통을 따르면서 자신의 고유한 문화와 가치를 지키려는 욕구와 함께, 동시대 사람들로부터 존경받을 수 있는지도 의식하게 된다. 따라서, 전통은 동질감을 느끼게 하며, 개인의 자부심과 집단 속성을 동시에 만족시켜주는 역할을 한다. 예를 들어, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀 있었지만, 동시에 전통적인 가치관과 유산을 지키려는 욕구로 인해 전통을 따르기였다.\n",
            "\n",
            "질문 3:\n",
            "제시문 (다)를 기반으로, 어떤 요인들이 사회적인 존재로서의 역할을 통해 언어 사용에서 차이를 보이는지 설명하시오.\n",
            "\n",
            "답변 3:\n",
            "언어 사용에서 차이는 주로 사회적인 역할과 연관되어 나타난다는 것을 알 수 있습니다. 예를 들어, '반모'와 같은 유행어는 주로 젊은 세대에서 많이 쓰이며, 이들은 자주 새로운 변화를 받아들이고 자신의 독창성을 표현하기 위해 말을 잘 익히고 유행어를 많이 사용합니다. 반면에, 노인층이나 다른 연령층들은 이미 굳어진 언어를 사용해 전통적인 언어 패턴을 유지하려는 경향이 있습니다. 이러한 현상은 세대 변화에 따라 달라질 수 있으며, 유행어와 전통적인 언어의 변화는 사회적인 역할과 관련된 것이다.\n",
            "**************************************************\n",
            "질문 1:\n",
            "[제시문 (가)] 에서 언급된 내용에 따라, 누구나 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 충족시키는 특이한 현상이 무엇인지 설명하시오.\n",
            "\n",
            "답변 1:\n",
            "[제시문 (가)] 는 유행이 모든 사람들에게 모방하려는 경향이 있다는 것을 말해 준다. 이는 사람들이 자신들만의 고유한 스타일을 가지기를 원하면서 동시에 사람들과 함께 행동하고 싶기 때문이다. 이 두 가지욕구를 충족시키기 위해 사람들은 유행을 따르게 되는데, 이를 통해 themselves를 표현하면서 동시에 사회적으로 인정받을 수 있기 때문이다. 예를 들어, 패션에서 유행이 생겨나면 사람들은 그 유행을 따르는 것으로 자신들만의 고유한 스타일을 유지하면서 사람들 사이에서통합의감을 느낄 수 있다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "제시문 (가)를 기반으로 질문을 만들면 다음과 같습니다.\n",
            "가) 주어진 시기 동안 유행이 변화한 경우, 어떤 요인들이 그 변화를 가져왔을까요?\n",
            "나) 전통적인 가치에 대한 반항과 현대 사회의 변화에 대한 수용성이 강한 시대에서는 유행이 어떻게 형성될 수 있을까요?\n",
            "다) 어떤 집단은 옛 것들에 집착하고, 다른 사람들은 새로운 것에 열중하는 경우, 왜 이렇게 나타나는 것이일까요?\n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)를 기반으로 질문을 만들면 다음과 같습니다.\n",
            "가) 오래된 관습을 따르는 경향이 있는 문화에서는 어떠한 것들이 유행이 될 수 있을까요?\n",
            "나) 어떤 사회에서는 변화에 대한 애정과 동시대인들에 대한 모방이 강력한 영향을 미치는지 분석해보세요.\n",
            "다) 어떤 문화에서는 사람들이 가장 좋아하는 것, 즉 유행에 대해 어떻게 생각하느냐를 살펴봅시다.\n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)를 기반으로 질문을 만들면 다음과 같습니다.\n",
            "가) 언어 사용에서 어떠한 요인들이 유행이 되는 것을 설명하는 역할을 할 수 있을까요?\n",
            "나) 어떤 사회에서는 청소년층과 어른들의 언어 사용할 방식이 다르게 형성되는지 분석해보세요.\n",
            "다) 왜 사람들은 항상 새로운 유행에 관심을 가지며, 어떤 요인들이 유행의 변화가 발생하는지를 설명해주세요.\n",
            "**************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NEz1fY-qHC1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 영문입력"
      ],
      "metadata": {
        "id": "A8-cYHYw7zOm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRrrBGxn70MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_list = []"
      ],
      "metadata": {
        "id": "8dDGOTxa70Y-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "OFu1rSZ470Y-"
      },
      "outputs": [],
      "source": [
        "PROMPT = '''\n",
        "Role: \"You are a teacher who runs a college entrance essay interview academy in South Korea.\"\n",
        "Task: \"Your main job is to generate example questions from a set of example statements.\n",
        "I'm going to show you an example question. When you're given a new set of presentations, you figure out what they're about and then you create a similar set of questions.\n",
        "At this time, the questions are to identify the relationship between the presentations and ask them.\"\n",
        "\n",
        "Format :\n",
        "\"(다)의 사례들을 (가)와 (나)에 비추어 평가하시오.\",\n",
        "\"(가)의 내용에 기반하여 (다)의 상황이 가능한 이유를 설명하시오.\",\n",
        "\"(가), (나)를 바탕으로, (라)의 ‘건강한 사회’가 유지될 수 있는 이유를 설명하시오.\",\n",
        "\"(가) 또는 (나)에서 문제가 된 상황과 유사한 다른 사례를 제시하고 그 이유를 설명하시오.\",\n",
        "\"제시문 (나)를 바탕으로 기술에 대한 제시문 (가)와 제시문 (다)의 주장을 설명하시오.\",\n",
        "\"제시문 (가)와 (나)의 내용을 바탕으로 제시문 (다)의 ᄀ을 평가하시오.\",\n",
        "\"제시문 (가)와 (나), 그리고 제시문 (나)와 (다)의 공통점과 차이점을 각각 논하시오.\",\n",
        "\"(가)와 (나)에서 작가가 갖추어야 할 자질들을 찾아 차이점과 공통점을 설명하시오.\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Df2BF0wn70Y-"
      },
      "outputs": [],
      "source": [
        "test_text = '''You need to look at the following 3 presentations and create 3 sets of questions and answers for them. Please delete all other unnecessary sentences.\n",
        "All questions and answers should be in Korean, and concise, consisting of 1-2 sentences.\n",
        "Each question should have a different form and content that should not overlap.\n",
        "Please create three questions and three answers. As I said, when generating questions, the content should not imply or suggest the answers. When creating questions, it's important to consider the relationships between the contents of each prompt.\n",
        "The question must be a commanding tone, and without honorifics.\n",
        "Each question and answer should be related to all prompts.\n",
        "Questions should be concise, and should not quote prompt.\n",
        "\n",
        "The generation format is as follows:\n",
        "[\"Question 1\"]\n",
        "\"Question 2\"\n",
        "\"Question 3\"\n",
        "\"Answer 1\"\n",
        "\"Answer 2\"\n",
        "[Answer 3]\n",
        "\n",
        "Now, let me show you a new set of presentations.\n",
        "\n",
        "[제시문 (가)]\n",
        "유행은 누구나 가는 길로 가려는 모방 욕구와 자신을 표현하고 싶은 개성 욕구를 동시에 충족시켜주는 매우 독특한 현상이다.\n",
        "유행을 따름으로써 자신도 주변 사람들과 똑같이 행동하고 있다는 안도감을 얻으려는 심리, 그리고 다른 사람들과 구별되는 만족감을 얻으려는 심리가 복합적으로 얽혀 있는 것이다.\n",
        "남을 따르고자 하는 욕구나 소망이 결여되는 경우, 아니면 반대로 개성을 드러내려는 욕구가 결여되는 경우 유행의 영역은 더 이상 존재하지 않게 된다.\n",
        "예를 들면, 모든 개인이 제각각 특수한 존재가 되려 하고 모방을 기피하는 집단에서는 유행이 발생하지 않는다.\n",
        "1390년경 피렌체에서 남성 복장의 뚜렷한 유행이 존재하지 않았던 이유는 모두가 각자 독특한 방식으로 차려입고자 했기 때문이다.\n",
        "반면에 유행이 전체를 지배하게 되면, 즉 처음에는 몇몇 사람이 주도했던 일을 예외 없이 모두가 따라 하게 되면, 그것이 옷이든 음악이든 더 이상 유행이라고 부르지 않는다.\n",
        "유행은 결코 현재 상태에 머물지 않으며 부단히 진행된다.\n",
        "\n",
        "[제시문 (나)]\n",
        "옛것이나 선조를 추종하는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고 동시대인들을 모방하려는 경향이 유행의 시대를 특징 짓는다.\n",
        "이제 사람들은 선조를 닮으려고 하기보다는, 주위 사람들을 닮으려 한다는 것이다.\n",
        "변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시대를 이끄는 두 가지 중요한 원리이다.\n",
        "이 원리들에는 선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 경향이 동반된다.\n",
        "옛것은 더 이상 존경해야 하는 것으로 여겨지지 않는다.\n",
        "사람들은 끊임없이 새것을 탐닉하며, 같은 시대에 사는 사람들을 따라 한다.\n",
        "예를 들면, 근대 유럽의 상류사회는 변화에 대한 열정에 사로잡혀 있었다.\n",
        "그 열정은 최신 발명품과 이국적 문물에 대한 열망으로 타올랐다. 이탈리아, 스페인, 프랑스가 선도하는 유행을 다른 국가의 상류층 사람들이 앞다퉈 모방하였다.\n",
        "그들은 새로운 것이면 무엇이든지 뒤쫓아가려고 했고, 가장 최근에 생겨난 변화를 받아들이고자 했다.\n",
        "중하층 사람들은 다시 상류층의 유행을 모방하였다. 유행은 이런 식으로 사회 전체에 확산하였고, 언제나 새로운 것이 나타날 때마다 그것을 추종하는 상류층에 의해 계속해서 변화하였다.\n",
        "\n",
        "[제시문 (다)]\n",
        "사람은 사회적인 존재이기 때문에 지위나 역할이 달라지면 자연히 거기에 맞는 행동양식을 하게 된다. 그 행동양식 가운데 하나가 언어 사용이다.\n",
        "가령,‘엄마, 아빠’와 같은 말은 어린이들의 말이고,‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적 행동 양식이 반영된 것이라고 할 수 있다.\n",
        "이런‘연령 단계’에 의한 언어 차이 외에도, 기성세대와 신세대, 또는 노년층 세대와 청소년층 세대처럼 ‘세대 차이’에 의한 언어 차이도 있을 수 있다.\n",
        "젊은 세대는 대체로 기성세대와 비교해 볼 때 유행에 민감하고 새로운 변화를 쉽게 받아들인다.\n",
        "언어에서도 젊은 세대의 이런 특성이 반영된다. 젊은 세대는 ‘반모’(반말 모드), ‘인싸’(무리에 잘 어울려 지내는 사람, 영어 ‘insider’의 의미) 등과 같은 유행어와 신조어를 만드는 주축으로, 대부분의 유행어와 신조어는 자신의 독특한 개성을 표출하려는 젊은 세대를 중심으로 사용된다.\n",
        "이러한 언어의 유행 현상은 세대 변화에 따른 것이다.\n",
        "                '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ebbe1a-ba30-4a3c-b672-881e9e1d35a8",
        "id": "K-gjjhnU70Y_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are three sets of questions and answers based on the three presents above.\n",
            "\n",
            "Set 1:\n",
            "- 질문 1: 유행은 누구의 욕구가 충족되는 현상인가?\n",
            "답변 1: 유행은 주도적으로 유행을 만드는 사람들의 욕구가 충족되는 현상이다.\n",
            "- 질문 2: 왜 유행이 중요한 역할을 하는가에 대해 설명해주세요.\n",
            "답변 2: 유행은 사람들이 자신들 속에 있는 개성을 표현하고, 그들끼리 서로 다른 존재임을 확인하고자 하는 욕구를 충족시켜주는 역할을 한다.\n",
            "- 질문 3: 어떤 조건에서 아래와 같은 말이 성립할 수 있을까요? \"모든 개인이 제각각 특수한 존재가 되려 하고 모방을 기피하는 집단에서는 유행이 발생하지 않는다.\"\n",
            "답변 3: 모든 개인이 제각각 특수한 되고자 하는 욕구가 결여되는 경우이거나, 혹은 모방을 기피하는 집단의 경우이다.\n",
            "\n",
            "Set 2:\n",
            "- 질문 1: 어떤 원리로 인해 옛것을 따라가는 경향이 유행의 시기를 지배한다면?\n",
            "답변 1: 옛것을 따라가는 현상은 변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시기를 이끄는 원리이다.\n",
            "- 질문 2: 왜 대중들이 새로운 것을 선호하느냐에 대해 설명해주세요.\n",
            "답변 2: 대중들은 새롭고 혁신적인 것에 대한 열정으로 가득 차 있으며, 이를 통해 자신들의 삶에서 변화를 바라는 욕구가 충족되기 때문이다.\n",
            "- 질문 3: 어떤 시대적 상황 속에서 사람들은 옛 것을 따라갔는지에 대해 설명해주세요.\n",
            "답변 3: 근대 유럽의 상류사회에서는 사람들이 새로운 것, 즉 최신 발명품과 이국적 문물에 대한 열정에 사로잡혀 있었고, 이를 선도하는 유행을 다른 국가의 상류층 사람들이 앞다 그랈다.\n",
            "\n",
            "Set 3:\n",
            "- 질문 1: 사회적인 존재로서의 역할은 어떤 영향을 미칠까?\n",
            "답변 1: 사회적인 존재로서의 역할은 행동양식 중 하나가 언어 사용이다. '예를 들어,  ‘엄마, 아빠’와 같은 말은 어린이들의 말이고,  ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적 행동 양식이 영향을 미친 것이다.\n",
            "- 질문 2: 왜 청년 세대는 새로운 변화를 쉽게 받아들여 유행에 민감하다는 것을 설명해주세요.\n",
            "답변 2: 청년 세대는 전반적으로 보다 개방적이고 혁신적인 성향을 가지고\n"
          ]
        }
      ],
      "source": [
        "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, verbose = 0)\n",
        "model = Llama(\n",
        "    model_path='/content/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=-1, verbose = 0        # Number of model layers to offload to GPU\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{test_text}\"}\n",
        "    ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt=True, verbose = 0\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":500,\n",
        "    \"stop\":[\"<|eot_id|>\"],\n",
        "    \"top_p\":0.8,\n",
        "    \"temperature\":0.3,\n",
        "    \"echo\":True, # Echo the prompt in the output\n",
        "}\n",
        "\n",
        "resonse_msg = model(prompt, **generation_kwargs)\n",
        "generated = resonse_msg['choices'][0]['text'][len(prompt):]\n",
        "print(generated)\n",
        "qa_list.append(generated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for a in range(len(qa_list)):\n",
        "  print(qa_list[a])\n",
        "  print('*'*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99148013-5b6d-44b1-e6ae-77624561e691",
        "id": "LLHz2Vin70Y_"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[질문 1]\n",
            "제시문 (가)의 내용을 기반으로, 어떤 사람들은 유행을 따른다는 이유가 무엇일까요? \n",
            "\n",
            "답변 1\n",
            "유행을 따름으로써 자신도 주변 사람들과 똑같이 행동하고 있다는 안도감을 얻으려고 하는 심리적인 이유와, 다른 사람들로부터 인정받고 동화되기 위한 만족감을 얻기 위해서이다. \n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)의 내용을 기반으로, 어떤 사람들은 왜 새로운 것을 숭배하고 동시대 사람들을 모방하느냐? \n",
            "\n",
            "답변 2\n",
            "변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시기를 이끌어내는 주요한 원리이다. 이 원칙들은 선조의 유산을 평가절하하고 현재의 규범을 중요시하려는 성향이 함께 동반된다. \n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)의 내용을 기반으로, 왜 사람들은 시대에 따라 다른 언어를 사용하느냐? \n",
            "\n",
            "답변 3\n",
            "사람은 사회적인 존재이기 때문에 신분이나 역할에 따라 행동양식이 달라지며, 그 행동양식 중 하나가 언어 사용이다. 따라서, 세대 변화나 사회적 변화에 따라 사용되는 언어도 변화하게 된다. 특히, 젊은 세대는 새로운 유행어와 신조어를 만들어내며 유행에 민감하고 변화를 쉽게 받아들이는 경향이 때문이다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "제시문 (가)의 내용을 바탕으로, 유행이 왜 모든 개인을 유혹하는 것인지 묻습니다.\n",
            "\n",
            "[답변 1]\n",
            "유행은 바로 다른 사람들과 나를 구분할 수 있는 좋은 방법이 되기 때문입니다. 모든 개인이 제각각 특수한 존재가 되고 싶어하는 욕구와 동시에 모방을 회피하고자 하는 심리적인 요인이 결합되어 있기 때문입니다. \n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)의 내용을 바탕으로, 왜 사람들은 새로운 것을 숭배하고 동시대인들을 모방하려는 경향이 유행의 시기를 특징으로 만들게 되었을까요?\n",
            "\n",
            "[답변 2]\n",
            "변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시기를 이끌어내는 두 가지 중요한 원리입니다. 이 원소들은 선조의 유산을 평가 절하하고 현재의 규범을 중요시하려는 육안으로 결합됩니다. \n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)의 내용을 바탕으로, 언어 사용은 사회적인 존재이기 때문에 어떠한 사회적 변화가 일어날 때마다 쉽게 변화하게 된다는 점에 대해 묻습니다.\n",
            "\n",
            "[답변 3]\n",
            "언어는 사회적 존재이기 때문에, 자연히 그들이 사회적 변화에 따라 행동양식이 변화하면 방언 역시 쉽게 변화하게 됩니다. 예를 들어, 새로운 세대가 유행에 민감하고 새로운 변화를 쉽게 받아들이면, 언어에서도 이를 반영하여 유행어와 신조어가 만들어지고 유행이 되기 때문에, 언어의 변화는 사회적인 변동과 함께 일어나게 됩니다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "제시문 (가)의 내용을 기반으로, 유행이 왜 모든 사람들에게 보편화되는 것이 아니라는 질문을 작성하시오.\n",
            "\n",
            "[답변 1]\n",
            "유행은 모든 사람들에게 보편화되지는 않습니다. 유행은 어떤 특정 집단을 중심으로 특정 행동이나 취향이 대중적으로 받아들여진 후에, 그 대상이 점차적으로 확산되면서 일반적으로 받아들여진다고 합니다. 따라서, 유행이 모든 사람에게 보편화되는 것은 아니라는 것이 핵심입니다.\n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)의 내용을 기반으로, 왜 사람들은 옛 것을 따라가는 경향이 있는지에 대해 질문을 작성하시오.\n",
            "\n",
            "[답변 2]\n",
            "옛 것을 따라가는 경향은 사람들이 현재의 규범을 중요시하며, 변화를 평가절하하는 경향이 결합되어 있기 때문입니다. 이는 선조를 존경하는 경향이 동반되어있다는 것을 의미합니다. 따라서, 사람들은 옛 것을 따라가는 경향이 있는 것입니다.\n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)의 내용을 기반으로, 왜 언어 사용에서 세대 차이가 발생하기 때문에 새로운 말이 유행이 되는지에 대해 질문을 작성하시오.\n",
            "\n",
            "[답변 3]\n",
            "언어 사용에서 세대 차이가 발생하는 이유는 젊은 세대가 새로운 변화를 쉽게 받아들여 새로운 용어와 유행어를 만들어내기 때문입니다. 따라서, 새로운 말은 주로 젊은 세대에서 시작되어 점차적으로 확산됩니다. 이는 세대 변화에 따른 언어의 유행 현상으로 이해할 수 있습니다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "제시문 (가)의 내용을 기반으로, 유행이 왜 모든 사람들에게 보편화되는 것이 아니라는 질문을 작성하시오.\n",
            "\n",
            "[답변 1]\n",
            "유행은 모든 사람들에게 보편화되지는 않습니다. 유행은 어떤 특정 집단을 중심으로 특정 행동이나 취향이 대중적으로 받아들여진 후에, 그 대상이 점차적으로 확산되면서 일반적으로 받아들여진다고 합니다. 따라서, 유행이 모든 사람에게 보편화되는 것은 아니라는 것이 핵심입니다.\n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)의 내용을 기반으로, 왜 사람들은 옛 것을 따라가는 경향이 있는지에 대해 질문을 작성하시오.\n",
            "\n",
            "[답변 2]\n",
            "옛 것을 따라가는 경향은 사람들이 현재의 규범을 중요시하며, 변화를 평가절하하는 경향이 결합되어 있기 때문입니다. 이는 선조를 존경하는 경향이 동반되어있다는 것을 의미합니다. 따라서, 사람들은 옛 것을 따라가는 경향이 있는 것입니다.\n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)의 내용을 기반으로, 왜 언어 사용에서 세대 차이가 발생하기 때문에 새로운 말이 유행이 되는지에 대해 질문을 작성하시오.\n",
            "\n",
            "[답변 3]\n",
            "언어 사용에서 세대 차이가 발생하는 이유는 젊은 세대가 새로운 변화를 쉽게 받아들여 새로운 용어와 유행어를 만들어내기 때문입니다. 따라서, 새로운 말은 주로 젊은 세대에서 시작되어 점차적으로 확산됩니다. 이는 세대 변화에 따른 언어의 유행 현상으로 이해할 수 있습니다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "제시문 (가)의 내용을 기반으로, 유행이 왜 모든 사람들에게 보편화되는 것이 아니라는 질문을 작성하시오.\n",
            "\n",
            "[답변 1]\n",
            "유행은 모든 사람들에게 보편화되지는 않습니다. 유행은 어떤 특정 집단을 중심으로 특정 행동이나 취향이 대중적으로 받아들여진 후에, 그 대상이 점차적으로 확산되면서 일반적으로 받아들여진다고 합니다. 따라서, 유행이 모든 사람에게 보편화되는 것은 아니라는 것이 핵심입니다.\n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)의 내용을 기반으로, 왜 사람들은 옛 것을 따라가는 경향이 있는지에 대해 질문을 작성하시오.\n",
            "\n",
            "[답변 2]\n",
            "옛 것을 따라가는 경향은 사람들이 현재의 규범을 중요시하며, 변화를 평가절하하는 경향이 결합되어 있기 때문입니다. 이는 선조를 존경하는 경향이 동반되어있다는 것을 의미합니다. 따라서, 사람들은 옛 것을 따라가는 경향이 있는 것입니다.\n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)의 내용을 기반으로, 왜 언어 사용에서 세대 차이가 발생하기 때문에 새로운 말이 유행이 되는지에 대해 질문을 작성하시오.\n",
            "\n",
            "[답변 3]\n",
            "언어 사용에서 세대 차이가 발생하는 이유는 젊은 세대가 새로운 변화를 쉽게 받아들여 새로운 용어와 유행어를 만들어내기 때문입니다. 따라서, 새로운 말은 주로 젊은 세대에서 시작되어 점차적으로 확산됩니다. 이는 세대 변화에 따른 언어의 유행 현상으로 이해할 수 있습니다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "제시문 (가)의 내용을 기반으로, 유행이 왜 모든 사람들에게 보편화되는 것이 아니라는 질문을 작성하시오.\n",
            "\n",
            "[답변 1]\n",
            "유행은 모든 사람들에게 보편화되지는 않습니다. 유행은 어떤 특정 집단을 중심으로 특정 행동이나 취향이 대중적으로 받아들여진 후에, 그 대상이 점차적으로 확산되면서 일반적으로 받아들여진다고 합니다. 따라서, 유행이 모든 사람에게 보편화되는 것은 아니라는 것이 핵심입니다.\n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)의 내용을 기반으로, 왜 사람들은 옛 것을 따라가는 경향이 있는지에 대해 질문을 작성하시오.\n",
            "\n",
            "[답변 2]\n",
            "옛 것을 따라가는 경향은 사람들이 현재의 규범을 중요시하며, 변화를 평가절하하는 경향이 결합되어 있기 때문입니다. 이는 선조를 존경하는 경향이 동반되어있다는 것을 의미합니다. 따라서, 사람들은 옛 것을 따라가는 경향이 있는 것입니다.\n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)의 내용을 기반으로, 왜 언어 사용에서 세대 차이가 발생하기 때문에 새로운 말이 유행이 되는지에 대해 질문을 작성하시오.\n",
            "\n",
            "[답변 3]\n",
            "언어 사용에서 세대 차이가 발생하는 이유는 젊은 세대가 새로운 변화를 쉽게 받아들여 새로운 용어와 유행어를 만들어내기 때문입니다. 따라서, 새로운 말은 주로 젊은 세대에서 시작되어 점차적으로 확산됩니다. 이는 세대 변화에 따른 언어의 유행 현상으로 이해할 수 있습니다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "유행은 누구에게나 가는 길이냐 아니면 어디론가 떠나 있느냐에 따라 현상의 유지 또는 소멸이 달려있을까요?\n",
            "\n",
            "[답변 1]\n",
            "유행은 어느 정도 예상 가능하고 예측 가능한 경향이 있다는 점에서, 누구나 갈 수 있는 일반적인 유행이라는 특성을 가지고 있습니다. 그러나 유행은 동시에 주변 사람들에게 이어져 있으며, 따라서 내가 어떤 행동을 하면 그 행동이 유행이 될지 알 수 없습니다. 즉, 어느 정도의 예측이 가능하지만, 항상 어떤 형태로든 유행이 계속 유지되거나 소멸하는 것은 아닙니다.\n",
            "\n",
            "[질문 2]\n",
            "유행이 생긴 이유와 동시대인들을 모방하려는 행동에 대해 어떤 사회적 원인이 있는 걸까요?\n",
            "\n",
            "[답변 2]\n",
            "유행이 생긴 원인과 동시대인들을 모방하려는 행동은 보통 어떤 깊은 심리적 원인에 기인합니다. 유행은 곧 다른 사람들과 나를 동일시시키는 것을 의미하며, 이런 의미에서 자신이 다른 사람들과 차별되기를 원하는 욕구에서 비롯될 수 있습니다. 그렇기 때문에 사람들은 자신들 주변에 있는 사람들과 비슷해지기 위해 유행을 따라가는 것입니다. 이는 더욱 구체적으로는 그들이 자신의 모습을 선명하게 보여줄 수 있는 방법 중 하나인 것입니다.\n",
            "\n",
            "[질문 3]\n",
            "최근의 유행 양식과 예전에 유행을 대한 사람들의 심리와 행동에는 어떤 차이가 있을까요?\n",
            "\n",
            "[답변 3]\n",
            "최근의 유행 양식과 예전에 유행을들에 대한 사람들의 심리와 행동 사이에는 몇 가지 차이점이 있습니다. 예를 들어, 예전에는 유행이 바뀌면 사람들이 크게 충격을 받았으며, 새로운 유행에 대해서는 거부감이 있었습니다. 하지만 현재는 정보 기술이 발달하면서 사람들은 새로운 유행에 대한 정보를 쉽게 얻을 수 있게 되었고, 따라서 더 빠르게 새로운 유행을 수용하고 따라하기 때문에 섣부른 변화를 방지할 수 있는 공간이 좁아졌습니다. 또한, 오늘날은 다양성과 포러니즘 등의 개념으로 인해 사람들이 다양성을 인정하고 존중하는 자세가 높아졌기 때문에, 사람들은 다른 사람들의 행동을 비난하거나 혐오의 감정을 표현하는 대신 다양성을 인정하며 상호 존중하는 경향이 있습니다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "제시문 (가)의 내용을 기반으로, 어떤 사람들은 유행을 따라 행동하고 어떤 사람들은 개성을 지키려고 하는 이유에 대해 설명해주세요.\n",
            "\n",
            "[답변 1]\n",
            "유행에 대한 민감함이 높은 사람들은 일단 자신의 행동과 옷차림 등을 다른 사람들과 동일시하는 것을 즐기기 때문에 유행을 따라 행동합니다. 반면, 개성을 중시하는 사람들은 그들이 스스로 선택할 수 있는 방식을 선호하며, 유행이 추구하는 것과 다를 수 있도록 행동하고 의상을 선택합니다.\n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)의 내용을 기반으로, 왜 사람들은 옛 것을 숭배하는 경향이 있다는 것을 설명해주세요.\n",
            "\n",
            "[답변 2]\n",
            "옛것을 숭배하는 현상은 사람들이 현재의 사회와문화에 대한 불안감을 느끼거나, 그들이 희망하며 원하는 것들이 현재와는 다르다는 것을 의미합니다. 그렇기 때문에 사람들은 과거에 머무르려고 하며, 기존의 사회적 규범과 관습을 따르는 경향이 생길 수 있습니다.\n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)의 내용을 바탕으로, 왜 언어 사용에서 세대 차이가 생기는지 설명해주세요.\n",
            "\n",
            "[답변 3]\n",
            "세대 차이에 따라 언어 차이가 생기는 이유는 세대마다 유행어와 사회적인 역할 등이 다르기 때문입니다. 더 구체적으로는, 어린 세대는 새로운 유행어를 쉽게 받아들이고 사용하기 때문에, 언어의 변화 속도가 빨라집니다. 반면 노년층 세대는 이미 굳어진 언어를 유지하며, 변화를 거부할 가능성이 높습니다. 이러한 이유로, 세대마다 언어 차이가 생기게 됩니다.\n",
            "**************************************************\n",
            "[질문 1]\n",
            "제시문 (가)의 내용을 바탕으로, 유행이 왜 모든 개인이 각자 독특하게 행동하고 싶을 때 나타나는 현상인가요? \n",
            "\n",
            "[답변 1]\n",
            "제시문 (가)에 따르면, 유행은 누구든 쉽게 따라할 수 있는 행동입니다. 모든 개인이 제각각 특수한 것이 되고 싶어하는 욕구와, 자신과 다른 사람들과 구별되는 만족감을 얻으려는 심리적인 원리에 의해 형성됩니다. 따라서, 사람들은 유행을 따라함으로써 스스로에게 안도감을 느끼며, 독특함을 유지하고자 합니다. \n",
            "\n",
            "[질문 2]\n",
            "제시문 (나)의 내용을 바탕으로, 왜 옛 것을 따르는 경향이 관습의 시기를 지배한다면 새로운 것을 숭배하고 동시대 사람들을 모방하려는 여유가 생기는지 설명해주세요.\n",
            "\n",
            "[답변 2]\n",
            "제시문 (나)에 따르면, 옛것을 따라가는 것은 변화에도 애착을 느끼고, 현재의 규범을 중요시하는 경향이 함께 동반됩니다. 이는 사람들이 지난 것을 존경하지 않고, 현재를 위한 새로운 것을 추구한다는 것을 의미합니다. 이러한 원리에서, 사람들은 새로운 것을 탐내며, 같은 시대에 사는 사람들을 따라 다닌다는 것이 생기는 것입니다. \n",
            "\n",
            "[질문 3]\n",
            "제시문 (다)의 내용을 바탕으로, 왜 언어 사용에서 연령 단계와 세대 차이가 생기는지 설명해주세요.\n",
            "\n",
            "[답변 3]\n",
            "제시문 (다)에 따르면, 언어 사용에서 연령 단계와 세대 차이가 생기는 이유는 사람들이 사회적인 존재이기 때문입니다. 지위가 달라지면 자연히 그 행동양식에 맞춰야 하며, 언어 사용 역시 그러한 영향을 받습니다. 따라서, 어린 세대는 어른들의 말이나 어린이들의 말을 따라하며, 노인층 세대는 청소년층 세대의 유행어와 표현 방식을 채택하는 식으로 세대 차이가 언어 사용에서 반영됩니다. 또한, 젊은 세대는 새로운 유행어와 조어 등을 만들어 내는 주축이 되기 때문에, 언어 사용에서 더욱 혁신적인 변화를 가져올 수 있습니다.\n",
            "**************************************************\n",
            "Here are three sets of questions and answers for the three presents.\n",
            "\n",
            "[질문 1]\n",
            "유행은 누구든 따라 할 수 있는 행동이나 옷차림 같은 것을 말합니다. 그렇다면 유행이 왜 생겨나는 건가요?\n",
            "\n",
            "[답변 1]\n",
            "유행은 사람들이 자신들만의 고유한 개성을 표현하고 싶어하는 욕구와, 다른 사람들과 함께 행동하고 싶은 불안함이 결합된 결과입니다. 즉, 누구든 따라 할 수 있는 행동이나 옷차림 같은 것을 말합니다. \n",
            "\n",
            "[질문 2]\n",
            "옛것을 따르는 경향이 관습의 시대를 지배했다면, 새로운 것을 숭배하고 동시대 사람들을 모방하려는 성격이 유행의 시기를 특징 짓습니다. 어떤 의미인지 설명해주세요.\n",
            "\n",
            "[답변 2]\n",
            "옛것이나 선조를 따라가는 것은 전통적인 가치를 지키는 것을 의미합니다. 그러나 새로운 것을 모르는 것은 동시대 사람들의 생활 방식을 따라가지 않으면 소외됩니다. 따라서, 사람들은 새로운 것을 살펴보고 모방하여 동시대 사람들을 따라가는 경향이 있습니다. \n",
            "\n",
            "[질문 3]\n",
            "언어는 사회적인 존재이기 때문에, 나이나 세대별로 차이가 있을 수 있습니다. 어떤 내용을 담고 있죠?\n",
            "\n",
            "[답변 3]\n",
            "언어에는 나이에 따른 사회적 행동 양식이나, 세대 차이 등에 따른 언어 차이도 있습니다. 예를 들어, 어린 아이들은 어린이들의 말로 발음하기 때문에 어른들의 말인 ‘자네, 댁’과 같은 단어를 많이 사용합니다. 또한, 젊은 세대는 새로운 유행어와 신조어를 만들어 내는 주축이 되기 때문에, 이들이 사용하는 언어는 다른 세대에 비해 더욱 독특하고 놀라운 특징이 있습니다.\n",
            "**************************************************\n",
            "Here are three sets of questions and answers based on the three presents above.\n",
            "\n",
            "Set 1:\n",
            "- 질문 1: 유행은 누구의 욕구가 충족되는 현상인가?\n",
            "답변 1: 유행은 주도적으로 유행을 만드는 사람들의 욕구가 충족되는 현상이다.\n",
            "- 질문 2: 왜 유행이 중요한 역할을 하는가에 대해 설명해주세요.\n",
            "답변 2: 유행은 사람들이 자신들 속에 있는 개성을 표현하고, 그들끼리 서로 다른 존재임을 확인하고자 하는 욕구를 충족시켜주는 역할을 한다.\n",
            "- 질문 3: 어떤 조건에서 아래와 같은 말이 성립할 수 있을까요? \"모든 개인이 제각각 특수한 존재가 되려 하고 모방을 기피하는 집단에서는 유행이 발생하지 않는다.\"\n",
            "답변 3: 모든 개인이 제각각 특수한 되고자 하는 욕구가 결여되는 경우이거나, 혹은 모방을 기피하는 집단의 경우이다.\n",
            "\n",
            "Set 2:\n",
            "- 질문 1: 어떤 원리로 인해 옛것을 따라가는 경향이 유행의 시기를 지배한다면?\n",
            "답변 1: 옛것을 따라가는 현상은 변화에 대한 애정과 동시대인들에 대한 모방이 유행의 시기를 이끄는 원리이다.\n",
            "- 질문 2: 왜 대중들이 새로운 것을 선호하느냐에 대해 설명해주세요.\n",
            "답변 2: 대중들은 새롭고 혁신적인 것에 대한 열정으로 가득 차 있으며, 이를 통해 자신들의 삶에서 변화를 바라는 욕구가 충족되기 때문이다.\n",
            "- 질문 3: 어떤 시대적 상황 속에서 사람들은 옛 것을 따라갔는지에 대해 설명해주세요.\n",
            "답변 3: 근대 유럽의 상류사회에서는 사람들이 새로운 것, 즉 최신 발명품과 이국적 문물에 대한 열정에 사로잡혀 있었고, 이를 선도하는 유행을 다른 국가의 상류층 사람들이 앞다 그랈다.\n",
            "\n",
            "Set 3:\n",
            "- 질문 1: 사회적인 존재로서의 역할은 어떤 영향을 미칠까?\n",
            "답변 1: 사회적인 존재로서의 역할은 행동양식 중 하나가 언어 사용이다. '예를 들어,  ‘엄마, 아빠’와 같은 말은 어린이들의 말이고,  ‘자네, 댁’과 같은 단어는 어른들의 말이라는 것도 나이에 따른 사회적 행동 양식이 영향을 미친 것이다.\n",
            "- 질문 2: 왜 청년 세대는 새로운 변화를 쉽게 받아들여 유행에 민감하다는 것을 설명해주세요.\n",
            "답변 2: 청년 세대는 전반적으로 보다 개방적이고 혁신적인 성향을 가지고\n",
            "**************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7t5EmH9a70Y_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5cd69d040dd9479aa53cc8177981f559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b541d24c191a473db213535554f28577",
              "IPY_MODEL_70f3fbac817945f387a6c9edb275b0b4",
              "IPY_MODEL_4c6e3dec9eef45e48328c2adcd2ebe68"
            ],
            "layout": "IPY_MODEL_7ca548e4b40b481a96e1d5b793bdd625"
          }
        },
        "b541d24c191a473db213535554f28577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50c9fce6869b4616ada71b3802813439",
            "placeholder": "​",
            "style": "IPY_MODEL_1f4e691f7db64e3a90b497363f8353bf",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "70f3fbac817945f387a6c9edb275b0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9622e8c7fe7d47eeabe4b8e2dd06bcdf",
            "max": 51106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_488f53e9c9a149cfb72cd96b0f601dba",
            "value": 51106
          }
        },
        "4c6e3dec9eef45e48328c2adcd2ebe68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec580b1a7e3646949b4a37c73be1358d",
            "placeholder": "​",
            "style": "IPY_MODEL_585a8c25882f40bfb437f7dce836801e",
            "value": " 51.1k/51.1k [00:00&lt;00:00, 4.16MB/s]"
          }
        },
        "7ca548e4b40b481a96e1d5b793bdd625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c9fce6869b4616ada71b3802813439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f4e691f7db64e3a90b497363f8353bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9622e8c7fe7d47eeabe4b8e2dd06bcdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "488f53e9c9a149cfb72cd96b0f601dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec580b1a7e3646949b4a37c73be1358d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "585a8c25882f40bfb437f7dce836801e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25ea18b3265b42678f55ac2c15d1d099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ecbe3ed83ef4979987137f9ff5e2336",
              "IPY_MODEL_a27b7d2e610f4345b52f915e61b6a215",
              "IPY_MODEL_4db8025ad73744e7b7712f0184e01aa2"
            ],
            "layout": "IPY_MODEL_19d83363ee6042f1a4f5125ed0f3706f"
          }
        },
        "8ecbe3ed83ef4979987137f9ff5e2336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_807b7ab0cf5e4e8f9cf85bbbfe04333e",
            "placeholder": "​",
            "style": "IPY_MODEL_dcacf042ac844c66ada43b3b6d58ce4c",
            "value": "tokenizer.json: 100%"
          }
        },
        "a27b7d2e610f4345b52f915e61b6a215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ed433a493804b62b9b02396ad9c2839",
            "max": 9085671,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d16311ef3fba48fb96e117f0af72f8b6",
            "value": 9085671
          }
        },
        "4db8025ad73744e7b7712f0184e01aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8db6c26f95f4b68a693933118ce2ef4",
            "placeholder": "​",
            "style": "IPY_MODEL_0de0af0c1da44744859d3ab0311691c1",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 10.4MB/s]"
          }
        },
        "19d83363ee6042f1a4f5125ed0f3706f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "807b7ab0cf5e4e8f9cf85bbbfe04333e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcacf042ac844c66ada43b3b6d58ce4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ed433a493804b62b9b02396ad9c2839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d16311ef3fba48fb96e117f0af72f8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8db6c26f95f4b68a693933118ce2ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0de0af0c1da44744859d3ab0311691c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "173d1bcfb4234ca99f80c6d3252b3e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88282786833c4f508ca883f5f9fd2b1f",
              "IPY_MODEL_e265311a6a614da7a6034dcee867d9cc",
              "IPY_MODEL_ccdf21bbb6c04ae8a9eea1abb20904c4"
            ],
            "layout": "IPY_MODEL_909527dbdf864dbbb6556ac3147fb79b"
          }
        },
        "88282786833c4f508ca883f5f9fd2b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb6e3de109a461487af25cc138ac69f",
            "placeholder": "​",
            "style": "IPY_MODEL_8de0cc772525478c8c7b6f054a7fb794",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "e265311a6a614da7a6034dcee867d9cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5e571f3e4a243b7acc67926f2c28320",
            "max": 444,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84b03ce9a2a442cab58058f47d31b556",
            "value": 444
          }
        },
        "ccdf21bbb6c04ae8a9eea1abb20904c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eee2b6a0adf343b8a703a41a11aa97f0",
            "placeholder": "​",
            "style": "IPY_MODEL_6b68720baa83471cb6c13f9ccf4982d0",
            "value": " 444/444 [00:00&lt;00:00, 42.5kB/s]"
          }
        },
        "909527dbdf864dbbb6556ac3147fb79b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cb6e3de109a461487af25cc138ac69f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de0cc772525478c8c7b6f054a7fb794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5e571f3e4a243b7acc67926f2c28320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84b03ce9a2a442cab58058f47d31b556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eee2b6a0adf343b8a703a41a11aa97f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b68720baa83471cb6c13f9ccf4982d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0dcfdbc280148c1936dfd7a6777cb42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fd957d4fc8b4109a5590e5dca3467da",
              "IPY_MODEL_e565e1fbf1924ccda28c80c6c49dc4b5",
              "IPY_MODEL_c9211a3f5b244d3eadcf424b3155ede3"
            ],
            "layout": "IPY_MODEL_5ce85ae253db4912a68a315c1e95c21b"
          }
        },
        "4fd957d4fc8b4109a5590e5dca3467da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4340ddc0bc1f468fbf988e53ed4139f4",
            "placeholder": "​",
            "style": "IPY_MODEL_2417fc8d480849d08360c33795e8c983",
            "value": ""
          }
        },
        "e565e1fbf1924ccda28c80c6c49dc4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15b6f0b490e244bda4e9611de3647bba",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_450665a289604343bb6a2cd2ce356ba4",
            "value": 0
          }
        },
        "c9211a3f5b244d3eadcf424b3155ede3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f44c9551314046afb56f20b337b1f239",
            "placeholder": "​",
            "style": "IPY_MODEL_4c9a8dd3016e44149f114e11990c89c6",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "5ce85ae253db4912a68a315c1e95c21b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4340ddc0bc1f468fbf988e53ed4139f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2417fc8d480849d08360c33795e8c983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15b6f0b490e244bda4e9611de3647bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "450665a289604343bb6a2cd2ce356ba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f44c9551314046afb56f20b337b1f239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c9a8dd3016e44149f114e11990c89c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31ef79da5c5c4057a89f38c311bbb920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdae60a81c2d409bbf2784628013a0e0",
              "IPY_MODEL_33a893320f5a40969bb3b23bae2880b3",
              "IPY_MODEL_02e197c1f1514ebdbda102ecde23b4cc"
            ],
            "layout": "IPY_MODEL_50e7f7042b504b598ec182e80665dabd"
          }
        },
        "cdae60a81c2d409bbf2784628013a0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fcaccbadd254e86835979a4072a25c0",
            "placeholder": "​",
            "style": "IPY_MODEL_82220b67353b47a39d783e8a94cfadc0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "33a893320f5a40969bb3b23bae2880b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe108b7fdd364dc0b537cab29a612c68",
            "max": 51106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfc7f0e413704ca1a54023da4a1f123f",
            "value": 51106
          }
        },
        "02e197c1f1514ebdbda102ecde23b4cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2511d5487f56407f92432a3920cb5c74",
            "placeholder": "​",
            "style": "IPY_MODEL_5dff6eaef64841a186780474a2e566e6",
            "value": " 51.1k/51.1k [00:00&lt;00:00, 4.43MB/s]"
          }
        },
        "50e7f7042b504b598ec182e80665dabd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fcaccbadd254e86835979a4072a25c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82220b67353b47a39d783e8a94cfadc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe108b7fdd364dc0b537cab29a612c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfc7f0e413704ca1a54023da4a1f123f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2511d5487f56407f92432a3920cb5c74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dff6eaef64841a186780474a2e566e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b0a462efd8d4d04996445e9976c0f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bc933052abb4215b259e508906851f5",
              "IPY_MODEL_c13bf3f17c34411893c3deb12dce3568",
              "IPY_MODEL_d71e214e002743fe98290cfec3f9702a"
            ],
            "layout": "IPY_MODEL_63e2b02c95a6432ca1c43a51b50caba6"
          }
        },
        "8bc933052abb4215b259e508906851f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a301eb4fd34f422981096fb71beab995",
            "placeholder": "​",
            "style": "IPY_MODEL_78d383ce021449948a7aa5717f37e075",
            "value": "tokenizer.json: 100%"
          }
        },
        "c13bf3f17c34411893c3deb12dce3568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9f9a60d072f4374b3c991725c205135",
            "max": 9085671,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f49b142d432e4c28ba55a996552e7fc9",
            "value": 9085671
          }
        },
        "d71e214e002743fe98290cfec3f9702a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dff6437605804f6094713a93b8a2dd42",
            "placeholder": "​",
            "style": "IPY_MODEL_fdf2ed948e8642b1a5f9553fbff42444",
            "value": " 9.09M/9.09M [00:03&lt;00:00, 2.84MB/s]"
          }
        },
        "63e2b02c95a6432ca1c43a51b50caba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a301eb4fd34f422981096fb71beab995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d383ce021449948a7aa5717f37e075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9f9a60d072f4374b3c991725c205135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49b142d432e4c28ba55a996552e7fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dff6437605804f6094713a93b8a2dd42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf2ed948e8642b1a5f9553fbff42444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74fc111f2c064bcab473f7c2057bd290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b652bdaca53461a86e5b1f81fcc292f",
              "IPY_MODEL_a2c1003488f845fdbd54f225a99bd0be",
              "IPY_MODEL_f28e01b551fa4c43a89f7de903469669"
            ],
            "layout": "IPY_MODEL_187818161f6f4b01b2729cdd59f38d7a"
          }
        },
        "7b652bdaca53461a86e5b1f81fcc292f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28a31174163f4e08896c3b2c14064959",
            "placeholder": "​",
            "style": "IPY_MODEL_1f9b4a4be2234ce296975cd566c9f6fb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a2c1003488f845fdbd54f225a99bd0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e7d9ffaba5d43aa9ff560fc225a7682",
            "max": 444,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_705c1d122a154927af649b3083b42b24",
            "value": 444
          }
        },
        "f28e01b551fa4c43a89f7de903469669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9744075a51ae455e93e0806de1676b48",
            "placeholder": "​",
            "style": "IPY_MODEL_8bf2ea552d614197a00d5e54158c3e6f",
            "value": " 444/444 [00:00&lt;00:00, 42.3kB/s]"
          }
        },
        "187818161f6f4b01b2729cdd59f38d7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28a31174163f4e08896c3b2c14064959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9b4a4be2234ce296975cd566c9f6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e7d9ffaba5d43aa9ff560fc225a7682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "705c1d122a154927af649b3083b42b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9744075a51ae455e93e0806de1676b48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bf2ea552d614197a00d5e54158c3e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}